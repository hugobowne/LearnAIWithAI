{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Evaluation Harness for Transcript Agent\n",
    "\n",
    "This notebook implements and runs the evaluation harness for the Transcript Agent. It uses the processed and annotated data prepared in the `parse_spans.ipynb` notebook, which has been uploaded as a Phoenix Dataset.\n",
    "\n",
    "**Goal:** Evaluate the agent's performance on key criteria (Tool Usage, SQL Correctness, Final Answer Quality) using LLM-as-judge, leveraging the Phoenix Experiments framework.\n",
    "\n",
    "**Plan:**\n",
    "\n",
    "1.  **Setup:** Import necessary libraries and initialize the Phoenix client and the evaluation LLM (e.g., GPT-4o).\n",
    "2.  **Load Dataset:** Retrieve the specific evaluation dataset (`transcript-agent-eval-data-...`) previously uploaded to Phoenix.\n",
    "3.  **Define Task Function:** Create the simple \"dummy\" task function required by `run_experiment` to pass through the pre-computed agent outputs from the dataset.\n",
    "4.  **Define Evaluators:**\n",
    "    *   Create three LLM-as-judge evaluator functions using `phoenix.evals.llm_classify`.\n",
    "    *   Develop prompts for each evaluator (Tool Usage, SQL Correctness, Final Answer Quality) instructing the LLM to assess the agent's output based on the input query, agent's actions/results, and referencing the human-provided labels and explanations from the dataset.\n",
    "5.  **Run Experiment:** Execute `phoenix.experiments.run_experiment`, passing the loaded dataset, the task function, and the list of defined evaluators.\n",
    "6.  **Review Results:** Briefly note that results (scores, LLM judge explanations) should be reviewed in the Phoenix UI experiment view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load .env file...\n",
      "Finished loading .env file (if found).\n",
      "Successfully parsed headers: Key='api_key'\n",
      "\n",
      "Initializing Phoenix client explicitly for cloud...\n",
      "Attempting px.Client(endpoint='https://app.phoenix.arize.com', headers=...)\n",
      "Phoenix client initialized successfully using explicit arguments.\n",
      "\n",
      "Initializing evaluation LLM (GPT-4o)...\n",
      "Evaluation LLM initialized.\n",
      "\n",
      "--- Setup Cell Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Setup (MODIFIED FOR EXPLICIT CLOUD CLIENT CONFIGURATION - FINAL ATTEMPT) ---\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Optional: suppress warnings\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.evals import OpenAIModel, llm_classify\n",
    "from phoenix.experiments import run_experiment\n",
    "from phoenix.experiments.types import Example\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv # Make sure dotenv is imported\n",
    "\n",
    "# Apply nest_asyncio for environments like Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- Load Environment Variables ---\n",
    "# Ensure this runs reliably near the start if not already done\n",
    "if 'dotenv_loaded' not in locals(): # Simple flag to avoid reloading if already done\n",
    "    print(\"Attempting to load .env file...\")\n",
    "    # Ensure your .env file has PHOENIX_COLLECTOR_ENDPOINT and PHOENIX_CLIENT_HEADERS\n",
    "    load_dotenv(verbose=True) # verbose=True shows which file is loaded\n",
    "    dotenv_loaded = True # Set flag\n",
    "    print(\"Finished loading .env file (if found).\")\n",
    "else:\n",
    "    print(\".env file assumed to be loaded previously.\")\n",
    "# --- End Load ---\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the API endpoint (base URL for cloud)\n",
    "cloud_api_endpoint = \"https://app.phoenix.arize.com\"\n",
    "\n",
    "# Get the required header string from environment\n",
    "# This MUST be set correctly in your .env file: PHOENIX_CLIENT_HEADERS=\"api_key=YOUR_KEY_VALUE\"\n",
    "api_headers_str = os.getenv(\"PHOENIX_CLIENT_HEADERS\")\n",
    "\n",
    "if not api_headers_str:\n",
    "    raise ValueError(\"CRITICAL: PHOENIX_CLIENT_HEADERS environment variable not found. Ensure it's set in your .env file and load_dotenv() ran.\")\n",
    "\n",
    "# Parse the header string (\"api_key=value\") into the required dictionary format\n",
    "api_headers_dict = {}\n",
    "try:\n",
    "    key, value = api_headers_str.split('=', 1)\n",
    "    parsed_key_name = key.strip()\n",
    "    parsed_key_value = value.strip()\n",
    "    if parsed_key_name != \"api_key\" or not parsed_key_value:\n",
    "            raise ValueError(\"Parsed key name is not 'api_key' or value is empty.\")\n",
    "    api_headers_dict[parsed_key_name] = parsed_key_value # Store as dict {\"api_key\": \"value\"}\n",
    "    print(f\"Successfully parsed headers: Key='{parsed_key_name}'\")\n",
    "except Exception as parse_err:\n",
    "    print(f\"ERROR: Could not parse PHOENIX_CLIENT_HEADERS string: '{api_headers_str}'. Expected 'api_key=value' format. Error: {parse_err}\")\n",
    "    raise ValueError(\"Invalid PHOENIX_CLIENT_HEADERS format\") from parse_err\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "# --- Initialize Client Explicitly ---\n",
    "print(\"\\nInitializing Phoenix client explicitly for cloud...\")\n",
    "px_client = None # Initialize to None\n",
    "try:\n",
    "    # Initialize with explicit endpoint and headers arguments\n",
    "    print(f\"Attempting px.Client(endpoint='{cloud_api_endpoint}', headers=...)\")\n",
    "    px_client = px.Client(endpoint=cloud_api_endpoint, headers=api_headers_dict) # EXPLICIT INIT\n",
    "    print(\"Phoenix client initialized successfully using explicit arguments.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing Phoenix Client explicitly: {e}\")\n",
    "    print(\"Check endpoint URL and header format/value in your .env file.\")\n",
    "    # px_client remains None\n",
    "# --- End Initialization ---\n",
    "\n",
    "# --- Initialize Judge LLM ---\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"WARNING: OPENAI_API_KEY not found in environment. Evaluation LLM might fail.\")\n",
    "\n",
    "print(\"\\nInitializing evaluation LLM (GPT-4o)...\")\n",
    "eval_model = None # Initialize to None\n",
    "try:\n",
    "    eval_model = OpenAIModel(model=\"gpt-4o\")\n",
    "    print(\"Evaluation LLM initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAIModel: {e}\")\n",
    "    # eval_model remains None\n",
    "# --- End Judge LLM Init ---\n",
    "\n",
    "print(\"\\n--- Setup Cell Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Evaluation Dataset\n",
    "\n",
    "Retrieve the specific `transcript-agent-eval-data-...` dataset previously uploaded to Phoenix. We need this dataset object to pass to the `run_experiment` function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load dataset 'transcript-agent-eval-data-20250428-102511'...\n",
      "Dataset loaded successfully.\n",
      "Number of examples in dataset: 17\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Load Dataset ---\n",
    "\n",
    "# Exact dataset name identified from the parse_spans.ipynb notebook output\n",
    "dataset_name = \"transcript-agent-eval-data-20250428-102511\"\n",
    "\n",
    "print(f\"Attempting to load dataset '{dataset_name}'...\")\n",
    "\n",
    "# Load the specified dataset by its exact name\n",
    "# This will raise an error if the dataset doesn't exist or px_client isn't initialized\n",
    "evaluation_dataset = px_client.get_dataset(name=dataset_name)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Print number of examples\n",
    "print(f\"Number of examples in dataset: {len(evaluation_dataset)}\")\n",
    "if len(evaluation_dataset) != 17:\n",
    "     print(f\"Warning: Dataset contains {len(evaluation_dataset)} examples, but we expected 17 based on UI/previous note.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded dataset object: <class 'phoenix.experiments.types.Dataset'>\n",
      "\n",
      "--- First Example ---\n",
      "\n",
      "Input Data:\n",
      "{\n",
      "  \"tool_called\": true,\n",
      "  \"user_query\": \"Who is Jeff Pidcock?\",\n",
      "  \"generated_sql\": \"SELECT * FROM transcript_segments WHERE text LIKE '%Jeff Pidcock%'\",\n",
      "  \"final_answer\": \"I cannot answer the question about who Jeff Pidcock is based on the available transcript data.\"\n",
      "}\n",
      "\n",
      "Output/Expected Data:\n",
      "{\n",
      "  \"tool_usage_explanation\": \"The agent correctly identified that answering this question requires querying the database to find mentions of the name.\",\n",
      "  \"sql_correctness_label\": \"Incorrect\",\n",
      "  \"tool_usage_correctness_label\": \"Correct\",\n",
      "  \"final_answer_quality_label\": \"Fail\",\n",
      "  \"sql_correctness_explanation\": \"The specific SQL query (LIKE '%Jeff Pidcock%') failed functionally. It did not retrieve the existing mention of \\\"Jeff Pidcock\\\" from the transcript, most likely due to case sensitivity, making it an incorrect implementation for the task.\",\n",
      "  \"final_answer_explanation\": \"The agent provided a factually incorrect final answer (\\\"I cannot answer...\\\") because the underlying SQL query failed to retrieve the available information from the database.\"\n",
      "}\n",
      "\n",
      "Metadata:\n",
      "{\n",
      "  \"llm_span_idx\": 0,\n",
      "  \"root_span_idx\": 6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Inspect Loaded Dataset ---\n",
    "\n",
    "print(f\"Type of loaded dataset object: {type(evaluation_dataset)}\")\n",
    "\n",
    "# Display the first example to check structure\n",
    "if len(evaluation_dataset) > 0:\n",
    "    print(\"\\n--- First Example ---\")\n",
    "    first_example = evaluation_dataset[0]\n",
    "\n",
    "    print(\"\\nInput Data:\")\n",
    "    # Assumes first_example.input exists and is dict-like\n",
    "    print(json.dumps(first_example.input, indent=2))\n",
    "\n",
    "    print(\"\\nOutput/Expected Data:\")\n",
    "    # Assumes first_example.output exists and is dict-like\n",
    "    print(json.dumps(first_example.output, indent=2))\n",
    "\n",
    "    print(\"\\nMetadata:\")\n",
    "    # Assumes first_example.metadata exists and is dict-like\n",
    "    print(json.dumps(first_example.metadata, indent=2))\n",
    "\n",
    "else:\n",
    "    print(\"Dataset appears to be empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Task Function\n",
    "\n",
    "The Phoenix `run_experiment` function is designed to run a specific \"task\" (like executing our agent) for each example in a dataset and then evaluate the result.\n",
    "\n",
    "In our case, we've already run the agent and processed its results into our datase|t (in the `input` fields like `final_answer`, `generated_sql`, etc.). However, the `run_experiment` function still requires *some* function to be passed as the \"task\".\n",
    "\n",
    "So, we'll define a very simple \"dummy\" task function. Its only job is to take the `input` data provided for each example in our dataset and return it directly. This satisfies the structural requirement of `run_experiment` without re-running our agent. The evaluators we define later will then use this returned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dummy_task_function with the first example:\n",
      "Output from dummy task:\n",
      "{\n",
      "  \"tool_called\": true,\n",
      "  \"user_query\": \"Who is Jeff Pidcock?\",\n",
      "  \"generated_sql\": \"SELECT * FROM transcript_segments WHERE text LIKE '%Jeff Pidcock%'\",\n",
      "  \"final_answer\": \"I cannot answer the question about who Jeff Pidcock is based on the available transcript data.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Define Task Function ---\n",
    "from phoenix.experiments.types import Example\n",
    "\n",
    "def dummy_task_function(example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    This function acts as the 'task' for run_experiment.\n",
    "    Since our agent outputs are already pre-computed and stored in the\n",
    "    dataset's 'input' fields, this function simply returns that input data.\n",
    "    The evaluators will receive this dictionary as their 'output' parameter.\n",
    "    \"\"\"\n",
    "    # The input attribute of the Example object holds the dictionary\n",
    "    # containing user_query, final_answer, generated_sql, tool_called.\n",
    "    return example.input\n",
    "\n",
    "# --- Quick test of the function ---\n",
    "if len(evaluation_dataset) > 0:\n",
    "    print(\"Testing dummy_task_function with the first example:\")\n",
    "    test_output = dummy_task_function(evaluation_dataset[0])\n",
    "    print(\"Output from dummy task:\")\n",
    "    print(json.dumps(test_output, indent=2))\n",
    "else:\n",
    "    print(\"Skipping test, dataset is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluators\n",
    "\n",
    "Now we define the functions that will evaluate the agent's performance for each example. We will use the `phoenix.evals.llm_classify` function to create LLM-as-judge evaluators for our three criteria: Tool Usage Correctness, SQL Correctness, and Final Answer Quality.\n",
    "\n",
    "**Design Approach:**\n",
    "\n",
    "We'll build each evaluator sequentially using a \"Recipe & Chef\" analogy:\n",
    "\n",
    "1.  **Define the Prompt Template (The Recipe):** For each criterion, we'll first write the detailed instructions (the prompt template) telling the LLM *how* to perform the specific evaluation.\n",
    "2.  **Define the Evaluator Function (The Chef):** Next, we'll create the Python function (the evaluator) that takes the data for an example, uses the corresponding prompt template (recipe), and manages the call to the LLM (the worker) via `llm_classify`.\n",
    "3.  **Test the Evaluator:** We'll run a quick test on the first example to ensure the evaluator function works as expected.\n",
    "\n",
    "We will repeat this Prompt -> Function -> Test sequence for each of our three evaluation criteria:\n",
    "\n",
    "1.  **Tool Usage Correctness:** Was the decision to call the SQL tool (or not) appropriate?\n",
    "2.  **SQL Correctness:** If the SQL tool was called, was the generated SQL query correct and effective?\n",
    "3.  **Final Answer Quality:** Was the final text answer provided to the user clear, correct, and relevant?\n",
    "\n",
    "Each evaluator function will ultimately return a score (e.g., 1.0 for success, 0.0 for failure) based on the LLM judge's assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Usage Prompt Template defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 4a. Prompt Template: Tool Usage Correctness ---\n",
    "\n",
    "TOOL_USAGE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are evaluating an AI agent's decision on whether to use a specific tool ('query_database') to answer a user's query about a workshop transcript.\n",
    "The agent has access to a database table 'transcript_segments'.\n",
    "\n",
    "**Instructions:**\n",
    "1. Analyze the User Query.\n",
    "2. Analyze the Agent's Action: Did the agent call the 'query_database' tool? (indicated by 'tool_called' flag and presence/absence of 'generated_sql').\n",
    "3. Determine if the Agent's Action was Correct: Should the agent have used the tool to answer this query effectively? Consider if the query asks for specific factual information likely in the transcript vs. general knowledge or conversational elements.\n",
    "4. Compare your assessment to the Human Label and Explanation provided (for context, but make your own judgment).\n",
    "5. Output a final LABEL ('Correct' or 'Incorrect') based *only* on your assessment of the agent's action.\n",
    "6. Provide a detailed EXPLANATION for your label, referencing the query and the agent's action.\n",
    "\n",
    "**Input Data:**\n",
    "User Query: {user_query}\n",
    "Agent Called Tool ('query_database'): {tool_called}\n",
    "Agent Generated SQL (if tool called): {generated_sql}\n",
    "\n",
    "**Reference (Human Annotation):**\n",
    "Human Label: {tool_usage_correctness_label}\n",
    "Human Explanation: {tool_usage_explanation}\n",
    "\n",
    "**Your Task:**\n",
    "Based *only* on the User Query and the Agent's Action, was the decision to use (or not use) the 'query_database' tool correct?\n",
    "\n",
    "EXPLANATION: [Provide your reasoning here]\n",
    "LABEL: [Correct or Incorrect]\n",
    "\"\"\"\n",
    "\n",
    "print(\"Tool Usage Prompt Template defined.\")\n",
    "# print(TOOL_USAGE_PROMPT_TEMPLATE) # Optional: uncomment to view the template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator function 'evaluate_tool_usage' defined (updated to accept model).\n"
     ]
    }
   ],
   "source": [
    "    # --- 4b. Evaluator Function: Tool Usage Correctness ---\n",
    "\n",
    "    def evaluate_tool_usage(output: dict, expected: dict, input: dict, model_to_use: OpenAIModel) -> float:\n",
    "        \"\"\"\n",
    "        Evaluates Tool Usage Correctness using LLM-as-judge based on TOOL_USAGE_PROMPT_TEMPLATE.\n",
    "\n",
    "        Args:\n",
    "            output (dict): The dictionary returned by the dummy_task_function.\n",
    "                           Contains 'user_query', 'generated_sql', 'tool_called'.\n",
    "            expected (dict): The dictionary containing the expected outputs (human labels/explanations).\n",
    "                             Contains 'tool_usage_correctness_label', 'tool_usage_explanation'.\n",
    "            input (dict): The dictionary containing the original input keys.\n",
    "            model_to_use (OpenAIModel): The initialized OpenAIModel instance for the judge.\n",
    "\n",
    "        Returns:\n",
    "            float: Score (1.0 for Correct, 0.0 for Incorrect based on LLM judge).\n",
    "                   Returns 0.0 if evaluation fails or label is missing from LLM response.\n",
    "        \"\"\"\n",
    "        # Prepare data for the prompt template\n",
    "        user_query = output.get('user_query')\n",
    "        tool_called = output.get('tool_called')\n",
    "        generated_sql = output.get('generated_sql', 'N/A') # Use N/A if None\n",
    "        human_label = expected.get('tool_usage_correctness_label')\n",
    "        human_explanation = expected.get('tool_usage_explanation')\n",
    "\n",
    "        # Check if essential inputs for the LLM are present\n",
    "        if user_query is None or tool_called is None:\n",
    "             print(f\"Warning: Missing essential input (query or tool_called) for Tool Usage eval. Returning 0.0\")\n",
    "             return 0.0\n",
    "\n",
    "        # Create DataFrame for llm_classify (needs dicts)\n",
    "        eval_df = pd.DataFrame([{\n",
    "            \"user_query\": user_query,\n",
    "            \"tool_called\": tool_called,\n",
    "            \"generated_sql\": generated_sql,\n",
    "            \"tool_usage_correctness_label\": human_label if human_label is not None else \"N/A\",\n",
    "            \"tool_usage_explanation\": human_explanation if human_explanation is not None else \"N/A\"\n",
    "        }])\n",
    "\n",
    "        # Removed the check for eval_model in locals()\n",
    "\n",
    "        # Call LLM judge using the template defined previously, passing the specific model\n",
    "        response = llm_classify(\n",
    "            data=eval_df,\n",
    "            template=TOOL_USAGE_PROMPT_TEMPLATE, # Uses the variable defined earlier\n",
    "            model=model_to_use, # Use the passed-in model\n",
    "            rails=[\"Correct\", \"Incorrect\"], # Expected LLM output labels\n",
    "            provide_explanation=True\n",
    "        )\n",
    "\n",
    "        # Extract the label assigned by the LLM judge\n",
    "        try:\n",
    "            llm_label = response['label'].iloc[0]\n",
    "            score = 1.0 if llm_label == 'Correct' else 0.0\n",
    "            return score\n",
    "        except (IndexError, KeyError, TypeError) as e:\n",
    "             print(f\"Error parsing LLM response for Tool Usage: {e}. Response: {response}\")\n",
    "             return 0.0 # Score as incorrect if LLM response is malformed\n",
    "\n",
    "    print(\"Evaluator function 'evaluate_tool_usage' defined (updated to accept model).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Tool Usage evaluator with the first example:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4652c3336304b55840e9469d86ae6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Judge Score for Tool Usage (First Example): 0.0\n",
      "(Score reflects LLM judgment: 1.0 for 'Correct', 0.0 for 'Incorrect')\n"
     ]
    }
   ],
   "source": [
    "# --- 4c. Test: Tool Usage Evaluator ---\n",
    "\n",
    "# Ensure the dataset object exists and has examples\n",
    "if 'evaluation_dataset' in locals() and len(evaluation_dataset) > 0 and 'eval_model' in locals():\n",
    "    print(\"Testing Tool Usage evaluator with the first example:\")\n",
    "    # Get the necessary parts from the first example\n",
    "    first_example = evaluation_dataset[0]\n",
    "    test_output = dummy_task_function(first_example) # Use dummy task to get 'output' dict\n",
    "    test_expected = first_example.output # Ground truth labels/explanations\n",
    "    test_input = first_example.input # Contains original inputs\n",
    "\n",
    "    # Call the evaluator function (Corrected Line Below - passing model)\n",
    "    try:\n",
    "        # Ensure eval_model is passed to the updated function\n",
    "        score = evaluate_tool_usage(output=test_output,\n",
    "                                    expected=test_expected,\n",
    "                                    input=test_input,\n",
    "                                    model_to_use=eval_model) # Pass eval_model here\n",
    "        print(f\"LLM Judge Score for Tool Usage (First Example): {score}\")\n",
    "        print(\"(Score reflects LLM judgment: 1.0 for 'Correct', 0.0 for 'Incorrect')\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the evaluate_tool_usage test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "    # You can also inspect the individual components passed to the evaluator\n",
    "    # print(\"\\nData passed to evaluator:\")\n",
    "    # print(\"Output (from dummy task):\", json.dumps(test_output, indent=2))\n",
    "    # print(\"Expected (human labels):\", json.dumps(test_expected, indent=2))\n",
    "    # print(\"Input (original):\", json.dumps(test_input, indent=2))\n",
    "\n",
    "elif 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "    print(\"Skipping test, evaluation_dataset not loaded or is empty.\")\n",
    "else: # This means eval_model is missing\n",
    "    print(\"Skipping test, eval_model not found. Ensure the Setup cell was run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting Tool Usage LLM response for first 5 examples...\n",
      "\n",
      "--- Processing Example 0 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ed33ab662b4c1288e873a1b5f31793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 0: Score=0.0, LLM Explanation: The user query asks for information about 'Jeff Pidcock'. This is a specific factual query that likely requires information from the workshop transcript to answer accurately. The agent's decision to use the 'query_database' tool is appropriate because it allows the agent to search the transcript for mentions of 'Jeff Pidcock' and provide a precise answer based on the content of the transcript. The generated SQL query is correctly formulated to search for any segments in the transcript that mention 'Jeff Pidcock'. Therefore, the agent's action to call the tool is correct.\n",
      "\n",
      "--- Processing Example 1 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3aa2e6ec5b54281997af648e1aa9aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 1: Score=0.0, LLM Explanation: The user query specifically asks for what Stefan Krawczyk said during his introduction. This is a request for specific factual information that would be found in the transcript of the workshop. The agent correctly decided to use the 'query_database' tool to retrieve this information, as it involves searching for a specific speaker and context within the transcript. The generated SQL query is appropriately designed to find segments where Stefan Krawczyk is the speaker and the content is related to his introduction. Therefore, the agent's action to call the tool was correct.\n",
      "\n",
      "--- Processing Example 2 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20c232b407f4258978230d5b0d7fec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 2: Score=0.0, LLM Explanation: The user query asks for a list of all unique speakers mentioned in the workshop transcript. This is a request for specific factual information that is likely stored in the 'transcript_segments' database table. The agent correctly decided to use the 'query_database' tool to retrieve this information. The generated SQL query, 'SELECT DISTINCT speaker FROM transcript_segments', is appropriate for obtaining a list of unique speakers, as it selects distinct speaker names from the table. Therefore, the agent's action to call the tool and the SQL query generated are both correct for answering the user's query.\n",
      "\n",
      "--- Processing Example 3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ef949f87894c32a0a5700a8c7f28fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 3: Score=0.0, LLM Explanation: The user query asks for the total number of words spoken by a specific speaker, Hugo, in the workshop transcript. This is a factual query that requires specific data from the transcript, specifically the aggregation of word counts for Hugo. The agent correctly decided to use the 'query_database' tool to retrieve this information, as it involves summing up the word counts from the database table 'transcript_segments' where the speaker is Hugo. The generated SQL query is appropriate for this task, as it selects the sum of the word counts for the specified speaker. Therefore, the agent's action to call the tool was correct.\n",
      "\n",
      "--- Processing Example 4 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83aa60a7ed746c59d1bd61e521a200d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 4: Score=0.0, LLM Explanation: The user query specifically requests to find segments in a transcript that mention the word 'evaluation' and to provide the timestamps for these segments. This is a request for specific factual information that is likely stored in the 'transcript_segments' database table. The agent correctly called the 'query_database' tool and generated an appropriate SQL query to retrieve the relevant data. The SQL query is designed to search for the keyword 'evaluation' in the text of the transcript segments and return the start and end times along with the text, which aligns perfectly with the user's request. Therefore, the agent's action to use the tool was appropriate and necessary to fulfill the query.\n",
      "\n",
      "--- End Inspection ---\n"
     ]
    }
   ],
   "source": [
    "# --- Introspect Tool Usage on First 5 Examples (Show Explanations) ---\n",
    "import pandas as pd # Make sure pandas is imported if not already\n",
    "\n",
    "num_examples_to_test = 5\n",
    "print(f\"Inspecting Tool Usage LLM response for first {num_examples_to_test} examples...\")\n",
    "\n",
    "if 'evaluation_dataset' in locals() and len(evaluation_dataset) > 0 and 'eval_model' in locals():\n",
    "    for i in range(min(num_examples_to_test, len(evaluation_dataset))):\n",
    "        print(f\"\\n--- Processing Example {i} ---\")\n",
    "        example = evaluation_dataset[i]\n",
    "        output_data = dummy_task_function(example) # Agent's output from dataset\n",
    "        expected_data = example.output           # Human labels from dataset\n",
    "        input_data = example.input               # Original input query etc.\n",
    "\n",
    "        # Prepare data for llm_classify (similar to inside evaluate_tool_usage)\n",
    "        user_query = output_data.get('user_query')\n",
    "        tool_called = output_data.get('tool_called')\n",
    "        generated_sql = output_data.get('generated_sql', 'N/A')\n",
    "        human_label = expected_data.get('tool_usage_correctness_label', 'N/A')\n",
    "        human_explanation = expected_data.get('tool_usage_explanation', 'N/A')\n",
    "\n",
    "        if user_query is None or tool_called is None:\n",
    "             print(f\"  Skipping Example {i}: Missing essential input (query or tool_called).\")\n",
    "             continue\n",
    "\n",
    "        eval_df = pd.DataFrame([{\n",
    "            \"user_query\": user_query,\n",
    "            \"tool_called\": tool_called,\n",
    "            \"generated_sql\": generated_sql,\n",
    "            \"tool_usage_correctness_label\": human_label,\n",
    "            \"tool_usage_explanation\": human_explanation\n",
    "        }])\n",
    "\n",
    "        try:\n",
    "            # Call llm_classify directly HERE within the test cell\n",
    "            response_df = llm_classify(\n",
    "                data=eval_df,\n",
    "                template=TOOL_USAGE_PROMPT_TEMPLATE, # Use the existing template\n",
    "                model=eval_model,                   # Use the existing model\n",
    "                rails=[\"Correct\", \"Incorrect\"],\n",
    "                provide_explanation=True\n",
    "            )\n",
    "\n",
    "            # Extract score AND explanation from the response DataFrame\n",
    "            llm_label = response_df['label'].iloc[0]\n",
    "            explanation = response_df['explanation'].iloc[0]\n",
    "            score = 1.0 if llm_label == 'Correct' else 0.0\n",
    "\n",
    "            print(f\"  Example {i}: Score={score}, LLM Explanation: {explanation}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Example {i}: ERROR during llm_classify call: {e}\")\n",
    "            # import traceback\n",
    "            # traceback.print_exc() # Uncomment for full error details if needed\n",
    "\n",
    "else:\n",
    "    print(\"Skipping test - dataset or eval_model not loaded.\")\n",
    "\n",
    "print(\"\\n--- End Inspection ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised Tool Usage Prompt Template defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Revised Prompt: Tool Usage Correctness ---\n",
    "\n",
    "REVISED_TOOL_USAGE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are evaluating an AI agent's decision on whether to use a specific tool ('query_database') to answer a user's query about a workshop transcript.\n",
    "The agent has access to a database table 'transcript_segments'.\n",
    "\n",
    "**Instructions:**\n",
    "1. Analyze the User Query: What information is the user asking for?\n",
    "2. Analyze the Agent's Action: Did the agent call the 'query_database' tool? (indicated by 'tool_called' flag).\n",
    "3. Determine if the Agent's Action was Correct: Based ONLY on the User Query, should the agent have used the 'query_database' tool to answer effectively?\n",
    "    - 'Correct': Tool usage is appropriate if the query asks for specific factual information likely only found within the transcript data.\n",
    "    - 'Incorrect': Tool usage is inappropriate if the query is conversational, asks for general knowledge, or could be answered without accessing the transcript data.\n",
    "\n",
    "**Input Data:**\n",
    "User Query: {user_query}\n",
    "Agent Called Tool ('query_database'): {tool_called}\n",
    "Agent Generated SQL (if tool called): {generated_sql}\n",
    "\n",
    "**Your Task:**\n",
    "Based *only* on the User Query and the Agent's Action, was the decision to use (or not use) the 'query_database' tool correct?\n",
    "\n",
    "EXPLANATION: [Provide your reasoning here, focusing only on the query and the agent's action.]\n",
    "LABEL: [Correct or Incorrect]\n",
    "\"\"\"\n",
    "\n",
    "print(\"Revised Tool Usage Prompt Template defined.\")\n",
    "# print(REVISED_TOOL_USAGE_PROMPT_TEMPLATE) # Optional: uncomment to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Tool Usage with REVISED prompt for first 5 examples...\n",
      "\n",
      "--- Processing Example 0 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13099a55abca44d19131700f662622a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 0: Score=0.0, LLM Explanation: The user query asks for information about 'Jeff Pidcock'. This is a specific factual query that likely requires accessing the transcript data to find relevant information about this individual. The agent's decision to use the 'query_database' tool to search for mentions of 'Jeff Pidcock' in the transcript is appropriate, as it is the most direct way to obtain accurate and specific information about him from the workshop transcript.\n",
      "\n",
      "--- Processing Example 1 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e928bd9b42d4cfdbfe13560c2663eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 1: Score=0.0, LLM Explanation: The user query specifically asks for what Stefan Krawczyk said during his introduction. This is a request for specific factual information that would be found in the transcript data. The agent's decision to use the 'query_database' tool to retrieve this information is appropriate, as it allows the agent to access the exact words spoken by Stefan Krawczyk during his introduction, which is likely stored in the transcript segments.\n",
      "\n",
      "--- Processing Example 2 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2aaba738ca4d5eb7e2259eb3e50ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 2: Score=0.0, LLM Explanation: The user query asks for a list of all unique speakers mentioned in the workshop transcript. This is a specific factual request that requires accessing the transcript data to identify and list the unique speakers. The agent's decision to call the 'query_database' tool and execute a SQL query to retrieve distinct speaker names from the 'transcript_segments' table is appropriate and necessary to fulfill the user's request. Therefore, the agent's action to use the tool is correct.\n",
      "\n",
      "--- Processing Example 3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f295a4d9a4424591311a11ede401e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 3: Score=0.0, LLM Explanation: The user query asks for the total number of words spoken by a specific individual, Hugo, in a workshop transcript. This is a specific factual question that requires accessing the transcript data to calculate the total word count for Hugo. The agent's decision to use the 'query_database' tool is appropriate because the information needed to answer the query is likely stored in the database table 'transcript_segments'. Therefore, the agent's action to call the tool and generate the SQL query to sum the word count for Hugo is correct.\n",
      "\n",
      "--- Processing Example 4 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7611ca48f754c26ac880229b52edfc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 4: Score=0.0, LLM Explanation: The user query specifically asks for segments mentioning 'evaluation' along with their timestamps. This is a request for specific factual information that is likely only available in the transcript data. The agent's decision to use the 'query_database' tool to retrieve this information is appropriate, as it allows the agent to search the transcript segments for mentions of 'evaluation' and provide the corresponding timestamps, which cannot be answered without accessing the database.\n",
      "\n",
      "--- End Revised Prompt Test ---\n"
     ]
    }
   ],
   "source": [
    "# --- Test Tool Usage on First 5 Examples (Using REVISED Prompt) ---\n",
    "import pandas as pd # Ensure pandas is imported\n",
    "\n",
    "num_examples_to_test = 5\n",
    "print(f\"Testing Tool Usage with REVISED prompt for first {num_examples_to_test} examples...\")\n",
    "\n",
    "if 'evaluation_dataset' in locals() and len(evaluation_dataset) > 0 and 'eval_model' in locals() and 'REVISED_TOOL_USAGE_PROMPT_TEMPLATE' in locals():\n",
    "    for i in range(min(num_examples_to_test, len(evaluation_dataset))):\n",
    "        print(f\"\\n--- Processing Example {i} ---\")\n",
    "        example = evaluation_dataset[i]\n",
    "        output_data = dummy_task_function(example)\n",
    "        expected_data = example.output # Still needed if you want to compare later\n",
    "        input_data = example.input\n",
    "\n",
    "        # Prepare data for llm_classify\n",
    "        user_query = output_data.get('user_query')\n",
    "        tool_called = output_data.get('tool_called')\n",
    "        generated_sql = output_data.get('generated_sql', 'N/A')\n",
    "        # Note: We don't need human labels for the prompt input anymore, but keep for potential comparison\n",
    "        human_label = expected_data.get('tool_usage_correctness_label', 'N/A')\n",
    "        human_explanation = expected_data.get('tool_usage_explanation', 'N/A')\n",
    "\n",
    "\n",
    "        if user_query is None or tool_called is None:\n",
    "             print(f\"  Skipping Example {i}: Missing essential input.\")\n",
    "             continue\n",
    "\n",
    "        # DataFrame still includes human labels, though not used in revised prompt\n",
    "        eval_df = pd.DataFrame([{\n",
    "            \"user_query\": user_query,\n",
    "            \"tool_called\": tool_called,\n",
    "            \"generated_sql\": generated_sql,\n",
    "            \"tool_usage_correctness_label\": human_label,\n",
    "            \"tool_usage_explanation\": human_explanation\n",
    "        }])\n",
    "\n",
    "        try:\n",
    "            # Call llm_classify directly using the REVISED template\n",
    "            response_df = llm_classify(\n",
    "                data=eval_df,\n",
    "                template=REVISED_TOOL_USAGE_PROMPT_TEMPLATE, # Use the new template variable\n",
    "                model=eval_model,\n",
    "                rails=[\"Correct\", \"Incorrect\"],\n",
    "                provide_explanation=True\n",
    "            )\n",
    "\n",
    "            llm_label = response_df['label'].iloc[0]\n",
    "            explanation = response_df['explanation'].iloc[0]\n",
    "            score = 1.0 if llm_label == 'Correct' else 0.0\n",
    "\n",
    "            print(f\"  Example {i}: Score={score}, LLM Explanation: {explanation}\")\n",
    "            # You could add a comparison here if desired:\n",
    "            # print(f\"    (Human Label was: {human_label})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Example {i}: ERROR during llm_classify call: {e}\")\n",
    "\n",
    "else:\n",
    "    missing = []\n",
    "    if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "        missing.append(\"dataset\")\n",
    "    if 'eval_model' not in locals():\n",
    "        missing.append(\"eval_model\")\n",
    "    if 'REVISED_TOOL_USAGE_PROMPT_TEMPLATE' not in locals():\n",
    "        missing.append(\"revised prompt template\")\n",
    "    print(f\"Skipping test - required components not loaded: {', '.join(missing)}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- End Revised Prompt Test ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempting Direct LLM Call for Tool Usage Evaluation\n",
    "\n",
    "The previous attempts using `phoenix.evals.llm_classify` with various prompt refinements (`TOOL_USAGE_PROMPT_TEMPLATE`, `REVISED_TOOL_USAGE_PROMPT_TEMPLATE`, `REVISED_TOOL_USAGE_PROMPT_TEMPLATE_V2`) consistently produced contradictory results. The LLM's generated explanations indicated correct reasoning about tool usage appropriateness, but the final classification label forced by the `rails=[\"Correct\", \"Incorrect\"]` parameter was persistently 'Incorrect' (Score=0.0).\n",
    "\n",
    "This suggests a potential issue either with how `llm_classify` handles the rails in conjunction with the explanation for this specific task, or a deeper problem with the LLM's ability to follow the structured output format reliably under these conditions.\n",
    "\n",
    "To isolate the problem, we will now bypass `llm_classify` and directly query the evaluation LLM (`gpt-4o`) using the `REVISED_TOOL_USAGE_PROMPT_TEMPLATE_V2`. We will manually inspect the raw output to see if the explanation and the final label align when generated without the constraints of the `llm_classify` framework. This will help determine if the core LLM can perform the task correctly when called directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing DIRECT OpenAI API call test (Example 0, V1 Prompt)...\n",
      "OpenAI client initialized for model: gpt-4o\n",
      "\n",
      "--- Prompt Sent to OpenAI API (V1) ---\n",
      "\n",
      "You are evaluating an AI agent's decision on whether to use a specific tool ('query_database') to answer a user's query about a workshop transcript.\n",
      "The agent has access to a database table 'transcript_segments'.\n",
      "\n",
      "**Instructions:**\n",
      "1. Analyze the User Query: What information is the user asking for?\n",
      "2. Analyze the Agent's Action: Did the agent call the 'query_database' tool? (indicated by 'tool_called' flag).\n",
      "3. Determine if the Agent's Action was Correct: Based ONLY on the User Query, should the agent have used the 'query_database' tool to answer effectively?\n",
      "    - 'Correct': Tool usage is appropriate if the query asks for specific factual information likely only found within the transcript data.\n",
      "    - 'Incorrect': Tool usage is inappropriate if the query is conversational, asks for general knowledge, or could be answered without accessing the transcript data.\n",
      "\n",
      "**Input Data:**\n",
      "User Query: Who is Jeff Pidcock?\n",
      "Agent Called Tool ('query_database'): True\n",
      "Agent Generated SQL (if tool called): SELECT * FROM transcript_segments WHERE text LIKE '%Jeff Pidcock%'\n",
      "\n",
      "**Your Task:**\n",
      "Based *only* on the User Query and the Agent's Action, was the decision to use (or not use) the 'query_database' tool correct?\n",
      "\n",
      "EXPLANATION: [Provide your reasoning here, focusing only on the query and the agent's action.]\n",
      "LABEL: [Correct or Incorrect]\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "--- Raw OpenAI API Response ---\n",
      "EXPLANATION: The user query \"Who is Jeff Pidcock?\" is asking for specific information about an individual named Jeff Pidcock. This type of query typically requires factual information that might be found in a transcript if Jeff Pidcock was mentioned or discussed during the workshop. The agent's decision to use the 'query_database' tool to search for occurrences of \"Jeff Pidcock\" in the transcript segments is appropriate because it aims to retrieve specific details about him that are likely contained within the transcript data. Therefore, the agent's action aligns with the need to access specific factual information from the database.\n",
      "\n",
      "LABEL: Correct\n",
      "-----------------------------\n",
      "\n",
      "--- Parsed Output (V1 Prompt) ---\n",
      "Extracted Explanation: The user query \"Who is Jeff Pidcock?\" is asking for specific information about an individual named Jeff Pidcock. This type of query typically requires factual information that might be found in a transcript if Jeff Pidcock was mentioned or discussed during the workshop. The agent's decision to use the 'query_database' tool to search for occurrences of \"Jeff Pidcock\" in the transcript segments is appropriate because it aims to retrieve specific details about him that are likely contained within the transcript data. Therefore, the agent's action aligns with the need to access specific factual information from the database.\n",
      "Extracted Label: Correct\n",
      "---------------------------------\n",
      "\n",
      ">>> Please manually check if the Extracted Label ('Correct'/'Incorrect') logically follows the Extracted Explanation.\n",
      "    (For reference, Human Label was: Correct)\n",
      "\n",
      "--- End Direct OpenAI API Call Test ---\n"
     ]
    }
   ],
   "source": [
    "# --- Direct OpenAI API Call Test for Tool Usage (Example 0, V1 Prompt) ---\n",
    "# Bypasses Phoenix entirely. Requires 'openai' library and OPENAI_API_KEY env var.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"Performing DIRECT OpenAI API call test (Example 0, V1 Prompt)...\")\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_TO_USE = \"gpt-4o\" # Specify the model\n",
    "# --- End Configuration ---\n",
    "\n",
    "# Ensure necessary components are available (Dataset and Prompt V1)\n",
    "missing = []\n",
    "if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "    missing.append(\"evaluation_dataset\")\n",
    "if 'REVISED_TOOL_USAGE_PROMPT_TEMPLATE' not in locals():\n",
    "    missing.append(\"REVISED_TOOL_USAGE_PROMPT_TEMPLATE (V1)\")\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    missing.append(\"OPENAI_API_KEY environment variable\")\n",
    "\n",
    "if not missing:\n",
    "    # Initialize OpenAI client directly\n",
    "    try:\n",
    "        client = OpenAI()\n",
    "        print(f\"OpenAI client initialized for model: {MODEL_TO_USE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing OpenAI client: {e}\")\n",
    "        client = None\n",
    "\n",
    "    if client:\n",
    "        example_index = 0\n",
    "        example = evaluation_dataset[example_index]\n",
    "        # Get input data robustly\n",
    "        if 'dummy_task_function' in locals():\n",
    "            output_data = dummy_task_function(example)\n",
    "        elif hasattr(example, 'input'):\n",
    "             output_data = example.input\n",
    "        else:\n",
    "            print(f\"  Skipping Example {example_index}: Cannot access input data.\")\n",
    "            output_data = None\n",
    "\n",
    "        if output_data:\n",
    "            expected_data = example.output if hasattr(example, 'output') else {}\n",
    "\n",
    "            # Prepare data\n",
    "            user_query = output_data.get('user_query')\n",
    "            tool_called = output_data.get('tool_called')\n",
    "            generated_sql = output_data.get('generated_sql', 'N/A')\n",
    "            human_label = expected_data.get('tool_usage_correctness_label', 'N/A')\n",
    "\n",
    "            if user_query is None or tool_called is None:\n",
    "                 print(f\"  Skipping Example {example_index}: Missing essential input fields (user_query or tool_called).\")\n",
    "            else:\n",
    "                # Format the V1 prompt\n",
    "                formatted_prompt = REVISED_TOOL_USAGE_PROMPT_TEMPLATE.format(\n",
    "                    user_query=user_query,\n",
    "                    tool_called=tool_called,\n",
    "                    generated_sql=generated_sql\n",
    "                )\n",
    "\n",
    "                print(\"\\n--- Prompt Sent to OpenAI API (V1) ---\")\n",
    "                print(formatted_prompt)\n",
    "                print(\"------------------------------------\")\n",
    "\n",
    "                try:\n",
    "                    # Make the direct API call\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=MODEL_TO_USE,\n",
    "                        messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "                        temperature=0.0,\n",
    "                        # max_tokens=250 # Optional: limit response length\n",
    "                    )\n",
    "                    raw_output = response.choices[0].message.content\n",
    "\n",
    "                    print(\"\\n--- Raw OpenAI API Response ---\")\n",
    "                    print(raw_output)\n",
    "                    print(\"-----------------------------\")\n",
    "\n",
    "                    # Simple parsing attempt\n",
    "                    explanation_match = re.search(r\"EXPLANATION:\\s*(.*?)\\s*LABEL:\", raw_output, re.DOTALL | re.IGNORECASE)\n",
    "                    label_match = re.search(r\"LABEL:\\s*(\\w+)\", raw_output, re.IGNORECASE)\n",
    "\n",
    "                    extracted_explanation = explanation_match.group(1).strip() if explanation_match else \"Parsing failed\"\n",
    "                    extracted_label = label_match.group(1).strip() if label_match else \"Parsing failed\"\n",
    "\n",
    "                    print(\"\\n--- Parsed Output (V1 Prompt) ---\")\n",
    "                    print(f\"Extracted Explanation: {extracted_explanation}\")\n",
    "                    print(f\"Extracted Label: {extracted_label}\")\n",
    "                    print(\"---------------------------------\")\n",
    "\n",
    "                    print(\"\\n>>> Please manually check if the Extracted Label ('Correct'/'Incorrect') logically follows the Extracted Explanation.\")\n",
    "                    print(f\"    (For reference, Human Label was: {human_label})\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n--- ERROR during OpenAI API call ---\")\n",
    "                    print(e)\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    print(\"------------------------------------\")\n",
    "else:\n",
    "    print(f\"Skipping test - required components not loaded: {', '.join(missing)}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- End Direct OpenAI API Call Test ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ALL examples via DIRECT OpenAI API call (No Progress Bar)...\n",
      "OpenAI client initialized for model: gpt-4o\n",
      "Processing 17 examples...\n",
      "Processing Example 0...\n",
      "Processing Example 1...\n",
      "Processing Example 2...\n",
      "Processing Example 3...\n",
      "Processing Example 4...\n",
      "Processing Example 5...\n",
      "Processing Example 6...\n",
      "Processing Example 7...\n",
      "Processing Example 8...\n",
      "Processing Example 9...\n",
      "Processing Example 10...\n",
      "Processing Example 11...\n",
      "Processing Example 12...\n",
      "Processing Example 13...\n",
      "Processing Example 14...\n",
      "Processing Example 15...\n",
      "Processing Example 16...\n",
      "\n",
      "Evaluation complete. Creating DataFrame...\n",
      "Direct API Evaluation Results (No Progress Bar):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>query</th>\n",
       "      <th>human_label</th>\n",
       "      <th>llm_label</th>\n",
       "      <th>llm_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Who is Jeff Pidcock?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query \"Who is Jeff Pidcock?\" is asking for specific information about an individual named Jeff Pidcock. This type of query typically requ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What did Stefan Krawczyk say during his introduction?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for what Stefan Krawczyk said during his introduction. This is a request for specific factual information that is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>List all unique speakers mentioned.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query asks for a list of all unique speakers mentioned in the workshop transcript. This is a request for specific factual information tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>How many words did Hugo speak in total?</td>\n",
       "      <td>Correcr</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query asks for a specific factual piece of information: the total number of words spoken by Hugo. This type of query requires accessing d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Find segments mentioning 'evaluation' and provide timestamps.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for segments mentioning the word 'evaluation' along with their timestamps. This request requires searching throug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Which speaker has the most segments?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query asks for specific factual information about which speaker has the most segments in a workshop transcript. This type of information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>What is the total word count for all segments combined?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Parsing failed</td>\n",
       "      <td>** The user query asks for the total word count for all segments combined. This is a specific factual request that requires aggregating data from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Who mentioned Carvana?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query asks for specific information about who mentioned \"Carvana\" in a workshop transcript. This type of query requires accessing the tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>List the builders in residence mentioned.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for a list of \"builders in residence\" mentioned, which is a request for specific factual information that is like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>When did Nathan Danielsen first speak?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query asks for specific factual information about when Nathan Danielsen first spoke during a workshop. This type of information is likely...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Summarize the key points about evaluation driven development.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query asks for a summary of the key points about \"evaluation driven development.\" This type of query is seeking specific factual informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Find mentions of 'non-determinism' and provide timestamps.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for mentions of 'non-determinism' along with their timestamps. This request is for specific factual information t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Did the transcript mention monitoring?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks whether the transcript mentioned \"monitoring.\" This is a request for specific factual information that would be f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Tell me about AI observability.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query \"Tell me about AI observability\" is asking for information about a specific topic, AI observability. This type of query suggests th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>What was said about Discord?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for information about what was said regarding \"Discord\" in a workshop transcript. This is a request for specific ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>How many times was 'Discord' mentioned?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for the number of times the word 'Discord' was mentioned in the workshop transcript. This is a request for specif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>What did they say about reinforcement learning?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for information about what was said regarding \"reinforcement learning\" in a workshop transcript. This is a reques...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                                          query  \\\n",
       "0       0                                           Who is Jeff Pidcock?   \n",
       "1       1          What did Stefan Krawczyk say during his introduction?   \n",
       "2       2                            List all unique speakers mentioned.   \n",
       "3       3                        How many words did Hugo speak in total?   \n",
       "4       4  Find segments mentioning 'evaluation' and provide timestamps.   \n",
       "5       5                           Which speaker has the most segments?   \n",
       "6       6        What is the total word count for all segments combined?   \n",
       "7       7                                         Who mentioned Carvana?   \n",
       "8       8                      List the builders in residence mentioned.   \n",
       "9       9                         When did Nathan Danielsen first speak?   \n",
       "10     10  Summarize the key points about evaluation driven development.   \n",
       "11     11     Find mentions of 'non-determinism' and provide timestamps.   \n",
       "12     12                         Did the transcript mention monitoring?   \n",
       "13     13                                Tell me about AI observability.   \n",
       "14     14                                   What was said about Discord?   \n",
       "15     15                        How many times was 'Discord' mentioned?   \n",
       "16     16                What did they say about reinforcement learning?   \n",
       "\n",
       "   human_label       llm_label  \\\n",
       "0      Correct         Correct   \n",
       "1      Correct         Correct   \n",
       "2      Correct         Correct   \n",
       "3      Correcr         Correct   \n",
       "4      Correct         Correct   \n",
       "5      Correct         Correct   \n",
       "6      Correct  Parsing failed   \n",
       "7      Correct         Correct   \n",
       "8      Correct         Correct   \n",
       "9      Correct         Correct   \n",
       "10     Correct         Correct   \n",
       "11     Correct         Correct   \n",
       "12     Correct         Correct   \n",
       "13     Correct         Correct   \n",
       "14     Correct         Correct   \n",
       "15     Correct         Correct   \n",
       "16     Correct         Correct   \n",
       "\n",
       "                                                                                                                                          llm_explanation  \n",
       "0   The user query \"Who is Jeff Pidcock?\" is asking for specific information about an individual named Jeff Pidcock. This type of query typically requ...  \n",
       "1   The user query specifically asks for what Stefan Krawczyk said during his introduction. This is a request for specific factual information that is...  \n",
       "2   The user query asks for a list of all unique speakers mentioned in the workshop transcript. This is a request for specific factual information tha...  \n",
       "3   The user query asks for a specific factual piece of information: the total number of words spoken by Hugo. This type of query requires accessing d...  \n",
       "4   The user query specifically asks for segments mentioning the word 'evaluation' along with their timestamps. This request requires searching throug...  \n",
       "5   The user query asks for specific factual information about which speaker has the most segments in a workshop transcript. This type of information ...  \n",
       "6   ** The user query asks for the total word count for all segments combined. This is a specific factual request that requires aggregating data from ...  \n",
       "7   The user query asks for specific information about who mentioned \"Carvana\" in a workshop transcript. This type of query requires accessing the tra...  \n",
       "8   The user query specifically asks for a list of \"builders in residence\" mentioned, which is a request for specific factual information that is like...  \n",
       "9   The user query asks for specific factual information about when Nathan Danielsen first spoke during a workshop. This type of information is likely...  \n",
       "10  The user query asks for a summary of the key points about \"evaluation driven development.\" This type of query is seeking specific factual informat...  \n",
       "11  The user query specifically asks for mentions of 'non-determinism' along with their timestamps. This request is for specific factual information t...  \n",
       "12  The user query specifically asks whether the transcript mentioned \"monitoring.\" This is a request for specific factual information that would be f...  \n",
       "13  The user query \"Tell me about AI observability\" is asking for information about a specific topic, AI observability. This type of query suggests th...  \n",
       "14  The user query specifically asks for information about what was said regarding \"Discord\" in a workshop transcript. This is a request for specific ...  \n",
       "15  The user query specifically asks for the number of times the word 'Discord' was mentioned in the workshop transcript. This is a request for specif...  \n",
       "16  The user query specifically asks for information about what was said regarding \"reinforcement learning\" in a workshop transcript. This is a reques...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Direct OpenAI API Full Evaluation (No Progress Bar) ---\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate ALL Examples Directly via OpenAI API (No Progress Bar) ---\n",
    "# Focuses on core logic, minimal error handling, NO tqdm dependency.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "# Removed: from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Evaluating ALL examples via DIRECT OpenAI API call (No Progress Bar)...\")\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_TO_USE = \"gpt-4o\"\n",
    "PROMPT_TEMPLATE = REVISED_TOOL_USAGE_PROMPT_TEMPLATE # Assumes V1 is defined\n",
    "# --- End Configuration ---\n",
    "\n",
    "# List to store results\n",
    "evaluation_results = []\n",
    "\n",
    "# Initialize OpenAI client (basic check)\n",
    "try:\n",
    "    client = OpenAI()\n",
    "    print(f\"OpenAI client initialized for model: {MODEL_TO_USE}\")\n",
    "except Exception as e:\n",
    "    print(f\"STOPPING: Failed to initialize OpenAI Client: {e}. Make sure OPENAI_API_KEY is set.\")\n",
    "    client = None # Ensure client is None if init fails\n",
    "\n",
    "if client and 'evaluation_dataset' in locals() and PROMPT_TEMPLATE:\n",
    "    print(f\"Processing {len(evaluation_dataset)} examples...\")\n",
    "\n",
    "    # Removed tqdm wrapper from the loop\n",
    "    for i, example in enumerate(evaluation_dataset):\n",
    "        print(f\"Processing Example {i}...\") # Simple print indicator instead of progress bar\n",
    "\n",
    "        # --- 1. Get Data ---\n",
    "        user_query = example.input.get('user_query', 'N/A')\n",
    "        tool_called = example.input.get('tool_called', None)\n",
    "        generated_sql = example.input.get('generated_sql', 'N/A')\n",
    "        human_label = example.output.get('tool_usage_correctness_label', 'N/A')\n",
    "\n",
    "        llm_label = \"Skipped\"\n",
    "        llm_explanation = \"Skipped due to missing input\"\n",
    "\n",
    "        if user_query != 'N/A' and tool_called is not None:\n",
    "            # --- 2. Format Prompt ---\n",
    "            prompt = PROMPT_TEMPLATE.format(\n",
    "                user_query=user_query,\n",
    "                tool_called=tool_called,\n",
    "                generated_sql=generated_sql\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # --- 3. Call API ---\n",
    "                response = client.chat.completions.create(\n",
    "                    model=MODEL_TO_USE,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.0,\n",
    "                )\n",
    "                raw_output = response.choices[0].message.content\n",
    "\n",
    "                # --- 4. Parse Result ---\n",
    "                explanation_match = re.search(r\"EXPLANATION:\\s*(.*?)\\s*LABEL:\", raw_output, re.DOTALL | re.IGNORECASE)\n",
    "                label_match = re.search(r\"LABEL:\\s*(\\w+)\", raw_output, re.IGNORECASE)\n",
    "\n",
    "                llm_explanation = explanation_match.group(1).strip() if explanation_match else \"Parsing failed\"\n",
    "                llm_label = label_match.group(1).strip().capitalize() if label_match else \"Parsing failed\"\n",
    "\n",
    "            except Exception as e:\n",
    "                # Minimal error handling for API call failure\n",
    "                print(f\"  API Call Error on Example {i}: {e}\")\n",
    "                llm_label = \"API Error\"\n",
    "                llm_explanation = f\"API Call Error: {e}\"\n",
    "\n",
    "        # --- 5. Store Essentials ---\n",
    "        evaluation_results.append({\n",
    "            \"index\": i,\n",
    "            \"query\": user_query,\n",
    "            \"human_label\": human_label,\n",
    "            \"llm_label\": llm_label,\n",
    "            \"llm_explanation\": llm_explanation,\n",
    "        })\n",
    "\n",
    "    # --- Convert to DataFrame ---\n",
    "    print(\"\\nEvaluation complete. Creating DataFrame...\")\n",
    "    results_df_final = pd.DataFrame(evaluation_results)\n",
    "\n",
    "    # --- Display DataFrame ---\n",
    "    print(\"Direct API Evaluation Results (No Progress Bar):\")\n",
    "    pd.set_option('display.max_rows', 50)\n",
    "    pd.set_option('display.max_colwidth', 150)\n",
    "    display(results_df_final)\n",
    "\n",
    "else:\n",
    "    if not client:\n",
    "        print(\"Evaluation skipped because OpenAI client failed to initialize.\")\n",
    "    else:\n",
    "        print(\"Evaluation skipped - check dataset and prompt template definitions.\")\n",
    "\n",
    "print(\"\\n--- End Direct OpenAI API Full Evaluation (No Progress Bar) ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Evaluator Function for Tool Usage (Direct API Call)\n",
    "\n",
    "Our previous attempts to evaluate Tool Usage using `phoenix.evals.llm_classify` resulted in persistent inconsistencies: the LLM judge's explanations suggested correct reasoning, but the final label forced by the `rails=[\"Correct\", \"Incorrect\"]` parameter was always 'Incorrect'.\n",
    "\n",
    "We subsequently tested making direct calls to the OpenAI API (`gpt-4o`) using the simplified `REVISED_TOOL_USAGE_PROMPT_TEMPLATE`. This approach **worked correctly**, yielding consistent explanations and labels that matched the human annotations across all examples.\n",
    "\n",
    "Therefore, we will now define a new evaluator function, `evaluate_tool_usage_direct_api`, that encapsulates this successful direct API call logic. This function will:\n",
    "\n",
    "1.  Accept the `output` (from the agent/task function) and `expected` (from the dataset) dictionaries as input, following the standard Phoenix evaluator signature.\n",
    "2.  Extract the necessary fields (`user_query`, `tool_called`, `generated_sql`).\n",
    "3.  Format the `REVISED_TOOL_USAGE_PROMPT_TEMPLATE`.\n",
    "4.  Call the OpenAI API directly.\n",
    "5.  Parse the response to get the LLM's label ('Correct'/'Incorrect') and explanation.\n",
    "6.  Return a `phoenix.evals.models.scoring.Score` object containing the score (1.0 for 'Correct', 0.0 for 'Incorrect') and the LLM's explanation.\n",
    "\n",
    "This allows us to replace the faulty `llm_classify`-based evaluator with our custom, validated logic while potentially still using the `phoenix.experiments.run_experiment` framework for overall execution and integration with other evaluators (like SQL and Final Answer quality, which we will define next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing evaluate_tool_usage_direct_api (returns float) with Example 0 ---\n",
      "Score Returned: 1.0 (Type: <class 'float'>)\n",
      "--- End Test ---\n"
     ]
    }
   ],
   "source": [
    "# --- Phoenix-Compatible Evaluator: Tool Usage (Direct API - Returns Float) ---\n",
    "# Corrected based on L11(1).ipynb: Accepts output/expected, returns float score.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "# Removed Score object import - not needed based on reference\n",
    "import logging\n",
    "\n",
    "# Configure logging (optional)\n",
    "logging.basicConfig(level=logging.WARNING, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_TO_USE = \"gpt-4o\"\n",
    "# Assumes REVISED_TOOL_USAGE_PROMPT_TEMPLATE is defined globally\n",
    "if 'REVISED_TOOL_USAGE_PROMPT_TEMPLATE' not in globals():\n",
    "    logging.error(\"STOPPING: REVISED_TOOL_USAGE_PROMPT_TEMPLATE not found.\")\n",
    "    REVISED_TOOL_USAGE_PROMPT_TEMPLATE = \"Prompt not defined\"\n",
    "\n",
    "# Initialize OpenAI client once\n",
    "try:\n",
    "    openai_client = OpenAI()\n",
    "    # logging.info(f\"OpenAI client initialized for model: {MODEL_TO_USE}\") # Less verbose\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to initialize OpenAI Client: {e}. Check API key.\")\n",
    "    openai_client = None\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "def evaluate_tool_usage_direct_api(output: dict, expected: dict) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates tool usage correctness via direct OpenAI API calls.\n",
    "    Returns a float score (1.0 for Correct, 0.0 for Incorrect/Error).\n",
    "    Compatible with phoenix.experiments.run_experiment evaluators list.\n",
    "\n",
    "    Args:\n",
    "        output: Dictionary containing agent outputs (user_query, tool_called, etc.).\n",
    "        expected: Dictionary containing expected outputs/labels from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        A float score (0.0 or 1.0).\n",
    "    \"\"\"\n",
    "    global openai_client, REVISED_TOOL_USAGE_PROMPT_TEMPLATE\n",
    "\n",
    "    score = 0.0 # Default score for errors/skips/Incorrect\n",
    "\n",
    "    if not openai_client:\n",
    "        logging.warning(\"Skipping evaluation: OpenAI client not initialized\")\n",
    "        return score\n",
    "\n",
    "    if not REVISED_TOOL_USAGE_PROMPT_TEMPLATE or REVISED_TOOL_USAGE_PROMPT_TEMPLATE == \"Prompt not defined\":\n",
    "         logging.warning(\"Skipping evaluation: Prompt template not defined\")\n",
    "         return score\n",
    "\n",
    "    # --- 1. Get Data ---\n",
    "    user_query = output.get('user_query')\n",
    "    tool_called = output.get('tool_called')\n",
    "    generated_sql = output.get('generated_sql', 'N/A')\n",
    "\n",
    "    if user_query is None or tool_called is None:\n",
    "        logging.warning(\"Skipping evaluation: Missing 'user_query' or 'tool_called' in output\")\n",
    "        return score\n",
    "\n",
    "    # --- 2. Format Prompt ---\n",
    "    try:\n",
    "        prompt = REVISED_TOOL_USAGE_PROMPT_TEMPLATE.format(\n",
    "            user_query=user_query,\n",
    "            tool_called=tool_called,\n",
    "            generated_sql=generated_sql\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        logging.warning(f\"Skipping evaluation: Error formatting prompt - missing key {e}\")\n",
    "        return score # Return 0.0 on formatting error\n",
    "\n",
    "    # --- 3. Call API & Parse ---\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=MODEL_TO_USE,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        raw_output = response.choices[0].message.content\n",
    "\n",
    "        # Parse Label\n",
    "        label_match = re.search(r\"LABEL:\\s*(\\w+)\", raw_output, re.IGNORECASE)\n",
    "        llm_label_str = label_match.group(1).strip().capitalize() if label_match else \"Parsing failed\"\n",
    "\n",
    "        # --- 4. Determine Score ---\n",
    "        if llm_label_str == \"Correct\":\n",
    "            score = 1.0\n",
    "        # else: score remains 0.0 for \"Incorrect\", \"Parsing failed\", or API errors\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"API Call Error during tool usage evaluation: {e}\")\n",
    "        # score remains 0.0\n",
    "\n",
    "    return score\n",
    "\n",
    "# --- Quick Test (Optional) ---\n",
    "if 'evaluation_dataset' in locals() and len(evaluation_dataset) > 0:\n",
    "    if openai_client and REVISED_TOOL_USAGE_PROMPT_TEMPLATE != \"Prompt not defined\":\n",
    "        print(\"\\n--- Testing evaluate_tool_usage_direct_api (returns float) with Example 0 ---\")\n",
    "        test_example = evaluation_dataset[0]\n",
    "        test_output_data = test_example.input\n",
    "        test_expected_data = test_example.output\n",
    "        test_score_float = evaluate_tool_usage_direct_api(test_output_data, test_expected_data)\n",
    "        print(f\"Score Returned: {test_score_float} (Type: {type(test_score_float)})\")\n",
    "        print(\"--- End Test ---\")\n",
    "    else:\n",
    "        print(\"\\nSkipping function test - OpenAI client or prompt not ready.\")\n",
    "else:\n",
    "    print(\"\\nSkipping function test - evaluation_dataset not loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Test: Running Experiment with Custom Tool Usage Evaluator\n",
    "\n",
    "Before defining the evaluators for SQL Correctness and Final Answer Quality, we will run a minimal Phoenix experiment using **only** our custom `evaluate_tool_usage_direct_api` function.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "*   Verify that our custom evaluator integrates correctly with the `phoenix.experiments.run_experiment` framework.\n",
    "*   Confirm that the scores generated by the direct API calls are logged to the Phoenix backend and visible in the UI.\n",
    "\n",
    "This serves as a crucial integration check to ensure our direct API call approach works within the Phoenix ecosystem before proceeding further. We expect to see scores (likely all 1.0) for the `evaluate_tool_usage_direct_api` evaluator in the resulting Phoenix experiment UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # --- Run Experiment with ONLY the Custom Tool Usage Evaluator ---\n",
    "    # ... (previous code in the cell: imports, checks, defining experiment_name etc.) ...\n",
    "\n",
    "    if not missing:\n",
    "        print(\"Running experiment with only the custom direct API tool usage evaluator...\")\n",
    "        # ... (now_str, experiment_name defined here) ...\n",
    "\n",
    "        # *** ADDED: Explicitly set environment vars right before run_experiment ***\n",
    "        print(\"Ensuring environment variables are set for run_experiment...\")\n",
    "        cloud_endpoint = \"https://app.phoenix.arize.com\"\n",
    "        client_headers = os.getenv(\"PHOENIX_CLIENT_HEADERS\") # Get from env (should be loaded)\n",
    "        if client_headers:\n",
    "             os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = cloud_endpoint\n",
    "             os.environ[\"PHOENIX_CLIENT_HEADERS\"] = client_headers # Re-set it just in case\n",
    "             print(f\"  Set PHOENIX_COLLECTOR_ENDPOINT={cloud_endpoint}\")\n",
    "             print(f\"  Set PHOENIX_CLIENT_HEADERS=api_key=...\")\n",
    "        else:\n",
    "             print(\"  WARNING: PHOENIX_CLIENT_HEADERS not found in os.getenv(), cannot set for run_experiment.\")\n",
    "             # Optionally raise an error here if headers are essential\n",
    "\n",
    "        # *** Original run_experiment call (without client=px_client) ***\n",
    "        experiment_tool_usage_only = run_experiment(\n",
    "            evaluation_dataset,                 # First positional: dataset\n",
    "            dummy_task_function,                # Second positional: task function\n",
    "            evaluators=[evaluate_tool_usage_direct_api], # Use list with just the one evaluator\n",
    "            experiment_name=experiment_name,    # Keyword: experiment_name\n",
    "            experiment_description=\"Testing direct API tool usage evaluator integration.\" # Keyword: description\n",
    "            # concurrency=5 # Optional concurrency\n",
    "        )\n",
    "\n",
    "        print(f\"\\nExperiment '{experiment_name}' run initiated.\")\n",
    "        print(\"Please check the Phoenix UI for results.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping experiment - required components not loaded: {', '.join(missing)}\")\n",
    "\n",
    "    print(\"\\n--- End Tool Usage Only Experiment ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluators: SQL Correctness & Final Answer Quality (Direct API)\n",
    "\n",
    "Following the successful pattern established for `evaluate_tool_usage_direct_api`, we now define the evaluators for SQL Correctness and Final Answer Quality using direct calls to the Gemini API via our `call_gemini` helper function.\n",
    "\n",
    "Each evaluator:\n",
    "1. Takes the experiment `example` dictionary as input.\n",
    "2. Formats a specific prompt using data from the example (`user_query`, `generated_sql`, `final_answer`).\n",
    "3. Calls the `call_gemini` function.\n",
    "4. Parses the response (\"Correct\"/\"Incorrect\" or \"Good\"/\"Bad\") into a float score (1.0 or 0.0).\n",
    "5. Handles potential missing data or API errors gracefully by returning 0.0.\n",
    "\n",
    "Finally, we combine all three custom evaluators into a list (`all_custom_evaluators`) to be used in the experiment run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 1: DEFINITIONS ---\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI # Ensure OpenAI is imported\n",
    "# Assumes openai_client is initialized globally in a *previous* cell and is working\n",
    "# Assumes MODEL_TO_USE = \"gpt-4o\" is defined globally\n",
    "\n",
    "# --- Define Prompt Template ---\n",
    "SQL_CORRECTNESS_PROMPT_TEMPLATE = \"\"\"Evaluate if the Generated SQL is semantically correct and appropriate for the User Query. Consider typical schemas (e.g., transcript_segments table). Ignore Final Answer quality.\n",
    "\n",
    "User Query:\n",
    "{user_query}\n",
    "\n",
    "Generated SQL:\n",
    "{generated_sql}\n",
    "\n",
    "Is the SQL correct and appropriate?\n",
    "Provide a brief EXPLANATION and finish with LABEL: Correct or LABEL: Incorrect.\n",
    "\"\"\"\n",
    "print(\"Defined: SQL_CORRECTNESS_PROMPT_TEMPLATE\")\n",
    "\n",
    "# --- Helper Function to Call OpenAI ---\n",
    "# Define this if it's not already defined and available from another cell\n",
    "def call_openai_judge(prompt: str, model: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"Calls the specified OpenAI model as a judge and returns the raw text response.\"\"\"\n",
    "    if not openai_client:\n",
    "        raise RuntimeError(\"OpenAI client is not initialized.\")\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI API call: {e}\")\n",
    "        return \"API Error\" # Return error string\n",
    "print(\"Defined: call_openai_judge helper function\")\n",
    "\n",
    "# --- Evaluator Definition ---\n",
    "def evaluate_sql_correctness_direct_api(output: dict, expected: dict) -> float:\n",
    "    \"\"\"\n",
    "    (Simplified) Evaluates SQL correctness via direct OpenAI API call.\n",
    "    Returns 1.0 for Correct, 0.0 otherwise.\n",
    "    \"\"\"\n",
    "    # Function body as defined in the previous simplified version...\n",
    "    if not openai_client:\n",
    "        print(\"Prerequisite Error: OpenAI client not initialized.\")\n",
    "        return 0.0\n",
    "    if not SQL_CORRECTNESS_PROMPT_TEMPLATE:\n",
    "        print(\"Prerequisite Error: SQL_CORRECTNESS_PROMPT_TEMPLATE not defined.\")\n",
    "        return 0.0\n",
    "\n",
    "    user_query = output.get('user_query')\n",
    "    generated_sql = output.get('generated_sql')\n",
    "\n",
    "    if not user_query or not generated_sql:\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        prompt = SQL_CORRECTNESS_PROMPT_TEMPLATE.format(\n",
    "            user_query=user_query,\n",
    "            generated_sql=generated_sql\n",
    "        )\n",
    "    except KeyError as e:\n",
    "         print(f\"Prompt Formatting Error: Missing key {e}\")\n",
    "         return 0.0\n",
    "\n",
    "    try:\n",
    "        raw_output = call_openai_judge(prompt, model=MODEL_TO_USE) # Use helper\n",
    "        label_match = re.search(r\"LABEL:\\s*(\\w+)\", raw_output, re.IGNORECASE)\n",
    "\n",
    "        if label_match and label_match.group(1).strip().capitalize() == \"Correct\":\n",
    "            return 1.0\n",
    "        else:\n",
    "            # Covers API Error string, parsing failure, or Incorrect label\n",
    "            # print(f\"Debug: Raw output '{raw_output[:50]}...' resulted in 0.0\") # Optional debug\n",
    "            return 0.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during SQL correctness evaluation: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "print(\"Defined: evaluate_sql_correctness_direct_api function\")\n",
    "# --- END CELL 1 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL 2: FULL EVALUATION & DATAFRAME OUTPUT ---\n",
    "import re\n",
    "import pandas as pd # Import pandas\n",
    "# Assumes functions from Cell 1 (call_openai_judge, evaluate_sql_correctness_direct_api) are defined\n",
    "# Assumes openai_client, evaluation_dataset, MODEL_TO_USE, SQL_CORRECTNESS_PROMPT_TEMPLATE are available\n",
    "\n",
    "print(f\"\\n--- Running Full Evaluation for SQL Correctness on {len(evaluation_dataset)} Examples ---\")\n",
    "\n",
    "# --- Prerequisite Check ---\n",
    "test_passed = True\n",
    "if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "    print(\"Evaluation FAIL: evaluation_dataset not found or empty.\")\n",
    "    test_passed = False\n",
    "if 'openai_client' not in locals() or not openai_client:\n",
    "    print(\"Evaluation FAIL: openai_client not initialized.\")\n",
    "    test_passed = False\n",
    "if 'SQL_CORRECTNESS_PROMPT_TEMPLATE' not in locals() or not SQL_CORRECTNESS_PROMPT_TEMPLATE:\n",
    "     print(\"Evaluation FAIL: SQL_CORRECTNESS_PROMPT_TEMPLATE not defined.\")\n",
    "     test_passed = False\n",
    "if 'evaluate_sql_correctness_direct_api' not in locals():\n",
    "     print(\"Evaluation FAIL: evaluate_sql_correctness_direct_api function not defined (Run Cell 1?).\")\n",
    "     test_passed = False\n",
    "if 'call_openai_judge' not in locals():\n",
    "     print(\"Evaluation FAIL: call_openai_judge function not defined (Run Cell 1?).\")\n",
    "     test_passed = False\n",
    "# --- End Prerequisite Check ---\n",
    "\n",
    "results_list = [] # Initialize list to store results\n",
    "\n",
    "if test_passed:\n",
    "    # Iterate through ALL examples using enumerate\n",
    "    for i, test_example in enumerate(evaluation_dataset):\n",
    "        print(f\"Processing Example {i}...\") # Progress indicator\n",
    "\n",
    "        test_output_data = test_example.input\n",
    "        test_expected_data = test_example.output # needed for function signature\n",
    "\n",
    "        user_query = test_output_data.get('user_query', 'MISSING')\n",
    "        generated_sql = test_output_data.get('generated_sql', 'MISSING')\n",
    "\n",
    "        # --- Call LLM judge directly for raw output ---\n",
    "        raw_judge_response = \"Skipped direct call\" # Default\n",
    "        if user_query != 'MISSING' and generated_sql != 'MISSING':\n",
    "            try:\n",
    "                test_prompt = SQL_CORRECTNESS_PROMPT_TEMPLATE.format(\n",
    "                    user_query=user_query,\n",
    "                    generated_sql=generated_sql\n",
    "                )\n",
    "                raw_judge_response = call_openai_judge(test_prompt, model=MODEL_TO_USE) # Use helper\n",
    "                # Reduce printing inside the loop for large datasets\n",
    "                # print(\"--- Raw LLM Judge Response ---\")\n",
    "                # print(raw_judge_response)\n",
    "                # print(\"----------------------------\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error calling LLM Judge directly on Example {i}: {e}\")\n",
    "                raw_judge_response = f\"Error during direct call: {e}\"\n",
    "        # else:\n",
    "            # print(f\"  Skipping direct LLM call on Example {i} due to missing query/SQL.\")\n",
    "\n",
    "\n",
    "        # --- Call the evaluator function ---\n",
    "        test_score_float = 0.0 # Default score\n",
    "        try:\n",
    "            test_score_float = evaluate_sql_correctness_direct_api(test_output_data, test_expected_data)\n",
    "            # print(f\"  Score Returned: {test_score_float}\") # Optional print inside loop\n",
    "        except Exception as e:\n",
    "            print(f\"  Error calling evaluator function on Example {i}: {e}\")\n",
    "            test_score_float = 0.0 # Assign 0.0 on error\n",
    "\n",
    "        # --- Append results to list ---\n",
    "        results_list.append({\n",
    "            \"index\": i,\n",
    "            \"user_query\": user_query,\n",
    "            \"generated_sql\": generated_sql,\n",
    "            \"raw_llm_response\": raw_judge_response,\n",
    "            \"sql_correctness_score\": test_score_float\n",
    "        })\n",
    "\n",
    "    # --- Convert list to DataFrame ---\n",
    "    print(\"\\nEvaluation loop complete. Creating DataFrame...\")\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # --- Display DataFrame ---\n",
    "    print(\"SQL Correctness Evaluation Results:\")\n",
    "    pd.set_option('display.max_rows', 100) # Show more rows if needed\n",
    "    pd.set_option('display.max_colwidth', 200) # Show more text width\n",
    "    display(results_df) # Use display() for better rendering in notebooks\n",
    "\n",
    "else:\n",
    "    print(\"--- Evaluation Aborted due to failed prerequisites ---\")\n",
    "\n",
    "print(f\"\\n--- End Full Evaluation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare LLM SQL Correctness Scores to Human Labels ---\n",
    "import pandas as pd # Ensure pandas is imported\n",
    "\n",
    "print(\"Comparing LLM SQL scores to human labels...\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Key for the human label in evaluation_dataset[i].output\n",
    "# Identified from notebook inspection as 'sql_correctness_label'\n",
    "HUMAN_LABEL_KEY = 'sql_correctness_label'\n",
    "# --- End Configuration ---\n",
    "\n",
    "try:\n",
    "    # Check prerequisite DataFrames/Datasets\n",
    "    if 'results_df' not in locals():\n",
    "        raise NameError(\"'results_df' DataFrame not found. Please run the evaluation cell first.\")\n",
    "    if 'evaluation_dataset' not in locals():\n",
    "        raise NameError(\"'evaluation_dataset' not found. Please ensure it is loaded.\")\n",
    "\n",
    "    # 1. Extract Human Labels from the dataset\n",
    "    human_labels = [example.output.get(HUMAN_LABEL_KEY, \"MISSING\") for example in evaluation_dataset]\n",
    "\n",
    "    # Check length consistency\n",
    "    if len(human_labels) != len(results_df):\n",
    "        raise ValueError(f\"Mismatch in lengths: results_df has {len(results_df)} rows, but extracted {len(human_labels)} human labels.\")\n",
    "\n",
    "    # 2. Add Human Labels column to the results DataFrame\n",
    "    results_df['human_sql_label'] = human_labels\n",
    "\n",
    "    # 3. Add LLM Label (string format) & Comparison column\n",
    "    def score_to_label(score):\n",
    "        # Converts 1.0 to \"Correct\", anything else (0.0, errors) to \"Incorrect\"\n",
    "        return \"Correct\" if score == 1.0 else \"Incorrect\"\n",
    "\n",
    "    results_df['llm_sql_label'] = results_df['sql_correctness_score'].apply(score_to_label)\n",
    "\n",
    "    # Compare strings (case-insensitive), handle \"MISSING\" human labels\n",
    "    results_df['match'] = results_df.apply(\n",
    "        lambda row: str(row['llm_sql_label']).lower() == str(row['human_sql_label']).lower()\n",
    "                    if row['human_sql_label'] != \"MISSING\" else None, # Result is None if human label was missing\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # 4. Display Key Comparison Columns\n",
    "    print(\"\\nComparison Results (Human vs. LLM Judge for SQL Correctness):\")\n",
    "    display_cols = ['user_query', 'generated_sql', 'human_sql_label', 'llm_sql_label', 'match', 'raw_llm_response']\n",
    "    # Filter out any columns that might not exist (e.g., if df creation failed partially)\n",
    "    display_cols = [col for col in display_cols if col in results_df.columns]\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    pd.set_option('display.max_colwidth', 200)\n",
    "    display(results_df[display_cols])\n",
    "\n",
    "    # 5. Calculate and Print Match Percentage\n",
    "    if 'match' in results_df.columns:\n",
    "        match_count = results_df['match'].sum() # Counts True values\n",
    "        valid_comparisons = results_df['match'].notna().sum() # Counts non-None values\n",
    "        if valid_comparisons > 0:\n",
    "            match_percentage = (match_count / valid_comparisons) * 100\n",
    "            print(f\"\\nAgreement between LLM Judge and Human Labels: {match_percentage:.2f}% ({match_count}/{valid_comparisons})\")\n",
    "        else:\n",
    "            print(\"\\nCould not calculate agreement percentage (no valid comparisons).\")\n",
    "\n",
    "except (NameError, AttributeError, ValueError, KeyError) as e:\n",
    "    print(f\"\\nError during comparison: {e}\")\n",
    "    print(\"Please ensure 'results_df' exists, 'evaluation_dataset' is loaded correctly,\")\n",
    "    print(f\"and the key '{HUMAN_LABEL_KEY}' is correct for the human labels in evaluation_dataset[i].output.\")\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during comparison: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluator 3: Final Answer Quality (Mimicking Successful Pattern) ---\n",
    "import re\n",
    "import logging # Or use print if preferred for errors\n",
    "# Assumes openai_client is initialized globally\n",
    "# Assumes MODEL_TO_USE = \"gpt-4o\" is defined globally\n",
    "\n",
    "# --- Define Prompt Template ---\n",
    "# Make sure this is defined before the function\n",
    "FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE = \"\"\"Evaluate if the Final Answer accurately and completely answers the User Query, based ONLY on the query and answer text. Do not assume external data or SQL.\n",
    "\n",
    "User Query:\n",
    "{user_query}\n",
    "\n",
    "Final Answer:\n",
    "{final_answer}\n",
    "\n",
    "Is the Final Answer good quality (accurate, relevant, complete)?\n",
    "Provide a brief EXPLANATION and finish with LABEL: Good or LABEL: Bad.\n",
    "\"\"\"\n",
    "print(\"Defined: FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE\") # Optional confirmation\n",
    "\n",
    "# --- Evaluator Definition ---\n",
    "def evaluate_final_answer_quality_direct_api(output: dict, expected: dict) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates Final Answer quality via direct OpenAI API call (Mimics successful pattern).\n",
    "    Returns 1.0 for Good, 0.0 otherwise (Bad, Error, Parsing Failure).\n",
    "    \"\"\"\n",
    "    # Rely on global variables: openai_client, FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE, MODEL_TO_USE\n",
    "    score = 0.0 # Default to 0.0 (representing \"Bad\" or error)\n",
    "\n",
    "    # --- 1. Prerequisites & Data Checks ---\n",
    "    if not openai_client:\n",
    "        print(\"Prerequisite Error: OpenAI client not initialized.\") # Changed from logging\n",
    "        return 0.0\n",
    "    if not FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE:\n",
    "        print(\"Prerequisite Error: FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE not defined.\")\n",
    "        return 0.0\n",
    "\n",
    "    user_query = output.get('user_query')\n",
    "    final_answer = output.get('final_answer')\n",
    "\n",
    "    if not user_query or not final_answer:\n",
    "        # print(\"Data Error: Missing 'user_query' or 'final_answer' in output.\") # Optional\n",
    "        return 0.0\n",
    "\n",
    "    # --- 2. Format Prompt ---\n",
    "    try:\n",
    "        prompt = FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE.format(\n",
    "            user_query=user_query,\n",
    "            final_answer=final_answer\n",
    "        )\n",
    "    except KeyError as e:\n",
    "         print(f\"Prompt Formatting Error: Missing key {e}\")\n",
    "         return 0.0\n",
    "\n",
    "    # --- 3. Call API & Parse Result ---\n",
    "    try:\n",
    "        # Assuming call_openai_judge helper is defined and available\n",
    "        raw_output = call_openai_judge(prompt, model=MODEL_TO_USE)\n",
    "        label_match = re.search(r\"LABEL:\\s*(\\w+)\", raw_output, re.IGNORECASE)\n",
    "\n",
    "        # Return 1.0 ONLY if API succeeded AND label is exactly \"Good\"\n",
    "        if label_match and label_match.group(1).strip().capitalize() == \"Good\":\n",
    "            score = 1.0\n",
    "        # else: score remains 0.0\n",
    "\n",
    "    except NameError:\n",
    "        # Explicitly catch if the helper function isn't defined\n",
    "         print(\"Error: call_openai_judge function not found.\")\n",
    "         # score remains 0.0\n",
    "    except Exception as e:\n",
    "        # Covers cases: OpenAI API call failed inside helper, or other unexpected errors\n",
    "        print(f\"API Call/Evaluation Error during Final Answer evaluation: {e}\")\n",
    "        # score remains 0.0\n",
    "\n",
    "    return score\n",
    "\n",
    "print(\"Defined: evaluate_final_answer_quality_direct_api function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full Evaluation & Comparison for Final Answer Quality ---\n",
    "import pandas as pd\n",
    "import re # Ensure re is imported\n",
    "\n",
    "print(\"\\n--- Running Full Evaluation for Final Answer Quality ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Key for the human label in evaluation_dataset[i].output\n",
    "# Identified from notebook inspection as 'final_answer_quality_label'\n",
    "HUMAN_LABEL_KEY_ANSWER = 'final_answer_quality_label'\n",
    "\n",
    "# Expected Labels from the LLM for this evaluator\n",
    "# Note: Dataset uses \"Fail\" but our prompt asks for \"Bad\". We need to map.\n",
    "HUMAN_LABEL_MAP = {\"Fail\": \"Bad\"} # Map human label \"Fail\" to expected LLM \"Bad\"\n",
    "LLM_POSITIVE_LABEL = \"Good\" # What our prompt asks for as the \"good\" label\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "# --- Prerequisite Check ---\n",
    "eval_passed = True\n",
    "if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "    print(\"Evaluation FAIL: evaluation_dataset not found or empty.\")\n",
    "    eval_passed = False\n",
    "if 'openai_client' not in locals() or not openai_client:\n",
    "    print(\"Evaluation FAIL: openai_client not initialized.\")\n",
    "    eval_passed = False\n",
    "if 'FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE' not in locals() or not FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE:\n",
    "     print(\"Evaluation FAIL: FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE not defined.\")\n",
    "     eval_passed = False\n",
    "if 'evaluate_final_answer_quality_direct_api' not in locals():\n",
    "     print(\"Evaluation FAIL: evaluate_final_answer_quality_direct_api function not defined.\")\n",
    "     eval_passed = False\n",
    "if 'call_openai_judge' not in locals():\n",
    "     print(\"Evaluation FAIL: call_openai_judge function not defined.\")\n",
    "     eval_passed = False\n",
    "# --- End Prerequisite Check ---\n",
    "\n",
    "answer_results_list = [] # Initialize list for results\n",
    "\n",
    "if eval_passed:\n",
    "    print(f\"Processing {len(evaluation_dataset)} examples for Final Answer Quality...\")\n",
    "    # Iterate through ALL examples\n",
    "    for i, test_example in enumerate(evaluation_dataset):\n",
    "        # print(f\"Processing Example {i}...\") # Can uncomment for verbose progress\n",
    "\n",
    "        test_output_data = test_example.input\n",
    "        test_expected_data = test_example.output # needed for function signature\n",
    "\n",
    "        user_query = test_output_data.get('user_query', 'MISSING')\n",
    "        final_answer = test_output_data.get('final_answer', 'MISSING')\n",
    "        human_label_raw = test_example.output.get(HUMAN_LABEL_KEY_ANSWER, \"MISSING_KEY\")\n",
    "\n",
    "        # --- Call LLM judge directly for raw output ---\n",
    "        raw_judge_response_answer = \"Skipped direct call\" # Default\n",
    "        if user_query != 'MISSING' and final_answer != 'MISSING':\n",
    "            try:\n",
    "                test_prompt_answer = FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE.format(\n",
    "                    user_query=user_query,\n",
    "                    final_answer=final_answer\n",
    "                )\n",
    "                raw_judge_response_answer = call_openai_judge(test_prompt_answer, model=MODEL_TO_USE)\n",
    "            except Exception as e:\n",
    "                # print(f\"  Error calling LLM Judge directly on Example {i}: {e}\") # Verbose error\n",
    "                raw_judge_response_answer = f\"Error during direct call: {e}\"\n",
    "        # else:\n",
    "             # print(f\"  Skipping direct LLM call on Example {i} due to missing query/answer.\")\n",
    "\n",
    "\n",
    "        # --- Call the evaluator function ---\n",
    "        test_score_float_answer = 0.0 # Default score\n",
    "        try:\n",
    "            test_score_float_answer = evaluate_final_answer_quality_direct_api(test_output_data, test_expected_data)\n",
    "        except Exception as e:\n",
    "            # print(f\"  Error calling evaluator function on Example {i}: {e}\") # Verbose error\n",
    "            test_score_float_answer = 0.0 # Assign 0.0 on error\n",
    "\n",
    "        # --- Append results to list ---\n",
    "        answer_results_list.append({\n",
    "            \"index\": i,\n",
    "            \"user_query\": user_query,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"human_answer_label_raw\": human_label_raw, # Store the original human label\n",
    "            \"raw_llm_response_answer\": raw_judge_response_answer,\n",
    "            \"final_answer_score\": test_score_float_answer\n",
    "        })\n",
    "\n",
    "    # --- Convert list to DataFrame ---\n",
    "    print(\"\\nEvaluation loop complete. Creating DataFrame...\")\n",
    "    answer_results_df = pd.DataFrame(answer_results_list)\n",
    "\n",
    "    # --- Add Comparison Columns ---\n",
    "    print(\"Adding comparison columns...\")\n",
    "    # Map human label \"Fail\" to \"Bad\" for comparison\n",
    "    answer_results_df['human_answer_label_mapped'] = answer_results_df['human_answer_label_raw'].map(HUMAN_LABEL_MAP).fillna(answer_results_df['human_answer_label_raw'])\n",
    "\n",
    "    # Convert score (1.0/0.0) to LLM label (\"Good\"/\"Bad\")\n",
    "    def answer_score_to_label(score):\n",
    "        return LLM_POSITIVE_LABEL if score == 1.0 else \"Bad\" # Assumes 0.0 means Bad\n",
    "\n",
    "    answer_results_df['llm_answer_label'] = answer_results_df['final_answer_score'].apply(answer_score_to_label)\n",
    "\n",
    "    # Compare mapped human label with LLM label (case-insensitive)\n",
    "    answer_results_df['answer_match'] = answer_results_df.apply(\n",
    "        lambda row: str(row['llm_answer_label']).lower() == str(row['human_answer_label_mapped']).lower()\n",
    "                    if row['human_answer_label_mapped'] not in [\"MISSING\", \"MISSING_KEY\"] else None,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Display DataFrame ---\n",
    "    print(\"\\nFinal Answer Quality Evaluation Results:\")\n",
    "    # Select and reorder columns\n",
    "    display_cols_answer = ['index', 'user_query', 'final_answer', 'human_answer_label_raw', 'llm_answer_label', 'answer_match', 'raw_llm_response_answer']\n",
    "    display_cols_answer = [col for col in display_cols_answer if col in answer_results_df.columns] # Ensure columns exist\n",
    "\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    pd.set_option('display.max_colwidth', 200)\n",
    "    display(answer_results_df[display_cols_answer])\n",
    "\n",
    "    # --- Calculate Match Percentage ---\n",
    "    if 'answer_match' in answer_results_df.columns:\n",
    "        match_count_answer = answer_results_df['answer_match'].sum() # Counts True values\n",
    "        valid_comparisons_answer = answer_results_df['answer_match'].notna().sum() # Counts non-None values\n",
    "        if valid_comparisons_answer > 0:\n",
    "            match_percentage_answer = (match_count_answer / valid_comparisons_answer) * 100\n",
    "            print(f\"\\nAgreement between LLM Judge and Human Labels (Final Answer): {match_percentage_answer:.2f}% ({match_count_answer}/{valid_comparisons_answer})\")\n",
    "        else:\n",
    "            print(\"\\nCould not calculate agreement percentage (no valid comparisons).\")\n",
    "\n",
    "else:\n",
    "    print(\"--- Final Answer Evaluation Aborted due to failed prerequisites ---\")\n",
    "\n",
    "print(f\"\\n--- End Final Answer Quality Evaluation ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Answer Quality Evaluation & Alignment Challenge\n",
    "\n",
    "We successfully ran the `evaluate_final_answer_quality_direct_api` function across the dataset and compared its judgments to the human labels.\n",
    "\n",
    "**Key Result:**\n",
    "\n",
    "*   **Agreement with Human Labels:** ~18% (3/17)\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "The LLM judge achieved very low agreement with the human assessment for final answer quality. Examining the results table reveals the likely cause:\n",
    "*   The LLM judge, following our prompt to evaluate *only* based on the query and answer text (ignoring SQL/data correctness), rated almost all answers as \"Good\".\n",
    "*   The human labels (`Pass`/`Fail`/`Correct`), however, likely incorporated factual correctness based on the underlying data, resulting in many \"Fail\" labels.\n",
    "*   This fundamental mismatch in evaluation criteria led to the significant disagreement.\n",
    "\n",
    "**Pedagogical Opportunity:**\n",
    "\n",
    "This outcome serves as an excellent illustration of the challenges in automated evaluation:\n",
    "*   **Technical Success vs. Meaningful Results:** We successfully built and ran the evaluator, but the results lack strong alignment with human judgment in this case.\n",
    "*   **Importance of Criteria & Prompting:** It highlights how critical defining the *right* evaluation criteria and crafting effective prompts is. Simply asking if an answer is \"Good\" based on text alone was insufficient here.\n",
    "*   **Improvement Task:** This presents a clear opportunity for improvement. Students could be tasked with refining the `FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE` to encourage the LLM to consider factual accuracy (perhaps by providing the generated SQL or context), or exploring different evaluation scales beyond simple \"Good/Bad\", aiming to increase alignment with human judgment.\n",
    "\n",
    "This demonstrates that building LLM-as-judge systems requires not just coding but careful consideration of evaluation design and alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Experiment via Phoenix\n",
    "\n",
    "Now that all three evaluator functions (`evaluate_tool_usage_direct_api`, `evaluate_sql_correctness_direct_api`, `evaluate_final_answer_quality_direct_api`) have been defined using the direct OpenAI API call pattern and verified, we will execute the full evaluation harness using `phoenix.experiments.run_experiment`.\n",
    "\n",
    "This function will:\n",
    "1.  Iterate through each example in the `evaluation_dataset`.\n",
    "2.  Run the `dummy_task_function` for each example (simply passing through the pre-computed agent outputs).\n",
    "3.  Call each of our three defined evaluator functions for every example.\n",
    "4.  Log the inputs, outputs, human labels, and the scores from all three evaluators to the Phoenix/Arize platform under a timestamped experiment name.\n",
    "\n",
    "The results, including aggregate scores for each evaluator, will be summarized below the cell upon completion, and the full details can be explored in the linked Phoenix UI. This provides a centralized record of the agent's performance across all evaluation criteria for this version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Phoenix Experiment with All Corrected OpenAI Evaluators ---\n",
    "import phoenix as px # Ensure phoenix is imported\n",
    "from phoenix.experiments import run_experiment\n",
    "from datetime import datetime\n",
    "# Assumes 'evaluation_dataset' is loaded\n",
    "# Assumes 'dummy_task_function' is defined (or define it here)\n",
    "# Assumes the three evaluator functions are defined:\n",
    "#   - evaluate_tool_usage_direct_api\n",
    "#   - evaluate_sql_correctness_direct_api\n",
    "#   - evaluate_final_answer_quality_direct_api\n",
    "\n",
    "print(\"\\n--- Preparing to run Phoenix experiment with all OpenAI evaluators ---\")\n",
    "\n",
    "# --- Combine Evaluators ---\n",
    "# Ensure the function names below match exactly how they were defined\n",
    "try:\n",
    "    all_final_evaluators = [\n",
    "        evaluate_tool_usage_direct_api,\n",
    "        evaluate_sql_correctness_direct_api,\n",
    "        evaluate_final_answer_quality_direct_api\n",
    "    ]\n",
    "    print(f\"Created list 'all_final_evaluators' with {len(all_final_evaluators)} functions.\")\n",
    "except NameError as e:\n",
    "    print(f\"Error: One or more evaluator functions not defined: {e}\")\n",
    "    all_final_evaluators = None # Prevent running experiment if list creation failed\n",
    "\n",
    "# --- Define Dummy Task Function (if not already defined) ---\n",
    "# This function simply passes the input data through, as the agent results are pre-computed in the dataset.\n",
    "if 'dummy_task_function' not in locals():\n",
    "    print(\"Defining dummy_task_function...\")\n",
    "    def dummy_task_function(example):\n",
    "        \"\"\"Takes an example and returns its input field.\"\"\"\n",
    "        # Ensure it returns a dictionary suitable for the evaluators\n",
    "        return example.input if hasattr(example, 'input') else {}\n",
    "    print(\"dummy_task_function defined.\")\n",
    "\n",
    "\n",
    "# --- Prerequisite Check for Run ---\n",
    "run_passed = True\n",
    "if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "    print(\"Run FAIL: evaluation_dataset not found or empty.\")\n",
    "    run_passed = False\n",
    "if 'dummy_task_function' not in locals():\n",
    "     print(\"Run FAIL: dummy_task_function not defined.\")\n",
    "     run_passed = False\n",
    "if not all_final_evaluators or len(all_final_evaluators) != 3:\n",
    "     print(\"Run FAIL: Evaluator list 'all_final_evaluators' not ready.\")\n",
    "     run_passed = False\n",
    "# --- End Prerequisite Check ---\n",
    "\n",
    "\n",
    "if run_passed:\n",
    "    print(\"\\nRunning Phoenix experiment...\")\n",
    "    now_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    experiment_name = f\"Full_OpenAI_Eval_{now_str}\"\n",
    "\n",
    "    # Call run_experiment with positional dataset/task_fn and keyword evaluators\n",
    "    try:\n",
    "        experiment_run = run_experiment(\n",
    "            evaluation_dataset,         # 1st Positional: Dataset\n",
    "            dummy_task_function,        # 2nd Positional: Task Function\n",
    "            evaluators=all_final_evaluators, # Keyword: List of evaluator functions\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_description=\"Full evaluation using direct OpenAI calls for ToolUsage, SQLCorrectness, FinalAnswer.\"\n",
    "            # concurrency=... # Optional: Adjust concurrency if needed\n",
    "        )\n",
    "        print(f\"\\nExperiment '{experiment_name}' run initiated.\")\n",
    "        print(\"Check the Phoenix UI for detailed results and scores from all evaluators.\")\n",
    "        # The experiment summary table will print automatically below if successful.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during run_experiment: {e}\")\n",
    "        print(\"Please check function signatures, dataset structure, and Phoenix connection.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- Experiment Run Aborted due to failed prerequisites ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Evaluation Warnings and Zero Scores\n",
    "\n",
    "The previous execution of `run_experiment` for the `gpt-4o` dataset completed, but generated warnings (`Skipping evaluation: Missing 'user_query' or 'tool_called' in output`) and resulted in average scores of 0.0 for all evaluators.\n",
    "\n",
    "**Cause:**\n",
    "\n",
    "This happened because the `dummy_task_function` (defined earlier) simply returned the raw `.input` data from each example. This raw data has a nested structure (e.g., the user query is inside `input['messages'][1]['content']`). However, our evaluator functions (`evaluate_tool_usage_direct_api`, etc.) expect to receive a flat dictionary with top-level keys like `user_query`, `tool_called`, `final_answer`, and `generated_sql`. Since these keys weren't present at the top level of the data passed to the evaluators, they skipped the evaluation or defaulted to a 0.0 score.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "To fix this, we need to **redefine `dummy_task_function`**. The new definition (in the next cell) will correctly extract the required fields from the nested `example.input` data structure and return a flat dictionary in the format expected by our evaluator functions.\n",
    "\n",
    "After redefining `dummy_task_function`, we will re-run the `run_experiment` cell for the `gpt-4o` dataset. The warnings should disappear, and the evaluators should now compute meaningful scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Redefine dummy_task_function to extract data correctly\n",
    "\n",
    "import json # Ensure json is imported\n",
    "from phoenix.experiments.types import Example # Ensure Example is imported\n",
    "\n",
    "# --- Define Dummy Task Function (Corrected Extraction) ---\n",
    "# This version extracts the fields needed by the evaluators from the example input.\n",
    "print(\"\\n--- Re-defining dummy_task_function with data extraction ---\")\n",
    "\n",
    "# Helper function to safely extract user query from messages list\n",
    "def get_user_query(messages_list):\n",
    "    if not isinstance(messages_list, list): return None\n",
    "    for msg in messages_list:\n",
    "        if isinstance(msg, dict) and msg.get(\"role\") == \"user\" and \"content\" in msg:\n",
    "            return msg[\"content\"]\n",
    "    return None\n",
    "\n",
    "# Helper to find the final assistant answer (can be refined based on agent log structure)\n",
    "def get_final_answer(messages_list):\n",
    "     if not isinstance(messages_list, list): return None\n",
    "     for msg in reversed(messages_list): # Look from the end\n",
    "         if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\":\n",
    "             # Prefer final 'content' if available\n",
    "             if msg.get(\"content\"):\n",
    "                 return msg.get(\"content\")\n",
    "             # If no content, maybe check if tool call was the last action? Requires more context.\n",
    "     return None # Fallback\n",
    "\n",
    "# Helper to check if a specific tool was called\n",
    "def check_tool_called(messages_list, tool_name=\"query_database\"):\n",
    "    if not isinstance(messages_list, list): return False\n",
    "    for msg in messages_list:\n",
    "        if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n",
    "             for tool_call in msg.get(\"tool_calls\", []):\n",
    "                  if isinstance(tool_call, dict) and tool_call.get(\"function\", {}).get(\"name\") == tool_name:\n",
    "                       return True\n",
    "    return False\n",
    "\n",
    "# Helper to extract SQL query\n",
    "def get_generated_sql(messages_list):\n",
    "    if not isinstance(messages_list, list): return None\n",
    "    for msg in messages_list:\n",
    "         if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n",
    "              for tool_call in msg.get(\"tool_calls\", []):\n",
    "                   if isinstance(tool_call, dict) and tool_call.get(\"function\", {}).get(\"name\") == \"query_database\":\n",
    "                        try:\n",
    "                             args = json.loads(tool_call.get(\"function\", {}).get(\"arguments\", \"{}\"))\n",
    "                             return args.get(\"sql_query\")\n",
    "                        except json.JSONDecodeError:\n",
    "                             return None # Argument parsing failed\n",
    "    return None\n",
    "\n",
    "\n",
    "def dummy_task_function(example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts relevant fields from the example.input (which contains the raw run data)\n",
    "    and returns a FLAT dictionary matching the structure expected by the evaluators.\n",
    "    \"\"\"\n",
    "    if not hasattr(example, 'input') or not isinstance(example.input, dict):\n",
    "        return {} # Return empty if input is missing or not a dict\n",
    "\n",
    "    input_data = example.input\n",
    "    # Assuming the structure like {\"messages\": [...]} is consistent in example.input\n",
    "    messages = input_data.get(\"messages\", [])\n",
    "\n",
    "    # Extract the data pieces needed by the evaluators\n",
    "    user_query = get_user_query(messages)\n",
    "    final_answer = get_final_answer(messages)\n",
    "    tool_called = check_tool_called(messages)\n",
    "    generated_sql = get_generated_sql(messages)\n",
    "\n",
    "    # Return the flat dictionary that evaluators expect as their 'output' argument\n",
    "    return {\n",
    "        \"user_query\": user_query,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"tool_called\": tool_called,\n",
    "        \"generated_sql\": generated_sql\n",
    "    }\n",
    "\n",
    "print(\"dummy_task_function re-defined with extraction logic.\")\n",
    "\n",
    "# --- Quick test of the function (Optional but recommended) ---\n",
    "if 'evaluation_dataset_gpt4o' in locals() and len(evaluation_dataset_gpt4o) > 0:\n",
    "    print(\"\\nTesting updated dummy_task_function with the first gpt4o example:\")\n",
    "    test_output = dummy_task_function(evaluation_dataset_gpt4o[0])\n",
    "    print(\"Output from updated dummy task:\")\n",
    "    print(json.dumps(test_output, indent=2))\n",
    "    # Check if expected keys are present:\n",
    "    print(f\"  Keys present: {list(test_output.keys())}\")\n",
    "    assert \"user_query\" in test_output, \"user_query missing!\"\n",
    "    assert \"tool_called\" in test_output, \"tool_called missing!\"\n",
    "    assert \"final_answer\" in test_output, \"final_answer missing!\"\n",
    "    # generated_sql might be None if tool wasn't called, which is okay\n",
    "    print(\"  Test output structure seems correct.\")\n",
    "\n",
    "elif 'evaluation_dataset' in locals() and len(evaluation_dataset) > 0:\n",
    "     print(\"\\nTesting updated dummy_task_function with the first baseline example:\")\n",
    "     test_output = dummy_task_function(evaluation_dataset[0]) # Test with baseline data structure too if possible\n",
    "     print(\"Output from updated dummy task:\")\n",
    "     print(json.dumps(test_output, indent=2))\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping dummy_task_function test, datasets not ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to Run Experiment on GPT-4o Dataset (with function redefined inside)\n",
    "\n",
    "import json # Ensure json is imported\n",
    "import phoenix as px # Ensure phoenix is imported if needed\n",
    "from phoenix.experiments import run_experiment\n",
    "from datetime import datetime\n",
    "from typing import List, Callable, Dict, Any # Ensure imports\n",
    "from phoenix.experiments.types import Example # Make sure Example is imported\n",
    "\n",
    "# --- Re-define Dummy Task Function HERE to ensure it's used ---\n",
    "print(\"\\n--- Defining/Re-defining dummy_task_function with extraction logic IN THIS CELL ---\")\n",
    "\n",
    "# Helper function to safely extract user query from messages list\n",
    "def get_user_query(messages_list):\n",
    "    if not isinstance(messages_list, list): return None\n",
    "    for msg in messages_list:\n",
    "        if isinstance(msg, dict) and msg.get(\"role\") == \"user\" and \"content\" in msg:\n",
    "            return msg[\"content\"]\n",
    "    return None\n",
    "\n",
    "# Helper to find the final assistant answer\n",
    "def get_final_answer(messages_list):\n",
    "     if not isinstance(messages_list, list): return None\n",
    "     for msg in reversed(messages_list): # Look from the end\n",
    "         if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\":\n",
    "             # Prefer final 'content' if available\n",
    "             if msg.get(\"content\"):\n",
    "                 return msg.get(\"content\")\n",
    "             # If no content, maybe check if tool call was the last action? Requires more context.\n",
    "     return None # Fallback\n",
    "\n",
    "# Helper to check if a specific tool was called\n",
    "def check_tool_called(messages_list, tool_name=\"query_database\"):\n",
    "    if not isinstance(messages_list, list): return False\n",
    "    for msg in messages_list:\n",
    "        if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n",
    "             for tool_call in msg.get(\"tool_calls\", []):\n",
    "                  if isinstance(tool_call, dict) and tool_call.get(\"function\", {}).get(\"name\") == tool_name:\n",
    "                       return True\n",
    "    return False\n",
    "\n",
    "# Helper to extract SQL query\n",
    "def get_generated_sql(messages_list):\n",
    "    if not isinstance(messages_list, list): return None\n",
    "    for msg in messages_list:\n",
    "         if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n",
    "              for tool_call in msg.get(\"tool_calls\", []):\n",
    "                   if isinstance(tool_call, dict) and tool_call.get(\"function\", {}).get(\"name\") == \"query_database\":\n",
    "                        try:\n",
    "                             # Use .get() with default to handle potential missing keys safely\n",
    "                             func_dict = tool_call.get(\"function\", {})\n",
    "                             args_str = func_dict.get(\"arguments\", \"{}\")\n",
    "                             args = json.loads(args_str)\n",
    "                             return args.get(\"sql_query\")\n",
    "                        except json.JSONDecodeError:\n",
    "                             return None # Argument parsing failed\n",
    "                        except Exception: # Catch other potential errors during access\n",
    "                             return None\n",
    "    return None\n",
    "\n",
    "# The actual dummy task function definition\n",
    "def dummy_task_function(example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts relevant fields from the example.input (which contains the raw run data)\n",
    "    and returns a FLAT dictionary matching the structure expected by the evaluators.\n",
    "    \"\"\"\n",
    "    if not hasattr(example, 'input') or not isinstance(example.input, dict):\n",
    "        return {} # Return empty if input is missing or not a dict\n",
    "\n",
    "    input_data = example.input\n",
    "    # Assuming the structure like {\"messages\": [...]} is consistent in example.input\n",
    "    messages = input_data.get(\"messages\", [])\n",
    "\n",
    "    # Extract the data pieces needed by the evaluators\n",
    "    user_query = get_user_query(messages)\n",
    "    final_answer = get_final_answer(messages)\n",
    "    tool_called = check_tool_called(messages)\n",
    "    generated_sql = get_generated_sql(messages)\n",
    "\n",
    "    # Return the flat dictionary that evaluators expect as their 'output' argument\n",
    "    return {\n",
    "        \"user_query\": user_query,\n",
    "        \"final_answer\": final_answer,\n",
    "        \"tool_called\": tool_called,\n",
    "        \"generated_sql\": generated_sql\n",
    "    }\n",
    "\n",
    "print(\"dummy_task_function defined/re-defined in this cell.\")\n",
    "# --- End Function Definition ---\n",
    "\n",
    "\n",
    "# --- Now proceed with running the experiment ---\n",
    "# Check if the gpt4o dataset variable is loaded from previous cell\n",
    "if 'evaluation_dataset_gpt4o' in locals() and len(evaluation_dataset_gpt4o) > 0:\n",
    "    missing_deps = []\n",
    "    if 'dummy_task_function' not in locals(): missing_deps.append(\"dummy_task_function\") # Should be found now\n",
    "    # Ensure the evaluator list from earlier setup cells is available\n",
    "    if 'all_final_evaluators' not in locals(): missing_deps.append(\"all_final_evaluators list\")\n",
    "    if 'px_client' not in locals() or px_client is None: missing_deps.append(\"px_client\")\n",
    "\n",
    "    if not missing_deps:\n",
    "        print(\"\\nRunning full experiment for GPT-4o using direct API evaluators...\")\n",
    "        now_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        # Give this experiment run a distinct name\n",
    "        experiment_name_gpt4o = f\"GPT4o_Full_OpenAI_Eval_{now_str}\" # Matched naming convention\n",
    "\n",
    "        # Define type hint for evaluator list (if not defined globally)\n",
    "        EvaluatorList = List[Callable[[Dict[str, Any]], float]]\n",
    "\n",
    "        # Assuming run_experiment works without explicit env vars here\n",
    "        try:\n",
    "            full_experiment_gpt4o = run_experiment(\n",
    "                evaluation_dataset_gpt4o,           # Use the GPT-4o dataset variable\n",
    "                dummy_task_function,                # Uses the function defined above in this cell\n",
    "                evaluators=all_final_evaluators,   # Re-use the same evaluators list\n",
    "                experiment_name=experiment_name_gpt4o, # Use the new, distinct experiment name\n",
    "                experiment_description=\"Eval GPT-4o: Direct API ToolUsage, SQLCorrectness, FinalAnswer.\" # Updated description\n",
    "                # concurrency=5 # Optional: Adjust concurrency if needed\n",
    "            )\n",
    "\n",
    "            print(f\"\\nFull Experiment '{experiment_name_gpt4o}' run initiated.\")\n",
    "            print(f\"Check Phoenix UI for results associated with experiment name '{experiment_name_gpt4o}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch potential errors like Connection refused if it reappears\n",
    "            print(f\"\\nERROR during run_experiment for GPT-4o: {e}\")\n",
    "            print(\"If this is a connection error, consider adding os.environ lines before the call.\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping GPT-4o experiment - missing dependencies: {', '.join(missing_deps)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Skipping GPT-4o experiment - 'evaluation_dataset_gpt4o' was not found or is empty. Did you run the cell above to load it?\")\n",
    "\n",
    "print(\"\\n--- End GPT-4o Full Experiment ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluator 3: Final Answer Quality (Mimicking Successful Pattern) ---\n",
    "import re\n",
    "import logging # Or use print if preferred for errors\n",
    "# Assumes openai_client is initialized globally\n",
    "# Assumes MODEL_TO_USE = \"gpt-4o\" is defined globally\n",
    "\n",
    "# --- Define Prompt Template ---\n",
    "# Make sure this is defined before the function\n",
    "FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE = \"\"\"Evaluate if the Final Answer accurately and completely answers the User Query, based ONLY on the query and answer text. Do not assume external data or SQL.\n",
    "\n",
    "User Query:\n",
    "{user_query}\n",
    "\n",
    "Final Answer:\n",
    "{final_answer}\n",
    "\n",
    "Is the Final Answer good quality (accurate, relevant, complete)?\n",
    "Provide a brief EXPLANATION and finish with LABEL: Good or LABEL: Bad.\n",
    "\"\"\"\n",
    "print(\"Defined: FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE\") # Optional confirmation\n",
    "\n",
    "# --- Evaluator Definition ---\n",
    "def evaluate_final_answer_quality_direct_api(output: dict, expected: dict) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates Final Answer quality via direct OpenAI API call (Mimics successful pattern).\n",
    "    Returns 1.0 for Good, 0.0 otherwise (Bad, Error, Parsing Failure).\n",
    "    \"\"\"\n",
    "    # Rely on global variables: openai_client, FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE, MODEL_TO_USE\n",
    "    score = 0.0 # Default to 0.0 (representing \"Bad\" or error)\n",
    "\n",
    "    # --- 1. Prerequisites & Data Checks ---\n",
    "    if not openai_client:\n",
    "        print(\"Prerequisite Error: OpenAI client not initialized.\") # Changed from logging\n",
    "        return 0.0\n",
    "    if not FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE:\n",
    "        print(\"Prerequisite Error: FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE not defined.\")\n",
    "        return 0.0\n",
    "\n",
    "    user_query = output.get('user_query')\n",
    "    final_answer = output.get('final_answer')\n",
    "\n",
    "    if not user_query or not final_answer:\n",
    "        # print(\"Data Error: Missing 'user_query' or 'final_answer' in output.\") # Optional\n",
    "        return 0.0\n",
    "\n",
    "    # --- 2. Format Prompt ---\n",
    "    try:\n",
    "        prompt = FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE.format(\n",
    "            user_query=user_query,\n",
    "            final_answer=final_answer\n",
    "        )\n",
    "    except KeyError as e:\n",
    "         print(f\"Prompt Formatting Error: Missing key {e}\")\n",
    "         return 0.0\n",
    "\n",
    "    # --- 3. Call API & Parse Result ---\n",
    "    try:\n",
    "        # Assuming call_openai_judge helper is defined and available\n",
    "        raw_output = call_openai_judge(prompt, model=MODEL_TO_USE)\n",
    "        label_match = re.search(r\"LABEL:\\s*(\\w+)\", raw_output, re.IGNORECASE)\n",
    "\n",
    "        # Return 1.0 ONLY if API succeeded AND label is exactly \"Good\"\n",
    "        if label_match and label_match.group(1).strip().capitalize() == \"Good\":\n",
    "            score = 1.0\n",
    "        # else: score remains 0.0\n",
    "\n",
    "    except NameError:\n",
    "        # Explicitly catch if the helper function isn't defined\n",
    "         print(\"Error: call_openai_judge function not found.\")\n",
    "         # score remains 0.0\n",
    "    except Exception as e:\n",
    "        # Covers cases: OpenAI API call failed inside helper, or other unexpected errors\n",
    "        print(f\"API Call/Evaluation Error during Final Answer evaluation: {e}\")\n",
    "        # score remains 0.0\n",
    "\n",
    "    return score\n",
    "\n",
    "print(\"Defined: evaluate_final_answer_quality_direct_api function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Full Evaluation & Comparison for Final Answer Quality ---\n",
    "import pandas as pd\n",
    "import re # Ensure re is imported\n",
    "\n",
    "print(\"\\n--- Running Full Evaluation for Final Answer Quality ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Key for the human label in evaluation_dataset[i].output\n",
    "# Identified from notebook inspection as 'final_answer_quality_label'\n",
    "HUMAN_LABEL_KEY_ANSWER = 'final_answer_quality_label'\n",
    "\n",
    "# Expected Labels from the LLM for this evaluator\n",
    "# Note: Dataset uses \"Fail\" but our prompt asks for \"Bad\". We need to map.\n",
    "HUMAN_LABEL_MAP = {\"Fail\": \"Bad\"} # Map human label \"Fail\" to expected LLM \"Bad\"\n",
    "LLM_POSITIVE_LABEL = \"Good\" # What our prompt asks for as the \"good\" label\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "# --- Prerequisite Check ---\n",
    "eval_passed = True\n",
    "if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "    print(\"Evaluation FAIL: evaluation_dataset not found or empty.\")\n",
    "    eval_passed = False\n",
    "if 'openai_client' not in locals() or not openai_client:\n",
    "    print(\"Evaluation FAIL: openai_client not initialized.\")\n",
    "    eval_passed = False\n",
    "if 'FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE' not in locals() or not FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE:\n",
    "     print(\"Evaluation FAIL: FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE not defined.\")\n",
    "     eval_passed = False\n",
    "if 'evaluate_final_answer_quality_direct_api' not in locals():\n",
    "     print(\"Evaluation FAIL: evaluate_final_answer_quality_direct_api function not defined.\")\n",
    "     eval_passed = False\n",
    "if 'call_openai_judge' not in locals():\n",
    "     print(\"Evaluation FAIL: call_openai_judge function not defined.\")\n",
    "     eval_passed = False\n",
    "# --- End Prerequisite Check ---\n",
    "\n",
    "answer_results_list = [] # Initialize list for results\n",
    "\n",
    "if eval_passed:\n",
    "    print(f\"Processing {len(evaluation_dataset)} examples for Final Answer Quality...\")\n",
    "    # Iterate through ALL examples\n",
    "    for i, test_example in enumerate(evaluation_dataset):\n",
    "        # print(f\"Processing Example {i}...\") # Can uncomment for verbose progress\n",
    "\n",
    "        test_output_data = test_example.input\n",
    "        test_expected_data = test_example.output # needed for function signature\n",
    "\n",
    "        user_query = test_output_data.get('user_query', 'MISSING')\n",
    "        final_answer = test_output_data.get('final_answer', 'MISSING')\n",
    "        human_label_raw = test_example.output.get(HUMAN_LABEL_KEY_ANSWER, \"MISSING_KEY\")\n",
    "\n",
    "        # --- Call LLM judge directly for raw output ---\n",
    "        raw_judge_response_answer = \"Skipped direct call\" # Default\n",
    "        if user_query != 'MISSING' and final_answer != 'MISSING':\n",
    "            try:\n",
    "                test_prompt_answer = FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE.format(\n",
    "                    user_query=user_query,\n",
    "                    final_answer=final_answer\n",
    "                )\n",
    "                raw_judge_response_answer = call_openai_judge(test_prompt_answer, model=MODEL_TO_USE)\n",
    "            except Exception as e:\n",
    "                # print(f\"  Error calling LLM Judge directly on Example {i}: {e}\") # Verbose error\n",
    "                raw_judge_response_answer = f\"Error during direct call: {e}\"\n",
    "        # else:\n",
    "             # print(f\"  Skipping direct LLM call on Example {i} due to missing query/answer.\")\n",
    "\n",
    "\n",
    "        # --- Call the evaluator function ---\n",
    "        test_score_float_answer = 0.0 # Default score\n",
    "        try:\n",
    "            test_score_float_answer = evaluate_final_answer_quality_direct_api(test_output_data, test_expected_data)\n",
    "        except Exception as e:\n",
    "            # print(f\"  Error calling evaluator function on Example {i}: {e}\") # Verbose error\n",
    "            test_score_float_answer = 0.0 # Assign 0.0 on error\n",
    "\n",
    "        # --- Append results to list ---\n",
    "        answer_results_list.append({\n",
    "            \"index\": i,\n",
    "            \"user_query\": user_query,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"human_answer_label_raw\": human_label_raw, # Store the original human label\n",
    "            \"raw_llm_response_answer\": raw_judge_response_answer,\n",
    "            \"final_answer_score\": test_score_float_answer\n",
    "        })\n",
    "\n",
    "    # --- Convert list to DataFrame ---\n",
    "    print(\"\\nEvaluation loop complete. Creating DataFrame...\")\n",
    "    answer_results_df = pd.DataFrame(answer_results_list)\n",
    "\n",
    "    # --- Add Comparison Columns ---\n",
    "    print(\"Adding comparison columns...\")\n",
    "    # Map human label \"Fail\" to \"Bad\" for comparison\n",
    "    answer_results_df['human_answer_label_mapped'] = answer_results_df['human_answer_label_raw'].map(HUMAN_LABEL_MAP).fillna(answer_results_df['human_answer_label_raw'])\n",
    "\n",
    "    # Convert score (1.0/0.0) to LLM label (\"Good\"/\"Bad\")\n",
    "    def answer_score_to_label(score):\n",
    "        return LLM_POSITIVE_LABEL if score == 1.0 else \"Bad\" # Assumes 0.0 means Bad\n",
    "\n",
    "    answer_results_df['llm_answer_label'] = answer_results_df['final_answer_score'].apply(answer_score_to_label)\n",
    "\n",
    "    # Compare mapped human label with LLM label (case-insensitive)\n",
    "    answer_results_df['answer_match'] = answer_results_df.apply(\n",
    "        lambda row: str(row['llm_answer_label']).lower() == str(row['human_answer_label_mapped']).lower()\n",
    "                    if row['human_answer_label_mapped'] not in [\"MISSING\", \"MISSING_KEY\"] else None,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Display DataFrame ---\n",
    "    print(\"\\nFinal Answer Quality Evaluation Results:\")\n",
    "    # Select and reorder columns\n",
    "    display_cols_answer = ['index', 'user_query', 'final_answer', 'human_answer_label_raw', 'llm_answer_label', 'answer_match', 'raw_llm_response_answer']\n",
    "    display_cols_answer = [col for col in display_cols_answer if col in answer_results_df.columns] # Ensure columns exist\n",
    "\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    pd.set_option('display.max_colwidth', 200)\n",
    "    display(answer_results_df[display_cols_answer])\n",
    "\n",
    "    # --- Calculate Match Percentage ---\n",
    "    if 'answer_match' in answer_results_df.columns:\n",
    "        match_count_answer = answer_results_df['answer_match'].sum() # Counts True values\n",
    "        valid_comparisons_answer = answer_results_df['answer_match'].notna().sum() # Counts non-None values\n",
    "        if valid_comparisons_answer > 0:\n",
    "            match_percentage_answer = (match_count_answer / valid_comparisons_answer) * 100\n",
    "            print(f\"\\nAgreement between LLM Judge and Human Labels (Final Answer): {match_percentage_answer:.2f}% ({match_count_answer}/{valid_comparisons_answer})\")\n",
    "        else:\n",
    "            print(\"\\nCould not calculate agreement percentage (no valid comparisons).\")\n",
    "\n",
    "else:\n",
    "    print(\"--- Final Answer Evaluation Aborted due to failed prerequisites ---\")\n",
    "\n",
    "print(f\"\\n--- End Final Answer Quality Evaluation ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import re # Needed for parsing\n",
    "    from typing import Dict, Any # Added type hinting\n",
    "\n",
    "    # --- Evaluator 2: SQL Correctness (Direct API Call) ---\n",
    "\n",
    "    SQL_CORRECTNESS_PROMPT_TEMPLATE = \"\"\"Evaluate if the Generated SQL is semantically correct and appropriate for the User Query. Consider typical schemas (e.g., transcript_segments table). Ignore Final Answer quality.\n",
    "\n",
    "    User Query:\n",
    "    {user_query}\n",
    "\n",
    "    Generated SQL:\n",
    "    {generated_sql}\n",
    "\n",
    "    Is the SQL correct and appropriate? Answer ONLY \"Correct\" or \"Incorrect\".\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate_sql_correctness_direct_api(example: Dict[str, Any]) -> float:\n",
    "        \"\"\"Evaluates SQL correctness via direct LLM API call (1.0 Correct, 0.0 Incorrect).\"\"\"\n",
    "        input_data = example.get(\"input\", {})\n",
    "        user_query = input_data.get(\"user_query\", \"\")\n",
    "        generated_sql = input_data.get(\"generated_sql\", \"\")\n",
    "\n",
    "        if not user_query or not generated_sql: return 0.0\n",
    "\n",
    "        prompt = SQL_CORRECTNESS_PROMPT_TEMPLATE.format(user_query=user_query, generated_sql=generated_sql)\n",
    "\n",
    "        try:\n",
    "            response_text = call_gemini(prompt)\n",
    "            return 1.0 if re.search(r\"Correct\", response_text, re.IGNORECASE) else 0.0\n",
    "        except Exception as e:\n",
    "            # print(f\"Error during SQL correctness direct API call: {e}\") # Keep commented unless debugging\n",
    "            return 0.0\n",
    "\n",
    "    # --- Evaluator 3: Final Answer Quality (Direct API Call) ---\n",
    "\n",
    "    FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE = \"\"\"Evaluate if the Final Answer accurately and completely answers the User Query, based ONLY on the query and answer text. Do not assume external data or SQL.\n",
    "\n",
    "    User Query:\n",
    "    {user_query}\n",
    "\n",
    "    Final Answer:\n",
    "    {final_answer}\n",
    "\n",
    "    Is the Final Answer Good or Bad? Answer ONLY \"Good\" or \"Bad\".\n",
    "    \"\"\"\n",
    "\n",
    "    def evaluate_final_answer_quality_direct_api(example: Dict[str, Any]) -> float:\n",
    "        \"\"\"Evaluates Final Answer quality via direct LLM API call (1.0 Good, 0.0 Bad).\"\"\"\n",
    "        input_data = example.get(\"input\", {})\n",
    "        user_query = input_data.get(\"user_query\", \"\")\n",
    "        final_answer = input_data.get(\"final_answer\", \"\")\n",
    "\n",
    "        if not user_query or not final_answer: return 0.0\n",
    "\n",
    "        prompt = FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE.format(user_query=user_query, final_answer=final_answer)\n",
    "\n",
    "        try:\n",
    "            response_text = call_gemini(prompt)\n",
    "            return 1.0 if re.search(r\"Good\", response_text, re.IGNORECASE) else 0.0\n",
    "        except Exception as e:\n",
    "            # print(f\"Error during Final Answer quality direct API call: {e}\") # Keep commented unless debugging\n",
    "            return 0.0\n",
    "\n",
    "    print(\"Defined direct API evaluators: evaluate_sql_correctness_direct_api, evaluate_final_answer_quality_direct_api\")\n",
    "\n",
    "    # Combine all custom evaluators\n",
    "    all_custom_evaluators = [\n",
    "        evaluate_tool_usage_direct_api,\n",
    "        evaluate_sql_correctness_direct_api,\n",
    "        evaluate_final_answer_quality_direct_api\n",
    "    ]\n",
    "    print(f\"Combined evaluator list created with {len(all_custom_evaluators)} evaluators.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Experiment with All Custom Evaluators\n",
    "\n",
    "Now we execute the Phoenix `run_experiment` function using:\n",
    "* The `evaluation_dataset`.\n",
    "* The `dummy_task_function` (as a placeholder for the actual agent).\n",
    "* Our list of all three custom direct API evaluators (`all_custom_evaluators`).\n",
    "\n",
    "This will run the evaluation process for each example in the dataset, calling our custom functions to generate scores for Tool Usage, SQL Correctness, and Final Answer Quality based on the direct API calls.\n",
    "\n",
    "The results, including inputs, outputs, and scores, will be logged to a new, timestamped experiment in the Phoenix UI, allowing for detailed analysis and comparison later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Full Experiment with All Custom Direct API Evaluators (Corrected Call) ---\n",
    "import phoenix as px\n",
    "from phoenix.experiments import run_experiment\n",
    "from datetime import datetime\n",
    "from typing import List, Callable, Dict, Any # Added imports\n",
    "\n",
    "# Ensure necessary components are loaded\n",
    "missing = []\n",
    "if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0: missing.append(\"evaluation_dataset\")\n",
    "if 'dummy_task_function' not in locals(): missing.append(\"dummy_task_function\")\n",
    "if 'all_custom_evaluators' not in locals() or len(all_custom_evaluators) != 3: missing.append(\"all_custom_evaluators list\")\n",
    "if 'px_client' not in locals(): missing.append(\"px_client\")\n",
    "\n",
    "if not missing:\n",
    "    print(\"Running full experiment with all custom direct API evaluators...\")\n",
    "    now_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    experiment_name = f\"FullDirectAPI_Eval_{now_str}\"\n",
    "\n",
    "    # Define type hint for evaluator list\n",
    "    EvaluatorList = List[Callable[[Dict[str, Any]], float]]\n",
    "\n",
    "    # *** Corrected call: Pass dummy_task_function as the second positional argument ***\n",
    "    full_experiment = run_experiment(\n",
    "        evaluation_dataset,                 # First positional: dataset\n",
    "        dummy_task_function,                # Second positional: task function\n",
    "        evaluators=all_custom_evaluators,   # Keyword: evaluators\n",
    "        experiment_name=experiment_name,    # Keyword: experiment_name\n",
    "        experiment_description=\"Eval: Direct API ToolUsage, SQLCorrectness, FinalAnswer.\" # Keyword: description\n",
    "        # concurrency=5\n",
    "    )\n",
    "\n",
    "    print(f\"\\nFull Experiment '{experiment_name}' run initiated.\")\n",
    "    print(\"Check Phoenix UI for results.\")\n",
    "    # if hasattr(px_client, 'get_endpoint'): print(f\"Phoenix UI Endpoint: {px_client.get_endpoint()}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Skipping full experiment - missing: {', '.join(missing)}\")\n",
    "\n",
    "print(\"\\n--- End Full Experiment ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Running the Agent with a Different Model (`gpt-4o`)\n",
    "\n",
    "Having established a baseline run and evaluation using the **`gpt-4o-mini`** model, our next step is to run an experiment comparing its performance against a different model, **`gpt-4o`**.\n",
    "\n",
    "The process involves:\n",
    "1.  Modifying the `MODEL` constant in `src/agent/agent.py` to `\"gpt-4o\"`.\n",
    "2.  Re-running the agent script (`python src/agent/agent.py`), which executes the predefined test queries using the new model.\n",
    "3.  Crucially, ensuring the OpenTelemetry traces for this experimental run are captured in Arize Phoenix so we can directly compare latency, token usage, and potentially tool calls between the `gpt-4o-mini` (baseline) and `gpt-4o` runs within the Phoenix UI.\n",
    "\n",
    "#### Challenge: Capturing Traces for the Experimental Run (`gpt-4o`)\n",
    "\n",
    "When we attempted to re-run `src/agent/agent.py` (after changing the model to `gpt-4o`), we unexpectedly hit significant issues with the Arize Phoenix tracing configuration. Even though the tracing setup code and the `.env` file *hadn't changed* since the previous successful baseline run (or seemed straightforward based on initial setup), the script failed to connect to the Phoenix cloud endpoint (`https://app.phoenix.arize.com`).\n",
    "\n",
    "**Debugging the Connection Issues:**\n",
    "\n",
    "*   **Defaulting to Localhost:** The primary issue was that the `phoenix.otel.register()` function, despite having environment variables set (either standard `OTEL_...` or `PHOENIX_...`), consistently ignored the cloud endpoint and attempted to connect via **gRPC** to `localhost:4317`, resulting in `StatusCode.UNAVAILABLE` errors.\n",
    "*   **Unreliable Auto-Detection:** Attempts to guide the automatic detection by forcing the protocol (`http/protobuf`) or using specific environment variable names recommended in documentation (`PHOENIX_COLLECTOR_ENDPOINT`, `PHOENIX_CLIENT_HEADERS`) were unsuccessful in making the library use the correct hostname from the environment variables. It kept defaulting to `localhost` (either port 4317 for gRPC or 6006 for HTTP).\n",
    "\n",
    "**Working Solution: Explicit Configuration:**\n",
    "\n",
    "The only reliable way to ensure traces were sent correctly to `https://app.phoenix.arize.com` for this experimental run was to **bypass the automatic environment variable detection entirely within the `phoenix.otel.register()` function.**\n",
    "\n",
    "This required modifying `src/agent/agent.py` for the tracing setup block:\n",
    "1.  Ensure the `.env` file contains the correct header variable as specified in the Phoenix cloud documentation: `PHOENIX_CLIENT_HEADERS=\"api_key=YOUR_KEY_VALUE\"`.\n",
    "2.  Update the Python code to:\n",
    "    *   Explicitly define the full endpoint URL: `endpoint = \"https://app.phoenix.arize.com/v1/traces\"`\n",
    "    *   Read the `PHOENIX_CLIENT_HEADERS` environment variable using `os.getenv()`.\n",
    "    *   Parse the header string into the required dictionary format: `headers_dict = {\"api_key\": \"YOUR_KEY_VALUE\"}`.\n",
    "    *   Pass these directly to the registration function:\n",
    "        ```python\n",
    "        phoenix_tracer_provider = register(\n",
    "            project_name=PROJECT_NAME,\n",
    "            endpoint=endpoint,\n",
    "            headers=headers_dict\n",
    "        )\n",
    "        ```\n",
    "\n",
    "**Outcome:**\n",
    "\n",
    "With this explicit configuration hardcoded in the script, the agent successfully connected to Phoenix, and the traces for our `gpt-4o` experimental run were captured. This allows us to proceed with comparing the two models (`gpt-4o-mini` vs `gpt-4o`) within the Phoenix UI. This troubleshooting detour highlights potential fragility in automatic OTel configuration detection and underscores the utility of explicit configuration when encountering connection problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to Load GPT-4o Dataset\n",
    "\n",
    "import json # Make sure json is imported\n",
    "\n",
    "# This is the name we decided on for the dataset created in the UI.\n",
    "# Make sure this exactly matches the name in your Phoenix UI.\n",
    "new_dataset_name = \"Experiment_GPT4o_AllSpans\"\n",
    "\n",
    "print(f\"\\nAttempting to load dataset '{new_dataset_name}'...\")\n",
    "\n",
    "# Check if client was initialized successfully in the setup cell\n",
    "if 'px_client' is None or 'px_client' not in locals():\n",
    "    raise NameError(\"Phoenix client 'px_client' was not initialized successfully. Please re-run the modified Setup Cell (#1).\")\n",
    "\n",
    "# Load the specified dataset by its exact name into a NEW variable\n",
    "evaluation_dataset_gpt4o = px_client.get_dataset(name=new_dataset_name)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Print number of examples\n",
    "print(f\"Number of examples in new dataset '{new_dataset_name}': {len(evaluation_dataset_gpt4o)}\")\n",
    "\n",
    "# --- Inspect the first few examples ---\n",
    "num_examples_to_show = 3 # Adjust if you want to see more/less\n",
    "print(f\"\\n--- Inspecting First {num_examples_to_show} Examples from '{new_dataset_name}' ---\")\n",
    "\n",
    "if len(evaluation_dataset_gpt4o) > 0:\n",
    "    for i, example in enumerate(evaluation_dataset_gpt4o[:num_examples_to_show]):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(\"\\nInput Data:\")\n",
    "        try:\n",
    "            print(json.dumps(example.input, indent=2))\n",
    "        except Exception as e:\n",
    "            print(f\"Could not display input: {e}\")\n",
    "        print(\"\\nOutput/Label Data:\") # Check if labels are present as expected\n",
    "        try:\n",
    "            print(json.dumps(example.output, indent=2))\n",
    "        except Exception as e:\n",
    "            print(f\"Could not display output/labels: {e}\")\n",
    "        print(\"\\nMetadata:\")\n",
    "        try:\n",
    "            print(json.dumps(example.metadata, indent=2))\n",
    "        except Exception as e:\n",
    "            print(f\"Could not display metadata: {e}\")\n",
    "else:\n",
    "    print(f\"Dataset '{new_dataset_name}' appears to be empty.\")\n",
    "\n",
    "print(f\"\\n--- Finished Loading and Inspecting GPT-4o Dataset ({new_dataset_name}) ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Cell 1: Initialize Phoenix Client EXPLICITLY for Cloud\n",
    "\n",
    "    import phoenix as px\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    print(\"--- Initializing Phoenix Client Explicitly for Cloud ---\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    cloud_api_endpoint = \"https://app.phoenix.arize.com\"\n",
    "    api_headers_str = os.getenv(\"PHOENIX_CLIENT_HEADERS\") # Read from environment\n",
    "\n",
    "    if not api_headers_str:\n",
    "        raise ValueError(\"CRITICAL: PHOENIX_CLIENT_HEADERS environment variable not found.\")\n",
    "\n",
    "    api_headers_dict = {}\n",
    "    try:\n",
    "        key, value = api_headers_str.split('=', 1)\n",
    "        api_headers_dict[key.strip()] = value.strip()\n",
    "        if not api_headers_dict: raise ValueError(\"Parsed is empty.\")\n",
    "        print(f\"Found headers: Key='{list(api_headers_dict.keys())[0]}'\")\n",
    "    except Exception as parse_err:\n",
    "        raise ValueError(f\"Invalid PHOENIX_CLIENT_HEADERS format: '{api_headers_str}'. Expected 'key=value'. Error: {parse_err}\") from parse_err\n",
    "    # --- End Configuration ---\n",
    "\n",
    "    # --- Initialize Client Explicitly ---\n",
    "    try:\n",
    "        print(f\"Attempting px.Client(endpoint='{cloud_api_endpoint}', headers=...)\")\n",
    "        # Use explicit endpoint and headers\n",
    "        px_client = px.Client(endpoint=cloud_api_endpoint, headers=api_headers_dict)\n",
    "        print(\"Phoenix client initialized successfully using explicit arguments.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR initializing Phoenix Client explicitly: {e}\")\n",
    "        px_client = None\n",
    "    # --- End Initialization ---\n",
    "\n",
    "    print(\"--- Client Initialization Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Combined Data for Evaluation\n",
    "\n",
    "We now have two datasets loaded:\n",
    "\n",
    "1.  `evaluation_dataset`: Contains the results from the baseline (`gpt-4o-mini`) run. Crucially, this dataset was potentially created from examples where feedback was provided in the Phoenix UI, meaning it might not contain *all* examples from the original run and its `.output` field contains the ground truth labels and explanations derived from that UI feedback.\n",
    "2.  `evaluation_dataset_gpt4o`: Contains the results from the experimental (`gpt-4o`) run, loaded directly from the trace data. Its `.input` field contains the agent's outputs for this run (e.g., `final_answer`, `generated_sql`), but its `.output` field likely lacks the ground truth labels.\n",
    "\n",
    "Our LLM-as-a-Judge evaluators (`all_custom_evaluators`) need both the agent's output (from the `gpt-4o` run) and the corresponding ground truth labels (from the baseline dataset).\n",
    "\n",
    "Therefore, the next step is to **combine** these two datasets within the notebook. We will iterate through the examples from the `gpt-4o` run (`evaluation_dataset_gpt4o`) and, using the `user_query` as a key, attempt to find the matching example with ground truth labels in the baseline dataset (`evaluation_dataset`).\n",
    "\n",
    "We will create a new list, `combined_eval_data`, containing only the examples where a match was found. Each item in this list will have:\n",
    "*   The `user_query`.\n",
    "*   The outputs generated by the `gpt-4o` model (`gpt4o_output`).\n",
    "*   The ground truth labels and explanations (`ground_truth`) from the baseline dataset.\n",
    "\n",
    "This `combined_eval_data` list will be the input for our evaluation process in the subsequent steps. We will also report how many examples from the `gpt-4o` run could be matched and how many were skipped due to missing labels in the baseline dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Compare Datasets and Prepare Combined Data for Evaluation\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"\\n--- Comparing Datasets and Preparing Combined Data ---\")\n",
    "\n",
    "# Ensure both datasets are loaded\n",
    "if 'evaluation_dataset' not in locals() or 'evaluation_dataset_gpt4o' not in locals():\n",
    "    raise NameError(\"One or both datasets ('evaluation_dataset', 'evaluation_dataset_gpt4o') are not loaded.\")\n",
    "\n",
    "# --- Extract Baseline Labels (keyed by user_query) ---\n",
    "baseline_labels = {}\n",
    "for example in evaluation_dataset:\n",
    "    try:\n",
    "        # Assuming 'user_query' is directly in the input field\n",
    "        query = example.input.get(\"user_query\")\n",
    "        if query and example.output: # Check if query exists and there's label data\n",
    "            baseline_labels[query] = example.output\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not process baseline example: {e}\")\n",
    "\n",
    "print(f\"\\nExtracted {len(baseline_labels)} labeled examples from baseline dataset ('evaluation_dataset').\")\n",
    "if len(baseline_labels) < len(evaluation_dataset):\n",
    "     print(f\"  (Note: Some baseline examples might have been skipped if missing 'user_query' or 'output' field).\")\n",
    "\n",
    "\n",
    "# --- Extract Experimental Results (keyed by user_query) ---\n",
    "experimental_results = {}\n",
    "for example in evaluation_dataset_gpt4o:\n",
    "     try:\n",
    "        # Assuming 'user_query' is directly in the input field\n",
    "        query = example.input.get(\"user_query\")\n",
    "        if query:\n",
    "            # Store the whole input dict containing gpt-4o's answers/SQL etc.\n",
    "            experimental_results[query] = example.input\n",
    "     except Exception as e:\n",
    "        print(f\"Warning: Could not process experimental example: {e}\")\n",
    "\n",
    "print(f\"\\nExtracted {len(experimental_results)} examples from experimental dataset ('evaluation_dataset_gpt4o').\")\n",
    "\n",
    "# --- Create Combined Data for Evaluation ---\n",
    "combined_eval_data = []\n",
    "missing_labels_count = 0\n",
    "found_labels_count = 0\n",
    "\n",
    "print(\"\\nCombining data...\")\n",
    "for query, gpt4o_result in experimental_results.items():\n",
    "    if query in baseline_labels:\n",
    "        # Found corresponding labels in the baseline dataset\n",
    "        combined_item = {\n",
    "            \"user_query\": query,\n",
    "            \"gpt4o_output\": gpt4o_result, # Contains final_answer, generated_sql etc. from gpt-4o run\n",
    "            \"ground_truth\": baseline_labels[query] # Contains labels and explanations\n",
    "        }\n",
    "        combined_eval_data.append(combined_item)\n",
    "        found_labels_count += 1\n",
    "    else:\n",
    "        # No labels found for this query in the baseline dataset\n",
    "        # print(f\"  Skipping query (no labels found): {query[:80]}...\") # Uncomment to see skipped queries\n",
    "        missing_labels_count += 1\n",
    "\n",
    "print(f\"\\nSuccessfully combined data for {found_labels_count} examples.\")\n",
    "if missing_labels_count > 0:\n",
    "    print(f\"Skipped {missing_labels_count} examples from the gpt-4o run because corresponding labels were not found in the baseline dataset.\")\n",
    "\n",
    "# --- Inspect the first combined item ---\n",
    "if combined_eval_data:\n",
    "    print(\"\\n--- First Combined Example ---\")\n",
    "    print(json.dumps(combined_eval_data[0], indent=2))\n",
    "else:\n",
    "    print(\"\\nNo combined data was prepared. Check dataset alignment or content.\")\n",
    "\n",
    "print(\"\\n--- Data Preparation Complete ---\")\n",
    "# The variable 'combined_eval_data' now holds the data ready for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to Inspect Structure of Experimental Dataset Example\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"\\n--- Inspecting Structure of First Example from 'evaluation_dataset_gpt4o' ---\")\n",
    "\n",
    "if 'evaluation_dataset_gpt4o' in locals() and len(evaluation_dataset_gpt4o) > 0:\n",
    "    first_exp_example = evaluation_dataset_gpt4o[0]\n",
    "\n",
    "    print(\"\\n--- first_exp_example.input ---\")\n",
    "    try:\n",
    "        # See what keys are actually inside 'input'\n",
    "        print(f\"Keys: {list(first_exp_example.input.keys())}\")\n",
    "        print(json.dumps(first_exp_example.input, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display input: {e}\")\n",
    "\n",
    "    print(\"\\n--- first_exp_example.output ---\")\n",
    "    try:\n",
    "        # Check if 'output' contains anything useful (might be None or empty)\n",
    "        print(f\"Keys: {list(first_exp_example.output.keys())}\")\n",
    "        print(json.dumps(first_exp_example.output, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display output: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- first_exp_example.metadata ---\")\n",
    "    try:\n",
    "         # Check if metadata holds the query\n",
    "        print(f\"Keys: {list(first_exp_example.metadata.keys())}\")\n",
    "        print(json.dumps(first_exp_example.metadata, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display metadata: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Variable 'evaluation_dataset_gpt4o' not found or is empty.\")\n",
    "\n",
    "print(\"\\n--- Inspection Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Compare Datasets and Prepare Combined Data for Evaluation (Corrected Extraction)\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"\\n--- Comparing Datasets and Preparing Combined Data (Corrected Extraction) ---\")\n",
    "\n",
    "# Ensure both datasets are loaded\n",
    "if 'evaluation_dataset' not in locals() or 'evaluation_dataset_gpt4o' not in locals():\n",
    "    raise NameError(\"One or both datasets ('evaluation_dataset', 'evaluation_dataset_gpt4o') are not loaded.\")\n",
    "\n",
    "# Helper function to safely extract user query from messages list\n",
    "def get_user_query(messages_list):\n",
    "    if not isinstance(messages_list, list):\n",
    "        return None\n",
    "    for msg in messages_list:\n",
    "        if isinstance(msg, dict) and msg.get(\"role\") == \"user\" and \"content\" in msg:\n",
    "            return msg[\"content\"]\n",
    "    return None\n",
    "\n",
    "# --- Extract Baseline Labels (keyed by user_query) ---\n",
    "# Assumes baseline dataset structure also has query in input (potentially different structure)\n",
    "# If baseline also failed before, this needs adjustment too. Let's assume it worked.\n",
    "baseline_labels = {}\n",
    "for example in evaluation_dataset:\n",
    "    try:\n",
    "        # --- !! ADJUST THIS IF BASELINE STRUCTURE IS DIFFERENT !! ---\n",
    "        query = example.input.get(\"user_query\") # Assuming baseline WAS flat structure\n",
    "        # If baseline also had nested structure, use:\n",
    "        # query = get_user_query(example.input.get(\"messages\"))\n",
    "        # --- !! ---------------------------------------------- !! ---\n",
    "\n",
    "        if query and example.output: # Check if query exists and there's label data\n",
    "            baseline_labels[query] = example.output\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not process baseline example: {e}\")\n",
    "\n",
    "print(f\"\\nExtracted {len(baseline_labels)} labeled examples from baseline dataset ('evaluation_dataset').\")\n",
    "\n",
    "\n",
    "# --- Extract Experimental Results (keyed by user_query) ---\n",
    "# We now know the structure for the experimental dataset\n",
    "experimental_results = {}\n",
    "for example in evaluation_dataset_gpt4o:\n",
    "     try:\n",
    "        # Extract query using the helper function for the known nested structure\n",
    "        query = get_user_query(example.input.get(\"messages\"))\n",
    "        if query:\n",
    "            # Need to find the agent's actual output (final_answer, generated_sql)\n",
    "            # This likely comes from the processing done in parse_spans.ipynb or similar step\n",
    "            # Let's ASSUME the important outputs were copied into the .input field\n",
    "            # during dataset creation, similar to the baseline structure for simplicity.\n",
    "            # If not, we need to figure out where the final_answer etc. are stored for gpt4o run.\n",
    "            # For now, let's just store the whole input dict.\n",
    "            experimental_results[query] = example.input # Storing the raw input for now\n",
    "     except Exception as e:\n",
    "        print(f\"Warning: Could not process experimental example: {e}\")\n",
    "\n",
    "print(f\"\\nExtracted {len(experimental_results)} examples from experimental dataset ('evaluation_dataset_gpt4o').\")\n",
    "if len(experimental_results) == 0:\n",
    "     print(\"ERROR: Failed to extract any results from the experimental dataset. Check extraction logic.\")\n",
    "\n",
    "\n",
    "# --- Create Combined Data for Evaluation ---\n",
    "combined_eval_data = []\n",
    "missing_labels_count = 0\n",
    "found_labels_count = 0\n",
    "\n",
    "print(\"\\nCombining data...\")\n",
    "if len(experimental_results) > 0:\n",
    "    for query, gpt4o_exp_input in experimental_results.items():\n",
    "        if query in baseline_labels:\n",
    "            # Found corresponding labels in the baseline dataset\n",
    "\n",
    "            # **** IMPORTANT ASSUMPTION ****\n",
    "            # We assume the key agent outputs (final_answer, generated_sql, tool_called)\n",
    "            # are somehow available within gpt4o_exp_input (the .input field from the dataset).\n",
    "            # This might be incorrect if the Experiment dataset only stored raw messages/tools.\n",
    "            # If evaluation fails later, we need to revisit how gpt4o_output is constructed here.\n",
    "            # Let's default to passing the whole dict for now.\n",
    "            gpt4o_output_data_for_eval = gpt4o_exp_input\n",
    "\n",
    "            combined_item = {\n",
    "                \"user_query\": query,\n",
    "                # This is the data the evaluator will receive as the 'output' of the task\n",
    "                \"gpt4o_output_for_evaluator\": gpt4o_output_data_for_eval,\n",
    "                # This is the ground truth data from the baseline dataset\n",
    "                \"ground_truth\": baseline_labels[query]\n",
    "            }\n",
    "            combined_eval_data.append(combined_item)\n",
    "            found_labels_count += 1\n",
    "        else:\n",
    "            # No labels found for this query in the baseline dataset\n",
    "            missing_labels_count += 1\n",
    "\n",
    "    print(f\"\\nSuccessfully combined data for {found_labels_count} examples.\")\n",
    "    if missing_labels_count > 0:\n",
    "        print(f\"Skipped {missing_labels_count} examples from the gpt-4o run because corresponding labels were not found in the baseline dataset.\")\n",
    "    if found_labels_count == 0 and len(experimental_results) > 0:\n",
    "        print(\"WARNING: No examples could be matched between datasets based on user_query. Check for subtle differences in query strings.\")\n",
    "\n",
    "\n",
    "    # --- Inspect the first combined item ---\n",
    "    if combined_eval_data:\n",
    "        print(\"\\n--- First Combined Example Structure---\")\n",
    "        # Print structure, not necessarily full content\n",
    "        first_combined = combined_eval_data[0]\n",
    "        print(f\"Keys: {list(first_combined.keys())}\")\n",
    "        print(f\"  user_query: {first_combined.get('user_query')[:80]}...\")\n",
    "        print(f\"  gpt4o_output_for_evaluator keys: {list(first_combined.get('gpt4o_output_for_evaluator', {}).keys())}\")\n",
    "        print(f\"  ground_truth keys: {list(first_combined.get('ground_truth', {}).keys())}\")\n",
    "        # print(json.dumps(combined_eval_data[0], indent=2)) # Uncomment for full details\n",
    "    else:\n",
    "        print(\"\\nNo combined data was prepared. Check dataset alignment or content.\")\n",
    "\n",
    "else:\n",
    "     print(\"\\nSkipping combination because no experimental results were extracted.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Data Preparation Complete ---\")\n",
    "# The variable 'combined_eval_data' now holds the data ready for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GPT-4o Evaluation\n",
    "\n",
    "We have successfully loaded the experimental dataset containing the results from the `gpt-4o` agent run into the variable `evaluation_dataset_gpt4o`.\n",
    "\n",
    "We have also confirmed that our custom evaluation functions (`all_custom_evaluators`) operate in a **zero-shot** manner. They evaluate the agent's performance (Tool Usage, SQL Correctness, Final Answer Quality) by analyzing the agent's inputs and outputs (contained within `evaluation_dataset_gpt4o.input`) using an LLM judge (`call_gemini`), without requiring the separate ground truth labels that were added to the baseline dataset via the UI.\n",
    "\n",
    "**Next Step: Run Evaluation**\n",
    "\n",
    "The next step is to execute the evaluation using the `phoenix.experiments.run_experiment` function. We will pass it:\n",
    "*   The `evaluation_dataset_gpt4o` (containing the data from the `gpt-4o` run).\n",
    "*   The `dummy_task_function` (which simply passes the necessary input data through).\n",
    "*   The `all_custom_evaluators` list (containing our zero-shot evaluator functions).\n",
    "\n",
    "This will run the evaluations for the `gpt-4o` model and log the results to a new experiment run in Phoenix, allowing us to compare performance against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Run Experiment on GPT-4o Dataset (with function redefined inside to ensure correct version is used)\n",
    "\n",
    "import json # Ensure json is imported\n",
    "import phoenix as px # Ensure phoenix is imported if needed\n",
    "from phoenix.experiments import run_experiment\n",
    "from datetime import datetime\n",
    "from typing import List, Callable, Dict, Any # Ensure imports\n",
    "from phoenix.experiments.types import Example # Make sure Example is imported\n",
    "\n",
    "# --- Define/Re-define Dummy Task Function HERE to ensure it's used by run_experiment ---\n",
    "print(\"\\n--- Defining/Re-defining dummy_task_function with data extraction IN THIS CELL ---\")\n",
    "\n",
    "# Helper function to safely extract user query from messages list\n",
    "def get_user_query(messages_list):\n",
    "    if not isinstance(messages_list, list): return None\n",
    "    for msg in messages_list:\n",
    "        if isinstance(msg, dict) and msg.get(\"role\") == \"user\" and \"content\" in msg:\n",
    "            return msg[\"content\"]\n",
    "    return None\n",
    "\n",
    "# Helper to find the final assistant answer\n",
    "def get_final_answer(messages_list):\n",
    "     if not isinstance(messages_list, list): return None\n",
    "     for msg in reversed(messages_list): # Look from the end\n",
    "         if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\":\n",
    "             if msg.get(\"content\"): return msg.get(\"content\")\n",
    "     return None\n",
    "\n",
    "# Helper to check if a specific tool was called\n",
    "def check_tool_called(messages_list, tool_name=\"query_database\"):\n",
    "    if not isinstance(messages_list, list): return False\n",
    "    for msg in messages_list:\n",
    "        if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n",
    "             for tool_call in msg.get(\"tool_calls\", []):\n",
    "                  if isinstance(tool_call, dict) and tool_call.get(\"function\", {}).get(\"name\") == tool_name:\n",
    "                       return True\n",
    "    return False\n",
    "\n",
    "# Helper to extract SQL query\n",
    "def get_generated_sql(messages_list):\n",
    "    if not isinstance(messages_list, list): return None\n",
    "    for msg in messages_list:\n",
    "         if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n",
    "              for tool_call in msg.get(\"tool_calls\", []):\n",
    "                   if isinstance(tool_call, dict) and tool_call.get(\"function\", {}).get(\"name\") == \"query_database\":\n",
    "                        try:\n",
    "                             func_dict = tool_call.get(\"function\", {})\n",
    "                             args_str = func_dict.get(\"arguments\", \"{}\")\n",
    "                             args = json.loads(args_str)\n",
    "                             return args.get(\"sql_query\")\n",
    "                        except json.JSONDecodeError: return None\n",
    "                        except Exception: return None\n",
    "    return None\n",
    "\n",
    "# The actual dummy task function definition\n",
    "def dummy_task_function(example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts relevant fields from the example.input (which contains the raw run data)\n",
    "    and returns a FLAT dictionary matching the structure expected by the evaluators.\n",
    "    \"\"\"\n",
    "    if not hasattr(example, 'input') or not isinstance(example.input, dict): return {}\n",
    "    input_data = example.input\n",
    "    messages = input_data.get(\"messages\", [])\n",
    "    user_query = get_user_query(messages)\n",
    "    final_answer = get_final_answer(messages)\n",
    "    tool_called = check_tool_called(messages)\n",
    "    generated_sql = get_generated_sql(messages)\n",
    "    # Return the flat dictionary that evaluators expect as their 'output' argument\n",
    "    return { \"user_query\": user_query, \"final_answer\": final_answer,\n",
    "             \"tool_called\": tool_called, \"generated_sql\": generated_sql }\n",
    "\n",
    "print(\"dummy_task_function defined/re-defined in this cell.\")\n",
    "# --- End Function Definition ---\n",
    "\n",
    "\n",
    "# --- Now proceed with running the experiment ---\n",
    "# Check if the gpt4o dataset variable is loaded from previous cell\n",
    "if 'evaluation_dataset_gpt4o' in locals() and len(evaluation_dataset_gpt4o) > 0:\n",
    "    missing_deps = []\n",
    "    if 'dummy_task_function' not in locals(): missing_deps.append(\"dummy_task_function\") # Should be found now\n",
    "    # Ensure the evaluator list from earlier setup cells is available\n",
    "    if 'all_final_evaluators' not in locals(): missing_deps.append(\"all_final_evaluators list\")\n",
    "    if 'px_client' not in locals() or px_client is None: missing_deps.append(\"px_client\")\n",
    "\n",
    "    if not missing_deps:\n",
    "        print(\"\\nRunning full experiment for GPT-4o using direct API evaluators...\")\n",
    "        now_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        # Give this experiment run a distinct name\n",
    "        experiment_name_gpt4o = f\"GPT4o_Full_OpenAI_Eval_{now_str}\" # Matched naming convention\n",
    "\n",
    "        # Define type hint for evaluator list (if not defined globally)\n",
    "        EvaluatorList = List[Callable[[Dict[str, Any]], float]]\n",
    "\n",
    "        # Assuming run_experiment works without explicit env vars here\n",
    "        try:\n",
    "            full_experiment_gpt4o = run_experiment(\n",
    "                evaluation_dataset_gpt4o,           # Use the GPT-4o dataset variable\n",
    "                dummy_task_function,                # Uses the function defined above in this cell\n",
    "                evaluators=all_final_evaluators,   # Re-use the same evaluators list\n",
    "                experiment_name=experiment_name_gpt4o, # Use the new, distinct experiment name\n",
    "                experiment_description=\"Eval GPT-4o: Direct API ToolUsage, SQLCorrectness, FinalAnswer.\" # Updated description\n",
    "                # concurrency=5 # Optional: Adjust concurrency if needed\n",
    "            )\n",
    "\n",
    "            print(f\"\\nFull Experiment '{experiment_name_gpt4o}' run initiated.\")\n",
    "            print(f\"Check Phoenix UI for results associated with experiment name '{experiment_name_gpt4o}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch potential errors like Connection refused if it reappears\n",
    "            print(f\"\\nERROR during run_experiment for GPT-4o: {e}\")\n",
    "            print(\"If this is a connection error, consider adding os.environ lines before the call.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping GPT-4o experiment - missing dependencies: {', '.join(missing_deps)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Skipping GPT-4o experiment - 'evaluation_dataset_gpt4o' was not found or is empty. Did you run the cell above to load it?\")\n",
    "\n",
    "print(\"\\n--- End GPT-4o Full Experiment ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
