{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Evaluation Harness for Transcript Agent\n",
    "\n",
    "This notebook implements and runs the evaluation harness for the Transcript Agent. It uses the processed and annotated data prepared in the `parse_spans.ipynb` notebook, which has been uploaded as a Phoenix Dataset.\n",
    "\n",
    "**Goal:** Evaluate the agent's performance on key criteria (Tool Usage, SQL Correctness, Final Answer Quality) using LLM-as-judge, leveraging the Phoenix Experiments framework.\n",
    "\n",
    "**Plan:**\n",
    "\n",
    "1.  **Setup:** Import necessary libraries and initialize the Phoenix client and the evaluation LLM (e.g., GPT-4o).\n",
    "2.  **Load Dataset:** Retrieve the specific evaluation dataset (`transcript-agent-eval-data-...`) previously uploaded to Phoenix.\n",
    "3.  **Define Task Function:** Create the simple \"dummy\" task function required by `run_experiment` to pass through the pre-computed agent outputs from the dataset.\n",
    "4.  **Define Evaluators:**\n",
    "    *   Create three LLM-as-judge evaluator functions using `phoenix.evals.llm_classify`.\n",
    "    *   Develop prompts for each evaluator (Tool Usage, SQL Correctness, Final Answer Quality) instructing the LLM to assess the agent's output based on the input query, agent's actions/results, and referencing the human-provided labels and explanations from the dataset.\n",
    "5.  **Run Experiment:** Execute `phoenix.experiments.run_experiment`, passing the loaded dataset, the task function, and the list of defined evaluators.\n",
    "6.  **Review Results:** Briefly note that results (scores, LLM judge explanations) should be reviewed in the Phoenix UI experiment view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load .env file...\n",
      "Finished loading .env file (if found).\n",
      "Successfully parsed headers: Key='api_key'\n",
      "\n",
      "Initializing Phoenix client explicitly for cloud...\n",
      "Attempting px.Client(endpoint='https://app.phoenix.arize.com', headers=...)\n",
      "Phoenix client initialized successfully using explicit arguments.\n",
      "\n",
      "Initializing evaluation LLM (GPT-4o)...\n",
      "Evaluation LLM initialized.\n",
      "\n",
      "--- Setup Cell Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Setup (MODIFIED FOR EXPLICIT CLOUD CLIENT CONFIGURATION - FINAL ATTEMPT) ---\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Optional: suppress warnings\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.evals import OpenAIModel, llm_classify\n",
    "from phoenix.experiments import run_experiment\n",
    "from phoenix.experiments.types import Example\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv # Make sure dotenv is imported\n",
    "\n",
    "# Apply nest_asyncio for environments like Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- Load Environment Variables ---\n",
    "# Ensure this runs reliably near the start if not already done\n",
    "if 'dotenv_loaded' not in locals(): # Simple flag to avoid reloading if already done\n",
    "    print(\"Attempting to load .env file...\")\n",
    "    # Ensure your .env file has PHOENIX_COLLECTOR_ENDPOINT and PHOENIX_CLIENT_HEADERS\n",
    "    load_dotenv(verbose=True) # verbose=True shows which file is loaded\n",
    "    dotenv_loaded = True # Set flag\n",
    "    print(\"Finished loading .env file (if found).\")\n",
    "else:\n",
    "    print(\".env file assumed to be loaded previously.\")\n",
    "# --- End Load ---\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the API endpoint (base URL for cloud)\n",
    "cloud_api_endpoint = \"https://app.phoenix.arize.com\"\n",
    "\n",
    "# Get the required header string from environment\n",
    "# This MUST be set correctly in your .env file: PHOENIX_CLIENT_HEADERS=\"api_key=YOUR_KEY_VALUE\"\n",
    "api_headers_str = os.getenv(\"PHOENIX_CLIENT_HEADERS\")\n",
    "\n",
    "if not api_headers_str:\n",
    "    raise ValueError(\"CRITICAL: PHOENIX_CLIENT_HEADERS environment variable not found. Ensure it's set in your .env file and load_dotenv() ran.\")\n",
    "\n",
    "# Parse the header string (\"api_key=value\") into the required dictionary format\n",
    "api_headers_dict = {}\n",
    "try:\n",
    "    key, value = api_headers_str.split('=', 1)\n",
    "    parsed_key_name = key.strip()\n",
    "    parsed_key_value = value.strip()\n",
    "    if parsed_key_name != \"api_key\" or not parsed_key_value:\n",
    "            raise ValueError(\"Parsed key name is not 'api_key' or value is empty.\")\n",
    "    api_headers_dict[parsed_key_name] = parsed_key_value # Store as dict {\"api_key\": \"value\"}\n",
    "    print(f\"Successfully parsed headers: Key='{parsed_key_name}'\")\n",
    "except Exception as parse_err:\n",
    "    print(f\"ERROR: Could not parse PHOENIX_CLIENT_HEADERS string: '{api_headers_str}'. Expected 'api_key=value' format. Error: {parse_err}\")\n",
    "    raise ValueError(\"Invalid PHOENIX_CLIENT_HEADERS format\") from parse_err\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "# --- Initialize Client Explicitly ---\n",
    "print(\"\\nInitializing Phoenix client explicitly for cloud...\")\n",
    "px_client = None # Initialize to None\n",
    "try:\n",
    "    # Initialize with explicit endpoint and headers arguments\n",
    "    print(f\"Attempting px.Client(endpoint='{cloud_api_endpoint}', headers=...)\")\n",
    "    px_client = px.Client(endpoint=cloud_api_endpoint, headers=api_headers_dict) # EXPLICIT INIT\n",
    "    print(\"Phoenix client initialized successfully using explicit arguments.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing Phoenix Client explicitly: {e}\")\n",
    "    print(\"Check endpoint URL and header format/value in your .env file.\")\n",
    "    # px_client remains None\n",
    "# --- End Initialization ---\n",
    "\n",
    "# --- Initialize Judge LLM ---\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"WARNING: OPENAI_API_KEY not found in environment. Evaluation LLM might fail.\")\n",
    "\n",
    "print(\"\\nInitializing evaluation LLM (GPT-4o)...\")\n",
    "eval_model = None # Initialize to None\n",
    "try:\n",
    "    eval_model = OpenAIModel(model=\"gpt-4o\")\n",
    "    print(\"Evaluation LLM initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAIModel: {e}\")\n",
    "    # eval_model remains None\n",
    "# --- End Judge LLM Init ---\n",
    "\n",
    "print(\"\\n--- Setup Cell Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Evaluation Dataset\n",
    "\n",
    "Retrieve the specific `transcript-agent-eval-data-...` dataset previously uploaded to Phoenix. We need this dataset object to pass to the `run_experiment` function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load dataset 'transcript-agent-eval-data-20250428-102511'...\n",
      "Dataset loaded successfully.\n",
      "Number of examples in dataset: 17\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Load Dataset ---\n",
    "\n",
    "# Exact dataset name identified from the parse_spans.ipynb notebook output\n",
    "dataset_name = \"transcript-agent-eval-data-20250428-102511\"\n",
    "\n",
    "print(f\"Attempting to load dataset '{dataset_name}'...\")\n",
    "\n",
    "# Load the specified dataset by its exact name\n",
    "# This will raise an error if the dataset doesn't exist or px_client isn't initialized\n",
    "evaluation_dataset = px_client.get_dataset(name=dataset_name)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Print number of examples\n",
    "print(f\"Number of examples in dataset: {len(evaluation_dataset)}\")\n",
    "if len(evaluation_dataset) != 17:\n",
    "     print(f\"Warning: Dataset contains {len(evaluation_dataset)} examples, but we expected 17 based on UI/previous note.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of loaded dataset object: <class 'phoenix.experiments.types.Dataset'>\n",
      "\n",
      "--- First Example ---\n",
      "\n",
      "Input Data:\n",
      "{\n",
      "  \"tool_called\": true,\n",
      "  \"user_query\": \"Who is Jeff Pidcock?\",\n",
      "  \"generated_sql\": \"SELECT * FROM transcript_segments WHERE text LIKE '%Jeff Pidcock%'\",\n",
      "  \"final_answer\": \"I cannot answer the question about who Jeff Pidcock is based on the available transcript data.\"\n",
      "}\n",
      "\n",
      "Output/Expected Data:\n",
      "{\n",
      "  \"tool_usage_explanation\": \"The agent correctly identified that answering this question requires querying the database to find mentions of the name.\",\n",
      "  \"sql_correctness_label\": \"Incorrect\",\n",
      "  \"tool_usage_correctness_label\": \"Correct\",\n",
      "  \"final_answer_quality_label\": \"Fail\",\n",
      "  \"sql_correctness_explanation\": \"The specific SQL query (LIKE '%Jeff Pidcock%') failed functionally. It did not retrieve the existing mention of \\\"Jeff Pidcock\\\" from the transcript, most likely due to case sensitivity, making it an incorrect implementation for the task.\",\n",
      "  \"final_answer_explanation\": \"The agent provided a factually incorrect final answer (\\\"I cannot answer...\\\") because the underlying SQL query failed to retrieve the available information from the database.\"\n",
      "}\n",
      "\n",
      "Metadata:\n",
      "{\n",
      "  \"llm_span_idx\": 0,\n",
      "  \"root_span_idx\": 6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Inspect Loaded Dataset ---\n",
    "\n",
    "print(f\"Type of loaded dataset object: {type(evaluation_dataset)}\")\n",
    "\n",
    "# Display the first example to check structure\n",
    "if len(evaluation_dataset) > 0:\n",
    "    print(\"\\n--- First Example ---\")\n",
    "    first_example = evaluation_dataset[0]\n",
    "\n",
    "    print(\"\\nInput Data:\")\n",
    "    # Assumes first_example.input exists and is dict-like\n",
    "    print(json.dumps(first_example.input, indent=2))\n",
    "\n",
    "    print(\"\\nOutput/Expected Data:\")\n",
    "    # Assumes first_example.output exists and is dict-like\n",
    "    print(json.dumps(first_example.output, indent=2))\n",
    "\n",
    "    print(\"\\nMetadata:\")\n",
    "    # Assumes first_example.metadata exists and is dict-like\n",
    "    print(json.dumps(first_example.metadata, indent=2))\n",
    "\n",
    "else:\n",
    "    print(\"Dataset appears to be empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Task Function\n",
    "\n",
    "The Phoenix `run_experiment` function is designed to run a specific \"task\" (like executing our agent) for each example in a dataset and then evaluate the result.\n",
    "\n",
    "In our case, we've already run the agent and processed its results into our datase|t (in the `input` fields like `final_answer`, `generated_sql`, etc.). However, the `run_experiment` function still requires *some* function to be passed as the \"task\".\n",
    "\n",
    "So, we'll define a very simple \"dummy\" task function. Its only job is to take the `input` data provided for each example in our dataset and return it directly. This satisfies the structural requirement of `run_experiment` without re-running our agent. The evaluators we define later will then use this returned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dummy_task_function with the first example:\n",
      "Output from dummy task:\n",
      "{\n",
      "  \"tool_called\": true,\n",
      "  \"user_query\": \"Who is Jeff Pidcock?\",\n",
      "  \"generated_sql\": \"SELECT * FROM transcript_segments WHERE text LIKE '%Jeff Pidcock%'\",\n",
      "  \"final_answer\": \"I cannot answer the question about who Jeff Pidcock is based on the available transcript data.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Define Task Function ---\n",
    "from phoenix.experiments.types import Example\n",
    "\n",
    "def dummy_task_function(example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    This function acts as the 'task' for run_experiment.\n",
    "    Since our agent outputs are already pre-computed and stored in the\n",
    "    dataset's 'input' fields, this function simply returns that input data.\n",
    "    The evaluators will receive this dictionary as their 'output' parameter.\n",
    "    \"\"\"\n",
    "    # The input attribute of the Example object holds the dictionary\n",
    "    # containing user_query, final_answer, generated_sql, tool_called.\n",
    "    return example.input\n",
    "\n",
    "# --- Quick test of the function ---\n",
    "if len(evaluation_dataset) > 0:\n",
    "    print(\"Testing dummy_task_function with the first example:\")\n",
    "    test_output = dummy_task_function(evaluation_dataset[0])\n",
    "    print(\"Output from dummy task:\")\n",
    "    print(json.dumps(test_output, indent=2))\n",
    "else:\n",
    "    print(\"Skipping test, dataset is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluators\n",
    "\n",
    "Now we define the functions that will evaluate the agent's performance for each example. We will use the `phoenix.evals.llm_classify` function to create LLM-as-judge evaluators for our three criteria: Tool Usage Correctness, SQL Correctness, and Final Answer Quality.\n",
    "\n",
    "**Design Approach:**\n",
    "\n",
    "We'll build each evaluator sequentially using a \"Recipe & Chef\" analogy:\n",
    "\n",
    "1.  **Define the Prompt Template (The Recipe):** For each criterion, we'll first write the detailed instructions (the prompt template) telling the LLM *how* to perform the specific evaluation.\n",
    "2.  **Define the Evaluator Function (The Chef):** Next, we'll create the Python function (the evaluator) that takes the data for an example, uses the corresponding prompt template (recipe), and manages the call to the LLM (the worker) via `llm_classify`.\n",
    "3.  **Test the Evaluator:** We'll run a quick test on the first example to ensure the evaluator function works as expected.\n",
    "\n",
    "We will repeat this Prompt -> Function -> Test sequence for each of our three evaluation criteria:\n",
    "\n",
    "1.  **Tool Usage Correctness:** Was the decision to call the SQL tool (or not) appropriate?\n",
    "2.  **SQL Correctness:** If the SQL tool was called, was the generated SQL query correct and effective?\n",
    "3.  **Final Answer Quality:** Was the final text answer provided to the user clear, correct, and relevant?\n",
    "\n",
    "Each evaluator function will ultimately return a score (e.g., 1.0 for success, 0.0 for failure) based on the LLM judge's assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Usage Prompt Template defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 4a. Prompt Template: Tool Usage Correctness ---\n",
    "\n",
    "TOOL_USAGE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are evaluating an AI agent's decision on whether to use a specific tool ('query_database') to answer a user's query about a workshop transcript.\n",
    "The agent has access to a database table 'transcript_segments'.\n",
    "\n",
    "**Instructions:**\n",
    "1. Analyze the User Query.\n",
    "2. Analyze the Agent's Action: Did the agent call the 'query_database' tool? (indicated by 'tool_called' flag and presence/absence of 'generated_sql').\n",
    "3. Determine if the Agent's Action was Correct: Should the agent have used the tool to answer this query effectively? Consider if the query asks for specific factual information likely in the transcript vs. general knowledge or conversational elements.\n",
    "4. Compare your assessment to the Human Label and Explanation provided (for context, but make your own judgment).\n",
    "5. Output a final LABEL ('Correct' or 'Incorrect') based *only* on your assessment of the agent's action.\n",
    "6. Provide a detailed EXPLANATION for your label, referencing the query and the agent's action.\n",
    "\n",
    "**Input Data:**\n",
    "User Query: {user_query}\n",
    "Agent Called Tool ('query_database'): {tool_called}\n",
    "Agent Generated SQL (if tool called): {generated_sql}\n",
    "\n",
    "**Reference (Human Annotation):**\n",
    "Human Label: {tool_usage_correctness_label}\n",
    "Human Explanation: {tool_usage_explanation}\n",
    "\n",
    "**Your Task:**\n",
    "Based *only* on the User Query and the Agent's Action, was the decision to use (or not use) the 'query_database' tool correct?\n",
    "\n",
    "EXPLANATION: [Provide your reasoning here]\n",
    "LABEL: [Correct or Incorrect]\n",
    "\"\"\"\n",
    "\n",
    "print(\"Tool Usage Prompt Template defined.\")\n",
    "# print(TOOL_USAGE_PROMPT_TEMPLATE) # Optional: uncomment to view the template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluator function 'evaluate_tool_usage' defined (updated to accept model).\n"
     ]
    }
   ],
   "source": [
    "# --- 4b. Evaluator Function: Tool Usage Correctness ---\n",
    "\n",
    "def evaluate_tool_usage(output: dict, expected: dict, input: dict, model_to_use: OpenAIModel) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates Tool Usage Correctness using LLM-as-judge based on TOOL_USAGE_PROMPT_TEMPLATE.\n",
    "\n",
    "    Args:\n",
    "        output (dict): The dictionary returned by the dummy_task_function.\n",
    "                        Contains 'user_query', 'generated_sql', 'tool_called'.\n",
    "        expected (dict): The dictionary containing the expected outputs (human labels/explanations).\n",
    "                            Contains 'tool_usage_correctness_label', 'tool_usage_explanation'.\n",
    "        input (dict): The dictionary containing the original input keys.\n",
    "        model_to_use (OpenAIModel): The initialized OpenAIModel instance for the judge.\n",
    "\n",
    "    Returns:\n",
    "        float: Score (1.0 for Correct, 0.0 for Incorrect based on LLM judge).\n",
    "                Returns 0.0 if evaluation fails or label is missing from LLM response.\n",
    "    \"\"\"\n",
    "    # Prepare data for the prompt template\n",
    "    user_query = output.get('user_query')\n",
    "    tool_called = output.get('tool_called')\n",
    "    generated_sql = output.get('generated_sql', 'N/A') # Use N/A if None\n",
    "    human_label = expected.get('tool_usage_correctness_label')\n",
    "    human_explanation = expected.get('tool_usage_explanation')\n",
    "\n",
    "    # Check if essential inputs for the LLM are present\n",
    "    if user_query is None or tool_called is None:\n",
    "            print(f\"Warning: Missing essential input (query or tool_called) for Tool Usage eval. Returning 0.0\")\n",
    "            return 0.0\n",
    "\n",
    "    # Create DataFrame for llm_classify (needs dicts)\n",
    "    eval_df = pd.DataFrame([{\n",
    "        \"user_query\": user_query,\n",
    "        \"tool_called\": tool_called,\n",
    "        \"generated_sql\": generated_sql,\n",
    "        \"tool_usage_correctness_label\": human_label if human_label is not None else \"N/A\",\n",
    "        \"tool_usage_explanation\": human_explanation if human_explanation is not None else \"N/A\"\n",
    "    }])\n",
    "\n",
    "    # Removed the check for eval_model in locals()\n",
    "\n",
    "    # Call LLM judge using the template defined previously, passing the specific model\n",
    "    response = llm_classify(\n",
    "        data=eval_df,\n",
    "        template=TOOL_USAGE_PROMPT_TEMPLATE, # Uses the variable defined earlier\n",
    "        model=model_to_use, # Use the passed-in model\n",
    "        rails=[\"Correct\", \"Incorrect\"], # Expected LLM output labels\n",
    "        provide_explanation=True\n",
    "    )\n",
    "\n",
    "    # Extract the label assigned by the LLM judge\n",
    "    try:\n",
    "        llm_label = response['label'].iloc[0]\n",
    "        score = 1.0 if llm_label == 'Correct' else 0.0\n",
    "        return score\n",
    "    except (IndexError, KeyError, TypeError) as e:\n",
    "            print(f\"Error parsing LLM response for Tool Usage: {e}. Response: {response}\")\n",
    "            return 0.0 # Score as incorrect if LLM response is malformed\n",
    "\n",
    "print(\"Evaluator function 'evaluate_tool_usage' defined (updated to accept model).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Tool Usage evaluator with the first example:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85178d617a1046478eaaea522f820820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Judge Score for Tool Usage (First Example): 0.0\n",
      "(Score reflects LLM judgment: 1.0 for 'Correct', 0.0 for 'Incorrect')\n"
     ]
    }
   ],
   "source": [
    "# --- 4c. Test: Tool Usage Evaluator ---\n",
    "\n",
    "# Ensure the dataset object exists and has examples\n",
    "if 'evaluation_dataset' in locals() and len(evaluation_dataset) > 0 and 'eval_model' in locals():\n",
    "    print(\"Testing Tool Usage evaluator with the first example:\")\n",
    "    # Get the necessary parts from the first example\n",
    "    first_example = evaluation_dataset[0]\n",
    "    test_output = dummy_task_function(first_example) # Use dummy task to get 'output' dict\n",
    "    test_expected = first_example.output # Ground truth labels/explanations\n",
    "    test_input = first_example.input # Contains original inputs\n",
    "\n",
    "    # Call the evaluator function (Corrected Line Below - passing model)\n",
    "    try:\n",
    "        # Ensure eval_model is passed to the updated function\n",
    "        score = evaluate_tool_usage(output=test_output,\n",
    "                                    expected=test_expected,\n",
    "                                    input=test_input,\n",
    "                                    model_to_use=eval_model) # Pass eval_model here\n",
    "        print(f\"LLM Judge Score for Tool Usage (First Example): {score}\")\n",
    "        print(\"(Score reflects LLM judgment: 1.0 for 'Correct', 0.0 for 'Incorrect')\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the evaluate_tool_usage test: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "    # You can also inspect the individual components passed to the evaluator\n",
    "    # print(\"\\nData passed to evaluator:\")\n",
    "    # print(\"Output (from dummy task):\", json.dumps(test_output, indent=2))\n",
    "    # print(\"Expected (human labels):\", json.dumps(test_expected, indent=2))\n",
    "    # print(\"Input (original):\", json.dumps(test_input, indent=2))\n",
    "\n",
    "elif 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "    print(\"Skipping test, evaluation_dataset not loaded or is empty.\")\n",
    "else: # This means eval_model is missing\n",
    "    print(\"Skipping test, eval_model not found. Ensure the Setup cell was run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting Tool Usage LLM response for first 5 examples...\n",
      "\n",
      "--- Processing Example 0 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656bf2acbfcd425d950d3ffbd4a7db65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 0: Score=0.0, LLM Explanation: The user query asks for information about 'Jeff Pidcock'. This is a specific factual query that likely requires information from the workshop transcript to answer accurately. The agent's decision to use the 'query_database' tool is appropriate because it allows the agent to search the transcript for mentions of 'Jeff Pidcock' and provide relevant information. The generated SQL query is designed to find any segments in the transcript that mention 'Jeff Pidcock', which is a logical approach to answering the user's question. Therefore, the agent's action to call the tool was correct.\n",
      "\n",
      "--- Processing Example 1 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8132de0bdd846aa830facf1e47f3bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 1: Score=0.0, LLM Explanation: The user query specifically asks for what Stefan Krawczyk said during his introduction. This is a request for specific factual information that would be found in the transcript of the workshop. The agent correctly decided to use the 'query_database' tool to retrieve this information, as it involves searching for a specific speaker and context within the transcript. The generated SQL query is appropriately designed to find segments where Stefan Krawczyk is the speaker and the content relates to his introduction. Therefore, the agent's action to call the tool was correct.\n",
      "\n",
      "--- Processing Example 2 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942fb346c887481ba9dd167c17f9885b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 2: Score=0.0, LLM Explanation: The user query asks for a list of all unique speakers mentioned in the workshop transcript. This is a specific factual request that requires accessing the database to retrieve distinct speaker names from the 'transcript_segments' table. The agent correctly decided to use the 'query_database' tool and generated an appropriate SQL query: SELECT DISTINCT speaker FROM transcript_segments. This action aligns with the need to extract specific data from the database, making the agent's decision to use the tool correct.\n",
      "\n",
      "--- Processing Example 3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706f404ff4074a8093ce02cc0f172065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 3: Score=0.0, LLM Explanation: The user query asks for the total number of words spoken by a specific speaker, Hugo, in a workshop transcript. This is a factual query that requires specific data from the transcript, specifically the aggregation of word counts for Hugo's segments. The agent correctly decided to use the 'query_database' tool to retrieve this information, as it involves summing up the word counts from the database where the speaker is Hugo. The generated SQL query is appropriate for this task, as it selects the sum of word counts for the specified speaker. Therefore, the agent's action to call the tool was correct.\n",
      "\n",
      "--- Processing Example 4 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66a2ef3b802d4e2b8b529e1835ea1536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 4: Score=0.0, LLM Explanation: The user query specifically requests to find segments in a transcript that mention the word 'evaluation' and to provide the timestamps for these segments. This is a request for specific factual information that is likely stored in the 'transcript_segments' database table. The agent correctly called the 'query_database' tool and generated an appropriate SQL query to retrieve the necessary data. The SQL query is designed to search for the keyword 'evaluation' in the text field and return the start and end times along with the text, which aligns perfectly with the user's request. Therefore, the agent's action to use the tool was appropriate and necessary to fulfill the query effectively.\n",
      "\n",
      "--- End Inspection ---\n"
     ]
    }
   ],
   "source": [
    "# --- Introspect Tool Usage on First 5 Examples (Show Explanations) ---\n",
    "import pandas as pd # Make sure pandas is imported if not already\n",
    "\n",
    "num_examples_to_test = 5\n",
    "print(f\"Inspecting Tool Usage LLM response for first {num_examples_to_test} examples...\")\n",
    "\n",
    "if 'evaluation_dataset' in locals() and len(evaluation_dataset) > 0 and 'eval_model' in locals():\n",
    "    for i in range(min(num_examples_to_test, len(evaluation_dataset))):\n",
    "        print(f\"\\n--- Processing Example {i} ---\")\n",
    "        example = evaluation_dataset[i]\n",
    "        output_data = dummy_task_function(example) # Agent's output from dataset\n",
    "        expected_data = example.output           # Human labels from dataset\n",
    "        input_data = example.input               # Original input query etc.\n",
    "\n",
    "        # Prepare data for llm_classify (similar to inside evaluate_tool_usage)\n",
    "        user_query = output_data.get('user_query')\n",
    "        tool_called = output_data.get('tool_called')\n",
    "        generated_sql = output_data.get('generated_sql', 'N/A')\n",
    "        human_label = expected_data.get('tool_usage_correctness_label', 'N/A')\n",
    "        human_explanation = expected_data.get('tool_usage_explanation', 'N/A')\n",
    "\n",
    "        if user_query is None or tool_called is None:\n",
    "             print(f\"  Skipping Example {i}: Missing essential input (query or tool_called).\")\n",
    "             continue\n",
    "\n",
    "        eval_df = pd.DataFrame([{\n",
    "            \"user_query\": user_query,\n",
    "            \"tool_called\": tool_called,\n",
    "            \"generated_sql\": generated_sql,\n",
    "            \"tool_usage_correctness_label\": human_label,\n",
    "            \"tool_usage_explanation\": human_explanation\n",
    "        }])\n",
    "\n",
    "        try:\n",
    "            # Call llm_classify directly HERE within the test cell\n",
    "            response_df = llm_classify(\n",
    "                data=eval_df,\n",
    "                template=TOOL_USAGE_PROMPT_TEMPLATE, # Use the existing template\n",
    "                model=eval_model,                   # Use the existing model\n",
    "                rails=[\"Correct\", \"Incorrect\"],\n",
    "                provide_explanation=True\n",
    "            )\n",
    "\n",
    "            # Extract score AND explanation from the response DataFrame\n",
    "            llm_label = response_df['label'].iloc[0]\n",
    "            explanation = response_df['explanation'].iloc[0]\n",
    "            score = 1.0 if llm_label == 'Correct' else 0.0\n",
    "\n",
    "            print(f\"  Example {i}: Score={score}, LLM Explanation: {explanation}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Example {i}: ERROR during llm_classify call: {e}\")\n",
    "            # import traceback\n",
    "            # traceback.print_exc() # Uncomment for full error details if needed\n",
    "\n",
    "else:\n",
    "    print(\"Skipping test - dataset or eval_model not loaded.\")\n",
    "\n",
    "print(\"\\n--- End Inspection ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revised Tool Usage Prompt Template defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Revised Prompt: Tool Usage Correctness ---\n",
    "\n",
    "REVISED_TOOL_USAGE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are evaluating an AI agent's decision on whether to use a specific tool ('query_database') to answer a user's query about a workshop transcript.\n",
    "The agent has access to a database table 'transcript_segments'.\n",
    "\n",
    "**Instructions:**\n",
    "1. Analyze the User Query: What information is the user asking for?\n",
    "2. Analyze the Agent's Action: Did the agent call the 'query_database' tool? (indicated by 'tool_called' flag).\n",
    "3. Determine if the Agent's Action was Correct: Based ONLY on the User Query, should the agent have used the 'query_database' tool to answer effectively?\n",
    "    - 'Correct': Tool usage is appropriate if the query asks for specific factual information likely only found within the transcript data.\n",
    "    - 'Incorrect': Tool usage is inappropriate if the query is conversational, asks for general knowledge, or could be answered without accessing the transcript data.\n",
    "\n",
    "**Input Data:**\n",
    "User Query: {user_query}\n",
    "Agent Called Tool ('query_database'): {tool_called}\n",
    "Agent Generated SQL (if tool called): {generated_sql}\n",
    "\n",
    "**Your Task:**\n",
    "Based *only* on the User Query and the Agent's Action, was the decision to use (or not use) the 'query_database' tool correct?\n",
    "\n",
    "EXPLANATION: [Provide your reasoning here, focusing only on the query and the agent's action.]\n",
    "LABEL: [Correct or Incorrect]\n",
    "\"\"\"\n",
    "\n",
    "print(\"Revised Tool Usage Prompt Template defined.\")\n",
    "# print(REVISED_TOOL_USAGE_PROMPT_TEMPLATE) # Optional: uncomment to view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Tool Usage with REVISED prompt for first 5 examples...\n",
      "\n",
      "--- Processing Example 0 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682620e76d66496bb9b52b07de6e02ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 0: Score=0.0, LLM Explanation: The user query asks for information about 'Jeff Pidcock'. This is a specific factual query that likely requires accessing the transcript data to find relevant information about this individual. The agent's decision to use the 'query_database' tool to search for mentions of 'Jeff Pidcock' in the transcript is appropriate, as it is the most direct way to obtain accurate and specific information about him from the workshop transcript.\n",
      "\n",
      "--- Processing Example 1 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e180db00209e4694a86f79bfcaa64e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 1: Score=0.0, LLM Explanation: The user query specifically asks for what Stefan Krawczyk said during his introduction. This is a request for specific factual information that would be found in the transcript data. The agent correctly used the 'query_database' tool to retrieve this information, as it is likely stored in the 'transcript_segments' table. The SQL query generated is appropriate for extracting the relevant segment of the transcript where Stefan Krawczyk speaks during his introduction.\n",
      "\n",
      "--- Processing Example 2 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2f0106ae6f4bbc988ed9a1dabb17c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 2: Score=0.0, LLM Explanation: The user query asks for a list of all unique speakers mentioned in the workshop transcript. This is a specific factual request that requires accessing the transcript data to identify and list the unique speakers. The agent's decision to call the 'query_database' tool and execute a SQL query to retrieve distinct speaker names from the 'transcript_segments' table is appropriate and necessary to fulfill the user's request. Therefore, the agent's action was correct.\n",
      "\n",
      "--- Processing Example 3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a98cd7979dc480eafea149afc480b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 3: Score=0.0, LLM Explanation: The user query asks for the total number of words spoken by a specific individual, Hugo, in a workshop transcript. This is a specific factual question that requires accessing the transcript data to calculate the total word count for Hugo. The agent's decision to use the 'query_database' tool is appropriate because it allows the agent to retrieve and sum the word counts from the database, which is necessary to answer the user's query accurately.\n",
      "\n",
      "--- Processing Example 4 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5993ec140dd84ad2a93c3d28c491cc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Example 4: Score=0.0, LLM Explanation: The user query specifically asks for segments mentioning 'evaluation' along with their timestamps. This is a request for specific factual information that is likely only available in the transcript data. The agent's decision to use the 'query_database' tool to search for these segments in the 'transcript_segments' table is appropriate, as it directly addresses the user's request for specific data from the transcript.\n",
      "\n",
      "--- End Revised Prompt Test ---\n"
     ]
    }
   ],
   "source": [
    "# --- Test Tool Usage on First 5 Examples (Using REVISED Prompt) ---\n",
    "import pandas as pd # Ensure pandas is imported\n",
    "\n",
    "num_examples_to_test = 5\n",
    "print(f\"Testing Tool Usage with REVISED prompt for first {num_examples_to_test} examples...\")\n",
    "\n",
    "if 'evaluation_dataset' in locals() and len(evaluation_dataset) > 0 and 'eval_model' in locals() and 'REVISED_TOOL_USAGE_PROMPT_TEMPLATE' in locals():\n",
    "    for i in range(min(num_examples_to_test, len(evaluation_dataset))):\n",
    "        print(f\"\\n--- Processing Example {i} ---\")\n",
    "        example = evaluation_dataset[i]\n",
    "        output_data = dummy_task_function(example)\n",
    "        expected_data = example.output # Still needed if you want to compare later\n",
    "        input_data = example.input\n",
    "\n",
    "        # Prepare data for llm_classify\n",
    "        user_query = output_data.get('user_query')\n",
    "        tool_called = output_data.get('tool_called')\n",
    "        generated_sql = output_data.get('generated_sql', 'N/A')\n",
    "        # Note: We don't need human labels for the prompt input anymore, but keep for potential comparison\n",
    "        human_label = expected_data.get('tool_usage_correctness_label', 'N/A')\n",
    "        human_explanation = expected_data.get('tool_usage_explanation', 'N/A')\n",
    "\n",
    "\n",
    "        if user_query is None or tool_called is None:\n",
    "             print(f\"  Skipping Example {i}: Missing essential input.\")\n",
    "             continue\n",
    "\n",
    "        # DataFrame still includes human labels, though not used in revised prompt\n",
    "        eval_df = pd.DataFrame([{\n",
    "            \"user_query\": user_query,\n",
    "            \"tool_called\": tool_called,\n",
    "            \"generated_sql\": generated_sql,\n",
    "            \"tool_usage_correctness_label\": human_label,\n",
    "            \"tool_usage_explanation\": human_explanation\n",
    "        }])\n",
    "\n",
    "        try:\n",
    "            # Call llm_classify directly using the REVISED template\n",
    "            response_df = llm_classify(\n",
    "                data=eval_df,\n",
    "                template=REVISED_TOOL_USAGE_PROMPT_TEMPLATE, # Use the new template variable\n",
    "                model=eval_model,\n",
    "                rails=[\"Correct\", \"Incorrect\"],\n",
    "                provide_explanation=True\n",
    "            )\n",
    "\n",
    "            llm_label = response_df['label'].iloc[0]\n",
    "            explanation = response_df['explanation'].iloc[0]\n",
    "            score = 1.0 if llm_label == 'Correct' else 0.0\n",
    "\n",
    "            print(f\"  Example {i}: Score={score}, LLM Explanation: {explanation}\")\n",
    "            # You could add a comparison here if desired:\n",
    "            # print(f\"    (Human Label was: {human_label})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Example {i}: ERROR during llm_classify call: {e}\")\n",
    "\n",
    "else:\n",
    "    missing = []\n",
    "    if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "        missing.append(\"dataset\")\n",
    "    if 'eval_model' not in locals():\n",
    "        missing.append(\"eval_model\")\n",
    "    if 'REVISED_TOOL_USAGE_PROMPT_TEMPLATE' not in locals():\n",
    "        missing.append(\"revised prompt template\")\n",
    "    print(f\"Skipping test - required components not loaded: {', '.join(missing)}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- End Revised Prompt Test ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempting Direct LLM Call for Tool Usage Evaluation\n",
    "\n",
    "The previous attempts using `phoenix.evals.llm_classify` with various prompt refinements (`TOOL_USAGE_PROMPT_TEMPLATE`, `REVISED_TOOL_USAGE_PROMPT_TEMPLATE`, `REVISED_TOOL_USAGE_PROMPT_TEMPLATE_V2`) consistently produced contradictory results. The LLM's generated explanations indicated correct reasoning about tool usage appropriateness, but the final classification label forced by the `rails=[\"Correct\", \"Incorrect\"]` parameter was persistently 'Incorrect' (Score=0.0).\n",
    "\n",
    "This suggests a potential issue either with how `llm_classify` handles the rails in conjunction with the explanation for this specific task, or a deeper problem with the LLM's ability to follow the structured output format reliably under these conditions.\n",
    "\n",
    "To isolate the problem, we will now bypass `llm_classify` and directly query the evaluation LLM (`gpt-4o`) using the `REVISED_TOOL_USAGE_PROMPT_TEMPLATE_V2`. We will manually inspect the raw output to see if the explanation and the final label align when generated without the constraints of the `llm_classify` framework. This will help determine if the core LLM can perform the task correctly when called directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing DIRECT OpenAI API call test (Example 0, V1 Prompt)...\n",
      "OpenAI client initialized for model: gpt-4o\n",
      "\n",
      "--- Prompt Sent to OpenAI API (V1) ---\n",
      "\n",
      "You are evaluating an AI agent's decision on whether to use a specific tool ('query_database') to answer a user's query about a workshop transcript.\n",
      "The agent has access to a database table 'transcript_segments'.\n",
      "\n",
      "**Instructions:**\n",
      "1. Analyze the User Query: What information is the user asking for?\n",
      "2. Analyze the Agent's Action: Did the agent call the 'query_database' tool? (indicated by 'tool_called' flag).\n",
      "3. Determine if the Agent's Action was Correct: Based ONLY on the User Query, should the agent have used the 'query_database' tool to answer effectively?\n",
      "    - 'Correct': Tool usage is appropriate if the query asks for specific factual information likely only found within the transcript data.\n",
      "    - 'Incorrect': Tool usage is inappropriate if the query is conversational, asks for general knowledge, or could be answered without accessing the transcript data.\n",
      "\n",
      "**Input Data:**\n",
      "User Query: Who is Jeff Pidcock?\n",
      "Agent Called Tool ('query_database'): True\n",
      "Agent Generated SQL (if tool called): SELECT * FROM transcript_segments WHERE text LIKE '%Jeff Pidcock%'\n",
      "\n",
      "**Your Task:**\n",
      "Based *only* on the User Query and the Agent's Action, was the decision to use (or not use) the 'query_database' tool correct?\n",
      "\n",
      "EXPLANATION: [Provide your reasoning here, focusing only on the query and the agent's action.]\n",
      "LABEL: [Correct or Incorrect]\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "--- Raw OpenAI API Response ---\n",
      "EXPLANATION: The user query \"Who is Jeff Pidcock?\" is asking for specific factual information about an individual named Jeff Pidcock. This type of query typically requires accessing detailed information that might be contained within a transcript or database, especially if Jeff Pidcock is a person mentioned in the context of a workshop or event. The agent's decision to call the 'query_database' tool and generate an SQL query to search for mentions of \"Jeff Pidcock\" in the 'transcript_segments' table is appropriate. This action suggests that the agent is attempting to retrieve specific information about Jeff Pidcock from the transcript data, which aligns with the user's request for factual information.\n",
      "\n",
      "LABEL: Correct\n",
      "-----------------------------\n",
      "\n",
      "--- Parsed Output (V1 Prompt) ---\n",
      "Extracted Explanation: The user query \"Who is Jeff Pidcock?\" is asking for specific factual information about an individual named Jeff Pidcock. This type of query typically requires accessing detailed information that might be contained within a transcript or database, especially if Jeff Pidcock is a person mentioned in the context of a workshop or event. The agent's decision to call the 'query_database' tool and generate an SQL query to search for mentions of \"Jeff Pidcock\" in the 'transcript_segments' table is appropriate. This action suggests that the agent is attempting to retrieve specific information about Jeff Pidcock from the transcript data, which aligns with the user's request for factual information.\n",
      "Extracted Label: Correct\n",
      "---------------------------------\n",
      "\n",
      ">>> Please manually check if the Extracted Label ('Correct'/'Incorrect') logically follows the Extracted Explanation.\n",
      "    (For reference, Human Label was: Correct)\n",
      "\n",
      "--- End Direct OpenAI API Call Test ---\n"
     ]
    }
   ],
   "source": [
    "# --- Direct OpenAI API Call Test for Tool Usage (Example 0, V1 Prompt) ---\n",
    "# Bypasses Phoenix entirely. Requires 'openai' library and OPENAI_API_KEY env var.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"Performing DIRECT OpenAI API call test (Example 0, V1 Prompt)...\")\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_TO_USE = \"gpt-4o\" # Specify the model\n",
    "# --- End Configuration ---\n",
    "\n",
    "# Ensure necessary components are available (Dataset and Prompt V1)\n",
    "missing = []\n",
    "if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "    missing.append(\"evaluation_dataset\")\n",
    "if 'REVISED_TOOL_USAGE_PROMPT_TEMPLATE' not in locals():\n",
    "    missing.append(\"REVISED_TOOL_USAGE_PROMPT_TEMPLATE (V1)\")\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    missing.append(\"OPENAI_API_KEY environment variable\")\n",
    "\n",
    "if not missing:\n",
    "    # Initialize OpenAI client directly\n",
    "    try:\n",
    "        client = OpenAI()\n",
    "        print(f\"OpenAI client initialized for model: {MODEL_TO_USE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing OpenAI client: {e}\")\n",
    "        client = None\n",
    "\n",
    "    if client:\n",
    "        example_index = 0\n",
    "        example = evaluation_dataset[example_index]\n",
    "        # Get input data robustly\n",
    "        if 'dummy_task_function' in locals():\n",
    "            output_data = dummy_task_function(example)\n",
    "        elif hasattr(example, 'input'):\n",
    "             output_data = example.input\n",
    "        else:\n",
    "            print(f\"  Skipping Example {example_index}: Cannot access input data.\")\n",
    "            output_data = None\n",
    "\n",
    "        if output_data:\n",
    "            expected_data = example.output if hasattr(example, 'output') else {}\n",
    "\n",
    "            # Prepare data\n",
    "            user_query = output_data.get('user_query')\n",
    "            tool_called = output_data.get('tool_called')\n",
    "            generated_sql = output_data.get('generated_sql', 'N/A')\n",
    "            human_label = expected_data.get('tool_usage_correctness_label', 'N/A')\n",
    "\n",
    "            if user_query is None or tool_called is None:\n",
    "                 print(f\"  Skipping Example {example_index}: Missing essential input fields (user_query or tool_called).\")\n",
    "            else:\n",
    "                # Format the V1 prompt\n",
    "                formatted_prompt = REVISED_TOOL_USAGE_PROMPT_TEMPLATE.format(\n",
    "                    user_query=user_query,\n",
    "                    tool_called=tool_called,\n",
    "                    generated_sql=generated_sql\n",
    "                )\n",
    "\n",
    "                print(\"\\n--- Prompt Sent to OpenAI API (V1) ---\")\n",
    "                print(formatted_prompt)\n",
    "                print(\"------------------------------------\")\n",
    "\n",
    "                try:\n",
    "                    # Make the direct API call\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=MODEL_TO_USE,\n",
    "                        messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "                        temperature=0.0,\n",
    "                        # max_tokens=250 # Optional: limit response length\n",
    "                    )\n",
    "                    raw_output = response.choices[0].message.content\n",
    "\n",
    "                    print(\"\\n--- Raw OpenAI API Response ---\")\n",
    "                    print(raw_output)\n",
    "                    print(\"-----------------------------\")\n",
    "\n",
    "                    # Simple parsing attempt\n",
    "                    explanation_match = re.search(r\"EXPLANATION:\\s*(.*?)\\s*LABEL:\", raw_output, re.DOTALL | re.IGNORECASE)\n",
    "                    label_match = re.search(r\"LABEL:\\s*(\\w+)\", raw_output, re.IGNORECASE)\n",
    "\n",
    "                    extracted_explanation = explanation_match.group(1).strip() if explanation_match else \"Parsing failed\"\n",
    "                    extracted_label = label_match.group(1).strip() if label_match else \"Parsing failed\"\n",
    "\n",
    "                    print(\"\\n--- Parsed Output (V1 Prompt) ---\")\n",
    "                    print(f\"Extracted Explanation: {extracted_explanation}\")\n",
    "                    print(f\"Extracted Label: {extracted_label}\")\n",
    "                    print(\"---------------------------------\")\n",
    "\n",
    "                    print(\"\\n>>> Please manually check if the Extracted Label ('Correct'/'Incorrect') logically follows the Extracted Explanation.\")\n",
    "                    print(f\"    (For reference, Human Label was: {human_label})\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\n--- ERROR during OpenAI API call ---\")\n",
    "                    print(e)\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    print(\"------------------------------------\")\n",
    "else:\n",
    "    print(f\"Skipping test - required components not loaded: {', '.join(missing)}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- End Direct OpenAI API Call Test ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ALL examples via DIRECT OpenAI API call (No Progress Bar)...\n",
      "OpenAI client initialized for model: gpt-4o\n",
      "Processing 17 examples...\n",
      "Processing Example 0...\n",
      "Processing Example 1...\n",
      "Processing Example 2...\n",
      "Processing Example 3...\n",
      "Processing Example 4...\n",
      "Processing Example 5...\n",
      "Processing Example 6...\n",
      "Processing Example 7...\n",
      "Processing Example 8...\n",
      "Processing Example 9...\n",
      "Processing Example 10...\n",
      "Processing Example 11...\n",
      "Processing Example 12...\n",
      "Processing Example 13...\n",
      "Processing Example 14...\n",
      "Processing Example 15...\n",
      "Processing Example 16...\n",
      "\n",
      "Evaluation complete. Creating DataFrame...\n",
      "Direct API Evaluation Results (No Progress Bar):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>query</th>\n",
       "      <th>human_label</th>\n",
       "      <th>llm_label</th>\n",
       "      <th>llm_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Who is Jeff Pidcock?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query \"Who is Jeff Pidcock?\" is asking for specific factual information about an individual named Jeff Pidcock. This type of query typica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What did Stefan Krawczyk say during his introduction?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for what Stefan Krawczyk said during his introduction. This is a request for specific factual information that is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>List all unique speakers mentioned.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query asks for a list of all unique speakers mentioned in the workshop transcript. This is a request for specific factual information tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>How many words did Hugo speak in total?</td>\n",
       "      <td>Correcr</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for the total number of words spoken by Hugo, which is a factual piece of information that would likely be stored...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Find segments mentioning 'evaluation' and provide timestamps.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for segments mentioning the word 'evaluation' along with their timestamps. This request requires searching throug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Which speaker has the most segments?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query asks for specific factual information about which speaker has the most segments in a workshop transcript. This type of information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>What is the total word count for all segments combined?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query asks for the total word count for all segments combined, which is specific factual information that would be stored in the 'transcr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Who mentioned Carvana?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for information about who mentioned \"Carvana\" in a workshop transcript. This is a request for specific factual in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>List the builders in residence mentioned.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for a list of \"builders in residence mentioned,\" which is a request for specific factual information that is like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>When did Nathan Danielsen first speak?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query asks for specific factual information about when Nathan Danielsen first spoke during a workshop. This type of information is likely...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Summarize the key points about evaluation driven development.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query asks for a summary of the key points about \"evaluation driven development.\" This type of query is seeking specific factual informat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Find mentions of 'non-determinism' and provide timestamps.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for mentions of 'non-determinism' along with their timestamps. This request is for specific factual information t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Did the transcript mention monitoring?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks whether the transcript mentioned \"monitoring.\" This is a request for specific factual information that would be f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Tell me about AI observability.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query \"Tell me about AI observability\" is asking for information about a specific topic, AI observability. This type of query suggests th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>What was said about Discord?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for information about what was said regarding \"Discord\" in a workshop transcript. This is a request for specific ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>How many times was 'Discord' mentioned?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>The user query specifically asks for the number of times the word 'Discord' was mentioned in the workshop transcript. This is a request for specif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>What did they say about reinforcement learning?</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Parsing failed</td>\n",
       "      <td>** The user query specifically asks for information about what was said regarding \"reinforcement learning\" in a workshop transcript. This is a req...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                                          query  \\\n",
       "0       0                                           Who is Jeff Pidcock?   \n",
       "1       1          What did Stefan Krawczyk say during his introduction?   \n",
       "2       2                            List all unique speakers mentioned.   \n",
       "3       3                        How many words did Hugo speak in total?   \n",
       "4       4  Find segments mentioning 'evaluation' and provide timestamps.   \n",
       "5       5                           Which speaker has the most segments?   \n",
       "6       6        What is the total word count for all segments combined?   \n",
       "7       7                                         Who mentioned Carvana?   \n",
       "8       8                      List the builders in residence mentioned.   \n",
       "9       9                         When did Nathan Danielsen first speak?   \n",
       "10     10  Summarize the key points about evaluation driven development.   \n",
       "11     11     Find mentions of 'non-determinism' and provide timestamps.   \n",
       "12     12                         Did the transcript mention monitoring?   \n",
       "13     13                                Tell me about AI observability.   \n",
       "14     14                                   What was said about Discord?   \n",
       "15     15                        How many times was 'Discord' mentioned?   \n",
       "16     16                What did they say about reinforcement learning?   \n",
       "\n",
       "   human_label       llm_label  \\\n",
       "0      Correct         Correct   \n",
       "1      Correct         Correct   \n",
       "2      Correct         Correct   \n",
       "3      Correcr         Correct   \n",
       "4      Correct         Correct   \n",
       "5      Correct         Correct   \n",
       "6      Correct         Correct   \n",
       "7      Correct         Correct   \n",
       "8      Correct         Correct   \n",
       "9      Correct         Correct   \n",
       "10     Correct         Correct   \n",
       "11     Correct         Correct   \n",
       "12     Correct         Correct   \n",
       "13     Correct         Correct   \n",
       "14     Correct         Correct   \n",
       "15     Correct         Correct   \n",
       "16     Correct  Parsing failed   \n",
       "\n",
       "                                                                                                                                          llm_explanation  \n",
       "0   The user query \"Who is Jeff Pidcock?\" is asking for specific factual information about an individual named Jeff Pidcock. This type of query typica...  \n",
       "1   The user query specifically asks for what Stefan Krawczyk said during his introduction. This is a request for specific factual information that is...  \n",
       "2   The user query asks for a list of all unique speakers mentioned in the workshop transcript. This is a request for specific factual information tha...  \n",
       "3   The user query specifically asks for the total number of words spoken by Hugo, which is a factual piece of information that would likely be stored...  \n",
       "4   The user query specifically asks for segments mentioning the word 'evaluation' along with their timestamps. This request requires searching throug...  \n",
       "5   The user query asks for specific factual information about which speaker has the most segments in a workshop transcript. This type of information ...  \n",
       "6   The user query asks for the total word count for all segments combined, which is specific factual information that would be stored in the 'transcr...  \n",
       "7   The user query specifically asks for information about who mentioned \"Carvana\" in a workshop transcript. This is a request for specific factual in...  \n",
       "8   The user query specifically asks for a list of \"builders in residence mentioned,\" which is a request for specific factual information that is like...  \n",
       "9   The user query asks for specific factual information about when Nathan Danielsen first spoke during a workshop. This type of information is likely...  \n",
       "10  The user query asks for a summary of the key points about \"evaluation driven development.\" This type of query is seeking specific factual informat...  \n",
       "11  The user query specifically asks for mentions of 'non-determinism' along with their timestamps. This request is for specific factual information t...  \n",
       "12  The user query specifically asks whether the transcript mentioned \"monitoring.\" This is a request for specific factual information that would be f...  \n",
       "13  The user query \"Tell me about AI observability\" is asking for information about a specific topic, AI observability. This type of query suggests th...  \n",
       "14  The user query specifically asks for information about what was said regarding \"Discord\" in a workshop transcript. This is a request for specific ...  \n",
       "15  The user query specifically asks for the number of times the word 'Discord' was mentioned in the workshop transcript. This is a request for specif...  \n",
       "16  ** The user query specifically asks for information about what was said regarding \"reinforcement learning\" in a workshop transcript. This is a req...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Direct OpenAI API Full Evaluation (No Progress Bar) ---\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate ALL Examples Directly via OpenAI API (No Progress Bar) ---\n",
    "# Focuses on core logic, minimal error handling, NO tqdm dependency.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "# Removed: from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Evaluating ALL examples via DIRECT OpenAI API call (No Progress Bar)...\")\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_TO_USE = \"gpt-4o\"\n",
    "PROMPT_TEMPLATE = REVISED_TOOL_USAGE_PROMPT_TEMPLATE # Assumes V1 is defined\n",
    "# --- End Configuration ---\n",
    "\n",
    "# List to store results\n",
    "evaluation_results = []\n",
    "\n",
    "# Initialize OpenAI client (basic check)\n",
    "try:\n",
    "    client = OpenAI()\n",
    "    print(f\"OpenAI client initialized for model: {MODEL_TO_USE}\")\n",
    "except Exception as e:\n",
    "    print(f\"STOPPING: Failed to initialize OpenAI Client: {e}. Make sure OPENAI_API_KEY is set.\")\n",
    "    client = None # Ensure client is None if init fails\n",
    "\n",
    "if client and 'evaluation_dataset' in locals() and PROMPT_TEMPLATE:\n",
    "    print(f\"Processing {len(evaluation_dataset)} examples...\")\n",
    "\n",
    "    # Removed tqdm wrapper from the loop\n",
    "    for i, example in enumerate(evaluation_dataset):\n",
    "        print(f\"Processing Example {i}...\") # Simple print indicator instead of progress bar\n",
    "\n",
    "        # --- 1. Get Data ---\n",
    "        user_query = example.input.get('user_query', 'N/A')\n",
    "        tool_called = example.input.get('tool_called', None)\n",
    "        generated_sql = example.input.get('generated_sql', 'N/A')\n",
    "        human_label = example.output.get('tool_usage_correctness_label', 'N/A')\n",
    "\n",
    "        llm_label = \"Skipped\"\n",
    "        llm_explanation = \"Skipped due to missing input\"\n",
    "\n",
    "        if user_query != 'N/A' and tool_called is not None:\n",
    "            # --- 2. Format Prompt ---\n",
    "            prompt = PROMPT_TEMPLATE.format(\n",
    "                user_query=user_query,\n",
    "                tool_called=tool_called,\n",
    "                generated_sql=generated_sql\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                # --- 3. Call API ---\n",
    "                response = client.chat.completions.create(\n",
    "                    model=MODEL_TO_USE,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0.0,\n",
    "                )\n",
    "                raw_output = response.choices[0].message.content\n",
    "\n",
    "                # --- 4. Parse Result ---\n",
    "                explanation_match = re.search(r\"EXPLANATION:\\s*(.*?)\\s*LABEL:\", raw_output, re.DOTALL | re.IGNORECASE)\n",
    "                label_match = re.search(r\"LABEL:\\s*(\\w+)\", raw_output, re.IGNORECASE)\n",
    "\n",
    "                llm_explanation = explanation_match.group(1).strip() if explanation_match else \"Parsing failed\"\n",
    "                llm_label = label_match.group(1).strip().capitalize() if label_match else \"Parsing failed\"\n",
    "\n",
    "            except Exception as e:\n",
    "                # Minimal error handling for API call failure\n",
    "                print(f\"  API Call Error on Example {i}: {e}\")\n",
    "                llm_label = \"API Error\"\n",
    "                llm_explanation = f\"API Call Error: {e}\"\n",
    "\n",
    "        # --- 5. Store Essentials ---\n",
    "        evaluation_results.append({\n",
    "            \"index\": i,\n",
    "            \"query\": user_query,\n",
    "            \"human_label\": human_label,\n",
    "            \"llm_label\": llm_label,\n",
    "            \"llm_explanation\": llm_explanation,\n",
    "        })\n",
    "\n",
    "    # --- Convert to DataFrame ---\n",
    "    print(\"\\nEvaluation complete. Creating DataFrame...\")\n",
    "    results_df_final = pd.DataFrame(evaluation_results)\n",
    "\n",
    "    # --- Display DataFrame ---\n",
    "    print(\"Direct API Evaluation Results (No Progress Bar):\")\n",
    "    pd.set_option('display.max_rows', 50)\n",
    "    pd.set_option('display.max_colwidth', 150)\n",
    "    display(results_df_final)\n",
    "\n",
    "else:\n",
    "    if not client:\n",
    "        print(\"Evaluation skipped because OpenAI client failed to initialize.\")\n",
    "    else:\n",
    "        print(\"Evaluation skipped - check dataset and prompt template definitions.\")\n",
    "\n",
    "print(\"\\n--- End Direct OpenAI API Full Evaluation (No Progress Bar) ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Evaluator Function for Tool Usage (Direct API Call)\n",
    "\n",
    "Our previous attempts to evaluate Tool Usage using `phoenix.evals.llm_classify` resulted in persistent inconsistencies: the LLM judge's explanations suggested correct reasoning, but the final label forced by the `rails=[\"Correct\", \"Incorrect\"]` parameter was always 'Incorrect'.\n",
    "\n",
    "We subsequently tested making direct calls to the OpenAI API (`gpt-4o`) using the simplified `REVISED_TOOL_USAGE_PROMPT_TEMPLATE`. This approach **worked correctly**, yielding consistent explanations and labels that matched the human annotations across all examples.\n",
    "\n",
    "Therefore, we will now define a new evaluator function, `evaluate_tool_usage_direct_api`, that encapsulates this successful direct API call logic. This function will:\n",
    "\n",
    "1.  Accept the `output` (from the agent/task function) and `expected` (from the dataset) dictionaries as input, following the standard Phoenix evaluator signature.\n",
    "2.  Extract the necessary fields (`user_query`, `tool_called`, `generated_sql`).\n",
    "3.  Format the `REVISED_TOOL_USAGE_PROMPT_TEMPLATE`.\n",
    "4.  Call the OpenAI API directly.\n",
    "5.  Parse the response to get the LLM's label ('Correct'/'Incorrect') and explanation.\n",
    "6.  Return a `phoenix.evals.models.scoring.Score` object containing the score (1.0 for 'Correct', 0.0 for 'Incorrect') and the LLM's explanation.\n",
    "\n",
    "This allows us to replace the faulty `llm_classify`-based evaluator with our custom, validated logic while potentially still using the `phoenix.experiments.run_experiment` framework for overall execution and integration with other evaluators (like SQL and Final Answer quality, which we will define next)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing evaluate_tool_usage_direct_api (returns float) with Example 0 ---\n",
      "Score Returned: 1.0 (Type: <class 'float'>)\n",
      "--- End Test ---\n"
     ]
    }
   ],
   "source": [
    "# --- Phoenix-Compatible Evaluator: Tool Usage (Direct API - Returns Float) ---\n",
    "# Corrected based on L11(1).ipynb: Accepts output/expected, returns float score.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from openai import OpenAI\n",
    "# Removed Score object import - not needed based on reference\n",
    "import logging\n",
    "\n",
    "# Configure logging (optional)\n",
    "logging.basicConfig(level=logging.WARNING, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_TO_USE = \"gpt-4o\"\n",
    "# Assumes REVISED_TOOL_USAGE_PROMPT_TEMPLATE is defined globally\n",
    "if 'REVISED_TOOL_USAGE_PROMPT_TEMPLATE' not in globals():\n",
    "    logging.error(\"STOPPING: REVISED_TOOL_USAGE_PROMPT_TEMPLATE not found.\")\n",
    "    REVISED_TOOL_USAGE_PROMPT_TEMPLATE = \"Prompt not defined\"\n",
    "\n",
    "# Initialize OpenAI client once\n",
    "try:\n",
    "    openai_client = OpenAI()\n",
    "    # logging.info(f\"OpenAI client initialized for model: {MODEL_TO_USE}\") # Less verbose\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to initialize OpenAI Client: {e}. Check API key.\")\n",
    "    openai_client = None\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "def evaluate_tool_usage_direct_api(output: dict, expected: dict) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates tool usage correctness via direct OpenAI API calls.\n",
    "    Returns a float score (1.0 for Correct, 0.0 for Incorrect/Error).\n",
    "    Compatible with phoenix.experiments.run_experiment evaluators list.\n",
    "\n",
    "    Args:\n",
    "        output: Dictionary containing agent outputs (user_query, tool_called, etc.).\n",
    "        expected: Dictionary containing expected outputs/labels from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        A float score (0.0 or 1.0).\n",
    "    \"\"\"\n",
    "    global openai_client, REVISED_TOOL_USAGE_PROMPT_TEMPLATE\n",
    "\n",
    "    score = 0.0 # Default score for errors/skips/Incorrect\n",
    "\n",
    "    if not openai_client:\n",
    "        logging.warning(\"Skipping evaluation: OpenAI client not initialized\")\n",
    "        return score\n",
    "\n",
    "    if not REVISED_TOOL_USAGE_PROMPT_TEMPLATE or REVISED_TOOL_USAGE_PROMPT_TEMPLATE == \"Prompt not defined\":\n",
    "         logging.warning(\"Skipping evaluation: Prompt template not defined\")\n",
    "         return score\n",
    "\n",
    "    # --- 1. Get Data ---\n",
    "    user_query = output.get('user_query')\n",
    "    tool_called = output.get('tool_called')\n",
    "    generated_sql = output.get('generated_sql', 'N/A')\n",
    "\n",
    "    if user_query is None or tool_called is None:\n",
    "        logging.warning(\"Skipping evaluation: Missing 'user_query' or 'tool_called' in output\")\n",
    "        return score\n",
    "\n",
    "    # --- 2. Format Prompt ---\n",
    "    try:\n",
    "        prompt = REVISED_TOOL_USAGE_PROMPT_TEMPLATE.format(\n",
    "            user_query=user_query,\n",
    "            tool_called=tool_called,\n",
    "            generated_sql=generated_sql\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        logging.warning(f\"Skipping evaluation: Error formatting prompt - missing key {e}\")\n",
    "        return score # Return 0.0 on formatting error\n",
    "\n",
    "    # --- 3. Call API & Parse ---\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=MODEL_TO_USE,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        raw_output = response.choices[0].message.content\n",
    "\n",
    "        # Parse Label\n",
    "        label_match = re.search(r\"LABEL:\\s*(\\w+)\", raw_output, re.IGNORECASE)\n",
    "        llm_label_str = label_match.group(1).strip().capitalize() if label_match else \"Parsing failed\"\n",
    "\n",
    "        # --- 4. Determine Score ---\n",
    "        if llm_label_str == \"Correct\":\n",
    "            score = 1.0\n",
    "        # else: score remains 0.0 for \"Incorrect\", \"Parsing failed\", or API errors\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"API Call Error during tool usage evaluation: {e}\")\n",
    "        # score remains 0.0\n",
    "\n",
    "    return score\n",
    "\n",
    "# --- Quick Test (Optional) ---\n",
    "if 'evaluation_dataset' in locals() and len(evaluation_dataset) > 0:\n",
    "    if openai_client and REVISED_TOOL_USAGE_PROMPT_TEMPLATE != \"Prompt not defined\":\n",
    "        print(\"\\n--- Testing evaluate_tool_usage_direct_api (returns float) with Example 0 ---\")\n",
    "        test_example = evaluation_dataset[0]\n",
    "        test_output_data = test_example.input\n",
    "        test_expected_data = test_example.output\n",
    "        test_score_float = evaluate_tool_usage_direct_api(test_output_data, test_expected_data)\n",
    "        print(f\"Score Returned: {test_score_float} (Type: {type(test_score_float)})\")\n",
    "        print(\"--- End Test ---\")\n",
    "    else:\n",
    "        print(\"\\nSkipping function test - OpenAI client or prompt not ready.\")\n",
    "else:\n",
    "    print(\"\\nSkipping function test - evaluation_dataset not loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluators: SQL Correctness & Final Answer Quality (Direct API)\n",
    "\n",
    "Following the successful pattern established for `evaluate_tool_usage_direct_api`, we now define the evaluators for SQL Correctness and Final Answer Quality using direct calls to the Gemini API via our `call_gemini` helper function.\n",
    "\n",
    "Each evaluator:\n",
    "1. Takes the experiment `example` dictionary as input.\n",
    "2. Formats a specific prompt using data from the example (`user_query`, `generated_sql`, `final_answer`).\n",
    "3. Calls the `call_gemini` function.\n",
    "4. Parses the response (\"Correct\"/\"Incorrect\" or \"Good\"/\"Bad\") into a float score (1.0 or 0.0).\n",
    "5. Handles potential missing data or API errors gracefully by returning 0.0.\n",
    "\n",
    "Finally, we combine all three custom evaluators into a list (`all_custom_evaluators`) to be used in the experiment run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined: SQL_CORRECTNESS_PROMPT_TEMPLATE\n",
      "Defined: call_openai_judge helper function\n",
      "Defined: evaluate_sql_correctness_direct_api function\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 1: DEFINITIONS ---\n",
    "import re\n",
    "import os\n",
    "from openai import OpenAI # Ensure OpenAI is imported\n",
    "# Assumes openai_client is initialized globally in a *previous* cell and is working\n",
    "# Assumes MODEL_TO_USE = \"gpt-4o\" is defined globally\n",
    "\n",
    "# --- Define Prompt Template ---\n",
    "SQL_CORRECTNESS_PROMPT_TEMPLATE = \"\"\"Evaluate if the Generated SQL is semantically correct and appropriate for the User Query. Consider typical schemas (e.g., transcript_segments table). Ignore Final Answer quality.\n",
    "\n",
    "User Query:\n",
    "{user_query}\n",
    "\n",
    "Generated SQL:\n",
    "{generated_sql}\n",
    "\n",
    "Is the SQL correct and appropriate?\n",
    "Provide a brief EXPLANATION and finish with LABEL: Correct or LABEL: Incorrect.\n",
    "\"\"\"\n",
    "print(\"Defined: SQL_CORRECTNESS_PROMPT_TEMPLATE\")\n",
    "\n",
    "# --- Helper Function to Call OpenAI ---\n",
    "# Define this if it's not already defined and available from another cell\n",
    "def call_openai_judge(prompt: str, model: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"Calls the specified OpenAI model as a judge and returns the raw text response.\"\"\"\n",
    "    if not openai_client:\n",
    "        raise RuntimeError(\"OpenAI client is not initialized.\")\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OpenAI API call: {e}\")\n",
    "        return \"API Error\" # Return error string\n",
    "print(\"Defined: call_openai_judge helper function\")\n",
    "\n",
    "# --- Evaluator Definition ---\n",
    "def evaluate_sql_correctness_direct_api(output: dict, expected: dict) -> float:\n",
    "    \"\"\"\n",
    "    (Simplified) Evaluates SQL correctness via direct OpenAI API call.\n",
    "    Returns 1.0 for Correct, 0.0 otherwise.\n",
    "    \"\"\"\n",
    "    # Function body as defined in the previous simplified version...\n",
    "    if not openai_client:\n",
    "        print(\"Prerequisite Error: OpenAI client not initialized.\")\n",
    "        return 0.0\n",
    "    if not SQL_CORRECTNESS_PROMPT_TEMPLATE:\n",
    "        print(\"Prerequisite Error: SQL_CORRECTNESS_PROMPT_TEMPLATE not defined.\")\n",
    "        return 0.0\n",
    "\n",
    "    user_query = output.get('user_query')\n",
    "    generated_sql = output.get('generated_sql')\n",
    "\n",
    "    if not user_query or not generated_sql:\n",
    "        return 0.0\n",
    "\n",
    "    try:\n",
    "        prompt = SQL_CORRECTNESS_PROMPT_TEMPLATE.format(\n",
    "            user_query=user_query,\n",
    "            generated_sql=generated_sql\n",
    "        )\n",
    "    except KeyError as e:\n",
    "         print(f\"Prompt Formatting Error: Missing key {e}\")\n",
    "         return 0.0\n",
    "\n",
    "    try:\n",
    "        raw_output = call_openai_judge(prompt, model=MODEL_TO_USE) # Use helper\n",
    "        label_match = re.search(r\"LABEL:\\s*(\\w+)\", raw_output, re.IGNORECASE)\n",
    "\n",
    "        if label_match and label_match.group(1).strip().capitalize() == \"Correct\":\n",
    "            return 1.0\n",
    "        else:\n",
    "            # Covers API Error string, parsing failure, or Incorrect label\n",
    "            # print(f\"Debug: Raw output '{raw_output[:50]}...' resulted in 0.0\") # Optional debug\n",
    "            return 0.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during SQL correctness evaluation: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "print(\"Defined: evaluate_sql_correctness_direct_api function\")\n",
    "# --- END CELL 1 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Full Evaluation for SQL Correctness on 17 Examples ---\n",
      "Processing Example 0...\n",
      "Processing Example 1...\n",
      "Processing Example 2...\n",
      "Processing Example 3...\n",
      "Processing Example 4...\n",
      "Processing Example 5...\n",
      "Processing Example 6...\n",
      "Processing Example 7...\n",
      "Processing Example 8...\n",
      "Processing Example 9...\n",
      "Processing Example 10...\n",
      "Processing Example 11...\n",
      "Processing Example 12...\n",
      "Processing Example 13...\n",
      "Processing Example 14...\n",
      "Processing Example 15...\n",
      "Processing Example 16...\n",
      "\n",
      "Evaluation loop complete. Creating DataFrame...\n",
      "SQL Correctness Evaluation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>user_query</th>\n",
       "      <th>generated_sql</th>\n",
       "      <th>raw_llm_response</th>\n",
       "      <th>sql_correctness_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Who is Jeff Pidcock?</td>\n",
       "      <td>SELECT * FROM transcript_segments WHERE text LIKE '%Jeff Pidcock%'</td>\n",
       "      <td>The generated SQL is semantically correct and appropriate for the user query. The user is asking for information about \"Jeff Pidcock,\" and the SQL query is designed to search for any transcript se...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What did Stefan Krawczyk say during his introduction?</td>\n",
       "      <td>SELECT text FROM transcript_segments WHERE speaker='Stefan Krawczyk' AND text LIKE '%introduction%'</td>\n",
       "      <td>The Generated SQL is attempting to retrieve segments of text spoken by Stefan Krawczyk that include the word \"introduction.\" However, this approach may not accurately capture the entire introducti...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>List all unique speakers mentioned.</td>\n",
       "      <td>SELECT DISTINCT speaker FROM transcript_segments</td>\n",
       "      <td>The User Query asks for a list of all unique speakers mentioned. The Generated SQL is:\\n\\n```sql\\nSELECT DISTINCT speaker FROM transcript_segments\\n```\\n\\nThis SQL query is semantically correct an...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>How many words did Hugo speak in total?</td>\n",
       "      <td>SELECT SUM(word_count) AS total_words FROM transcript_segments WHERE speaker = 'Hugo'</td>\n",
       "      <td>The User Query asks for the total number of words spoken by Hugo. The Generated SQL aims to calculate this by summing the `word_count` from the `transcript_segments` table where the `speaker` is '...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Find segments mentioning 'evaluation' and provide timestamps.</td>\n",
       "      <td>SELECT start_time_seconds, end_time_seconds, text FROM transcript_segments WHERE text LIKE '%evaluation%'</td>\n",
       "      <td>The User Query asks to find segments that mention the word 'evaluation' and provide their timestamps. The Generated SQL correctly addresses this request by selecting the `start_time_seconds`, `end...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Which speaker has the most segments?</td>\n",
       "      <td>SELECT speaker, COUNT(segment_id) AS segment_count FROM transcript_segments GROUP BY speaker ORDER BY segment_count DESC LIMIT 1;</td>\n",
       "      <td>The User Query asks for the speaker with the most segments. The Generated SQL is designed to achieve this by:\\n\\n1. Selecting the `speaker` and counting the number of `segment_id` for each speaker...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>What is the total word count for all segments combined?</td>\n",
       "      <td>SELECT SUM(word_count) as total_word_count FROM transcript_segments;</td>\n",
       "      <td>The User Query asks for the total word count for all segments combined. The Generated SQL is:\\n\\n```sql\\nSELECT SUM(word_count) as total_word_count FROM transcript_segments;\\n```\\n\\nThis SQL state...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Who mentioned Carvana?</td>\n",
       "      <td>SELECT speaker, text FROM transcript_segments WHERE text LIKE '%Carvana%'</td>\n",
       "      <td>The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about who mentioned \"Carvana\" in a transcript. The SQL query is designed to search ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>List the builders in residence mentioned.</td>\n",
       "      <td>SELECT text FROM transcript_segments WHERE text LIKE '%builder in residence%'</td>\n",
       "      <td>The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for a list of \"builders in residence\" mentioned, and the SQL query is designed to search for any te...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>When did Nathan Danielsen first speak?</td>\n",
       "      <td>SELECT start_time_seconds, text FROM transcript_segments WHERE speaker = 'Nathan Danielsen' ORDER BY start_time_seconds ASC LIMIT 1;</td>\n",
       "      <td>The User Query asks for the first time Nathan Danielsen spoke. The Generated SQL is designed to retrieve the start time and text of the first segment where Nathan Danielsen is the speaker. \\n\\n- T...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Summarize the key points about evaluation driven development.</td>\n",
       "      <td>SELECT * FROM transcript_segments WHERE text LIKE '%evaluation driven development%'</td>\n",
       "      <td>The SQL query is syntactically correct, as it uses a SELECT statement to retrieve all columns from the `transcript_segments` table where the `text` column contains the phrase \"evaluation driven de...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Find mentions of 'non-determinism' and provide timestamps.</td>\n",
       "      <td>SELECT start_time_seconds, end_time_seconds FROM transcript_segments WHERE text LIKE '%non-determinism%'</td>\n",
       "      <td>The generated SQL is semantically correct and appropriate for the user query. The user is asking to find mentions of 'non-determinism' and provide timestamps. The SQL query is selecting the `start...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Did the transcript mention monitoring?</td>\n",
       "      <td>SELECT * FROM transcript_segments WHERE text LIKE '%monitoring%'</td>\n",
       "      <td>The User Query asks if the transcript mentioned the word \"monitoring.\" The Generated SQL is designed to search for the presence of the word \"monitoring\" within the text of a table named `transcrip...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Tell me about AI observability.</td>\n",
       "      <td>SELECT * FROM transcript_segments WHERE text LIKE '%AI observability%'</td>\n",
       "      <td>The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about \"AI observability,\" and the SQL query is designed to search for any transcrip...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>What was said about Discord?</td>\n",
       "      <td>SELECT text FROM transcript_segments WHERE text LIKE '%Discord%'</td>\n",
       "      <td>The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about what was said regarding \"Discord.\" The SQL query is designed to search for an...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>How many times was 'Discord' mentioned?</td>\n",
       "      <td>SELECT COUNT(*) as count FROM transcript_segments WHERE text LIKE '%Discord%'</td>\n",
       "      <td>The generated SQL query is semantically correct and appropriate for the user query. The user wants to know how many times the word 'Discord' was mentioned. The SQL query achieves this by counting ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>What did they say about reinforcement learning?</td>\n",
       "      <td>SELECT text FROM transcript_segments WHERE text LIKE '%reinforcement learning%'</td>\n",
       "      <td>The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about what was said regarding \"reinforcement learning.\" The SQL query is designed t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                                     user_query  \\\n",
       "0       0                                           Who is Jeff Pidcock?   \n",
       "1       1          What did Stefan Krawczyk say during his introduction?   \n",
       "2       2                            List all unique speakers mentioned.   \n",
       "3       3                        How many words did Hugo speak in total?   \n",
       "4       4  Find segments mentioning 'evaluation' and provide timestamps.   \n",
       "5       5                           Which speaker has the most segments?   \n",
       "6       6        What is the total word count for all segments combined?   \n",
       "7       7                                         Who mentioned Carvana?   \n",
       "8       8                      List the builders in residence mentioned.   \n",
       "9       9                         When did Nathan Danielsen first speak?   \n",
       "10     10  Summarize the key points about evaluation driven development.   \n",
       "11     11     Find mentions of 'non-determinism' and provide timestamps.   \n",
       "12     12                         Did the transcript mention monitoring?   \n",
       "13     13                                Tell me about AI observability.   \n",
       "14     14                                   What was said about Discord?   \n",
       "15     15                        How many times was 'Discord' mentioned?   \n",
       "16     16                What did they say about reinforcement learning?   \n",
       "\n",
       "                                                                                                                           generated_sql  \\\n",
       "0                                                                     SELECT * FROM transcript_segments WHERE text LIKE '%Jeff Pidcock%'   \n",
       "1                                    SELECT text FROM transcript_segments WHERE speaker='Stefan Krawczyk' AND text LIKE '%introduction%'   \n",
       "2                                                                                       SELECT DISTINCT speaker FROM transcript_segments   \n",
       "3                                                  SELECT SUM(word_count) AS total_words FROM transcript_segments WHERE speaker = 'Hugo'   \n",
       "4                              SELECT start_time_seconds, end_time_seconds, text FROM transcript_segments WHERE text LIKE '%evaluation%'   \n",
       "5      SELECT speaker, COUNT(segment_id) AS segment_count FROM transcript_segments GROUP BY speaker ORDER BY segment_count DESC LIMIT 1;   \n",
       "6                                                                   SELECT SUM(word_count) as total_word_count FROM transcript_segments;   \n",
       "7                                                              SELECT speaker, text FROM transcript_segments WHERE text LIKE '%Carvana%'   \n",
       "8                                                          SELECT text FROM transcript_segments WHERE text LIKE '%builder in residence%'   \n",
       "9   SELECT start_time_seconds, text FROM transcript_segments WHERE speaker = 'Nathan Danielsen' ORDER BY start_time_seconds ASC LIMIT 1;   \n",
       "10                                                   SELECT * FROM transcript_segments WHERE text LIKE '%evaluation driven development%'   \n",
       "11                              SELECT start_time_seconds, end_time_seconds FROM transcript_segments WHERE text LIKE '%non-determinism%'   \n",
       "12                                                                      SELECT * FROM transcript_segments WHERE text LIKE '%monitoring%'   \n",
       "13                                                                SELECT * FROM transcript_segments WHERE text LIKE '%AI observability%'   \n",
       "14                                                                      SELECT text FROM transcript_segments WHERE text LIKE '%Discord%'   \n",
       "15                                                         SELECT COUNT(*) as count FROM transcript_segments WHERE text LIKE '%Discord%'   \n",
       "16                                                       SELECT text FROM transcript_segments WHERE text LIKE '%reinforcement learning%'   \n",
       "\n",
       "                                                                                                                                                                                           raw_llm_response  \\\n",
       "0   The generated SQL is semantically correct and appropriate for the user query. The user is asking for information about \"Jeff Pidcock,\" and the SQL query is designed to search for any transcript se...   \n",
       "1   The Generated SQL is attempting to retrieve segments of text spoken by Stefan Krawczyk that include the word \"introduction.\" However, this approach may not accurately capture the entire introducti...   \n",
       "2   The User Query asks for a list of all unique speakers mentioned. The Generated SQL is:\\n\\n```sql\\nSELECT DISTINCT speaker FROM transcript_segments\\n```\\n\\nThis SQL query is semantically correct an...   \n",
       "3   The User Query asks for the total number of words spoken by Hugo. The Generated SQL aims to calculate this by summing the `word_count` from the `transcript_segments` table where the `speaker` is '...   \n",
       "4   The User Query asks to find segments that mention the word 'evaluation' and provide their timestamps. The Generated SQL correctly addresses this request by selecting the `start_time_seconds`, `end...   \n",
       "5   The User Query asks for the speaker with the most segments. The Generated SQL is designed to achieve this by:\\n\\n1. Selecting the `speaker` and counting the number of `segment_id` for each speaker...   \n",
       "6   The User Query asks for the total word count for all segments combined. The Generated SQL is:\\n\\n```sql\\nSELECT SUM(word_count) as total_word_count FROM transcript_segments;\\n```\\n\\nThis SQL state...   \n",
       "7   The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about who mentioned \"Carvana\" in a transcript. The SQL query is designed to search ...   \n",
       "8   The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for a list of \"builders in residence\" mentioned, and the SQL query is designed to search for any te...   \n",
       "9   The User Query asks for the first time Nathan Danielsen spoke. The Generated SQL is designed to retrieve the start time and text of the first segment where Nathan Danielsen is the speaker. \\n\\n- T...   \n",
       "10  The SQL query is syntactically correct, as it uses a SELECT statement to retrieve all columns from the `transcript_segments` table where the `text` column contains the phrase \"evaluation driven de...   \n",
       "11  The generated SQL is semantically correct and appropriate for the user query. The user is asking to find mentions of 'non-determinism' and provide timestamps. The SQL query is selecting the `start...   \n",
       "12  The User Query asks if the transcript mentioned the word \"monitoring.\" The Generated SQL is designed to search for the presence of the word \"monitoring\" within the text of a table named `transcrip...   \n",
       "13  The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about \"AI observability,\" and the SQL query is designed to search for any transcrip...   \n",
       "14  The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about what was said regarding \"Discord.\" The SQL query is designed to search for an...   \n",
       "15  The generated SQL query is semantically correct and appropriate for the user query. The user wants to know how many times the word 'Discord' was mentioned. The SQL query achieves this by counting ...   \n",
       "16  The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about what was said regarding \"reinforcement learning.\" The SQL query is designed t...   \n",
       "\n",
       "    sql_correctness_score  \n",
       "0                     1.0  \n",
       "1                     0.0  \n",
       "2                     1.0  \n",
       "3                     1.0  \n",
       "4                     1.0  \n",
       "5                     1.0  \n",
       "6                     1.0  \n",
       "7                     1.0  \n",
       "8                     1.0  \n",
       "9                     1.0  \n",
       "10                    0.0  \n",
       "11                    1.0  \n",
       "12                    1.0  \n",
       "13                    1.0  \n",
       "14                    1.0  \n",
       "15                    1.0  \n",
       "16                    1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- End Full Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 2: FULL EVALUATION & DATAFRAME OUTPUT ---\n",
    "import re\n",
    "import pandas as pd # Import pandas\n",
    "# Assumes functions from Cell 1 (call_openai_judge, evaluate_sql_correctness_direct_api) are defined\n",
    "# Assumes openai_client, evaluation_dataset, MODEL_TO_USE, SQL_CORRECTNESS_PROMPT_TEMPLATE are available\n",
    "\n",
    "print(f\"\\n--- Running Full Evaluation for SQL Correctness on {len(evaluation_dataset)} Examples ---\")\n",
    "\n",
    "# --- Prerequisite Check ---\n",
    "test_passed = True\n",
    "if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "    print(\"Evaluation FAIL: evaluation_dataset not found or empty.\")\n",
    "    test_passed = False\n",
    "if 'openai_client' not in locals() or not openai_client:\n",
    "    print(\"Evaluation FAIL: openai_client not initialized.\")\n",
    "    test_passed = False\n",
    "if 'SQL_CORRECTNESS_PROMPT_TEMPLATE' not in locals() or not SQL_CORRECTNESS_PROMPT_TEMPLATE:\n",
    "     print(\"Evaluation FAIL: SQL_CORRECTNESS_PROMPT_TEMPLATE not defined.\")\n",
    "     test_passed = False\n",
    "if 'evaluate_sql_correctness_direct_api' not in locals():\n",
    "     print(\"Evaluation FAIL: evaluate_sql_correctness_direct_api function not defined (Run Cell 1?).\")\n",
    "     test_passed = False\n",
    "if 'call_openai_judge' not in locals():\n",
    "     print(\"Evaluation FAIL: call_openai_judge function not defined (Run Cell 1?).\")\n",
    "     test_passed = False\n",
    "# --- End Prerequisite Check ---\n",
    "\n",
    "results_list = [] # Initialize list to store results\n",
    "\n",
    "if test_passed:\n",
    "    # Iterate through ALL examples using enumerate\n",
    "    for i, test_example in enumerate(evaluation_dataset):\n",
    "        print(f\"Processing Example {i}...\") # Progress indicator\n",
    "\n",
    "        test_output_data = test_example.input\n",
    "        test_expected_data = test_example.output # needed for function signature\n",
    "\n",
    "        user_query = test_output_data.get('user_query', 'MISSING')\n",
    "        generated_sql = test_output_data.get('generated_sql', 'MISSING')\n",
    "\n",
    "        # --- Call LLM judge directly for raw output ---\n",
    "        raw_judge_response = \"Skipped direct call\" # Default\n",
    "        if user_query != 'MISSING' and generated_sql != 'MISSING':\n",
    "            try:\n",
    "                test_prompt = SQL_CORRECTNESS_PROMPT_TEMPLATE.format(\n",
    "                    user_query=user_query,\n",
    "                    generated_sql=generated_sql\n",
    "                )\n",
    "                raw_judge_response = call_openai_judge(test_prompt, model=MODEL_TO_USE) # Use helper\n",
    "                # Reduce printing inside the loop for large datasets\n",
    "                # print(\"--- Raw LLM Judge Response ---\")\n",
    "                # print(raw_judge_response)\n",
    "                # print(\"----------------------------\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error calling LLM Judge directly on Example {i}: {e}\")\n",
    "                raw_judge_response = f\"Error during direct call: {e}\"\n",
    "        # else:\n",
    "            # print(f\"  Skipping direct LLM call on Example {i} due to missing query/SQL.\")\n",
    "\n",
    "\n",
    "        # --- Call the evaluator function ---\n",
    "        test_score_float = 0.0 # Default score\n",
    "        try:\n",
    "            test_score_float = evaluate_sql_correctness_direct_api(test_output_data, test_expected_data)\n",
    "            # print(f\"  Score Returned: {test_score_float}\") # Optional print inside loop\n",
    "        except Exception as e:\n",
    "            print(f\"  Error calling evaluator function on Example {i}: {e}\")\n",
    "            test_score_float = 0.0 # Assign 0.0 on error\n",
    "\n",
    "        # --- Append results to list ---\n",
    "        results_list.append({\n",
    "            \"index\": i,\n",
    "            \"user_query\": user_query,\n",
    "            \"generated_sql\": generated_sql,\n",
    "            \"raw_llm_response\": raw_judge_response,\n",
    "            \"sql_correctness_score\": test_score_float\n",
    "        })\n",
    "\n",
    "    # --- Convert list to DataFrame ---\n",
    "    print(\"\\nEvaluation loop complete. Creating DataFrame...\")\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # --- Display DataFrame ---\n",
    "    print(\"SQL Correctness Evaluation Results:\")\n",
    "    pd.set_option('display.max_rows', 100) # Show more rows if needed\n",
    "    pd.set_option('display.max_colwidth', 200) # Show more text width\n",
    "    display(results_df) # Use display() for better rendering in notebooks\n",
    "\n",
    "else:\n",
    "    print(\"--- Evaluation Aborted due to failed prerequisites ---\")\n",
    "\n",
    "print(f\"\\n--- End Full Evaluation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing LLM SQL scores to human labels...\n",
      "\n",
      "Comparison Results (Human vs. LLM Judge for SQL Correctness):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_query</th>\n",
       "      <th>generated_sql</th>\n",
       "      <th>human_sql_label</th>\n",
       "      <th>llm_sql_label</th>\n",
       "      <th>match</th>\n",
       "      <th>raw_llm_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is Jeff Pidcock?</td>\n",
       "      <td>SELECT * FROM transcript_segments WHERE text LIKE '%Jeff Pidcock%'</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>Correct</td>\n",
       "      <td>False</td>\n",
       "      <td>The generated SQL is semantically correct and appropriate for the user query. The user is asking for information about \"Jeff Pidcock,\" and the SQL query is designed to search for any transcript se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What did Stefan Krawczyk say during his introduction?</td>\n",
       "      <td>SELECT text FROM transcript_segments WHERE speaker='Stefan Krawczyk' AND text LIKE '%introduction%'</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>True</td>\n",
       "      <td>The Generated SQL is attempting to retrieve segments of text spoken by Stefan Krawczyk that include the word \"introduction.\" However, this approach may not accurately capture the entire introducti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>List all unique speakers mentioned.</td>\n",
       "      <td>SELECT DISTINCT speaker FROM transcript_segments</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>True</td>\n",
       "      <td>The User Query asks for a list of all unique speakers mentioned. The Generated SQL is:\\n\\n```sql\\nSELECT DISTINCT speaker FROM transcript_segments\\n```\\n\\nThis SQL query is semantically correct an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many words did Hugo speak in total?</td>\n",
       "      <td>SELECT SUM(word_count) AS total_words FROM transcript_segments WHERE speaker = 'Hugo'</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>Correct</td>\n",
       "      <td>False</td>\n",
       "      <td>The User Query asks for the total number of words spoken by Hugo. The Generated SQL aims to calculate this by summing the `word_count` from the `transcript_segments` table where the `speaker` is '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Find segments mentioning 'evaluation' and provide timestamps.</td>\n",
       "      <td>SELECT start_time_seconds, end_time_seconds, text FROM transcript_segments WHERE text LIKE '%evaluation%'</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>True</td>\n",
       "      <td>The User Query asks to find segments that mention the word 'evaluation' and provide their timestamps. The Generated SQL correctly addresses this request by selecting the `start_time_seconds`, `end...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Which speaker has the most segments?</td>\n",
       "      <td>SELECT speaker, COUNT(segment_id) AS segment_count FROM transcript_segments GROUP BY speaker ORDER BY segment_count DESC LIMIT 1;</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>True</td>\n",
       "      <td>The User Query asks for the speaker with the most segments. The Generated SQL is designed to achieve this by:\\n\\n1. Selecting the `speaker` and counting the number of `segment_id` for each speaker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the total word count for all segments combined?</td>\n",
       "      <td>SELECT SUM(word_count) as total_word_count FROM transcript_segments;</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>True</td>\n",
       "      <td>The User Query asks for the total word count for all segments combined. The Generated SQL is:\\n\\n```sql\\nSELECT SUM(word_count) as total_word_count FROM transcript_segments;\\n```\\n\\nThis SQL state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Who mentioned Carvana?</td>\n",
       "      <td>SELECT speaker, text FROM transcript_segments WHERE text LIKE '%Carvana%'</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>True</td>\n",
       "      <td>The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about who mentioned \"Carvana\" in a transcript. The SQL query is designed to search ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>List the builders in residence mentioned.</td>\n",
       "      <td>SELECT text FROM transcript_segments WHERE text LIKE '%builder in residence%'</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>Correct</td>\n",
       "      <td>False</td>\n",
       "      <td>The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for a list of \"builders in residence\" mentioned, and the SQL query is designed to search for any te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>When did Nathan Danielsen first speak?</td>\n",
       "      <td>SELECT start_time_seconds, text FROM transcript_segments WHERE speaker = 'Nathan Danielsen' ORDER BY start_time_seconds ASC LIMIT 1;</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>True</td>\n",
       "      <td>The User Query asks for the first time Nathan Danielsen spoke. The Generated SQL is designed to retrieve the start time and text of the first segment where Nathan Danielsen is the speaker. \\n\\n- T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Summarize the key points about evaluation driven development.</td>\n",
       "      <td>SELECT * FROM transcript_segments WHERE text LIKE '%evaluation driven development%'</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>False</td>\n",
       "      <td>The SQL query is syntactically correct, as it uses a SELECT statement to retrieve all columns from the `transcript_segments` table where the `text` column contains the phrase \"evaluation driven de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Find mentions of 'non-determinism' and provide timestamps.</td>\n",
       "      <td>SELECT start_time_seconds, end_time_seconds FROM transcript_segments WHERE text LIKE '%non-determinism%'</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated SQL is semantically correct and appropriate for the user query. The user is asking to find mentions of 'non-determinism' and provide timestamps. The SQL query is selecting the `start...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Did the transcript mention monitoring?</td>\n",
       "      <td>SELECT * FROM transcript_segments WHERE text LIKE '%monitoring%'</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>Correct</td>\n",
       "      <td>False</td>\n",
       "      <td>The User Query asks if the transcript mentioned the word \"monitoring.\" The Generated SQL is designed to search for the presence of the word \"monitoring\" within the text of a table named `transcrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tell me about AI observability.</td>\n",
       "      <td>SELECT * FROM transcript_segments WHERE text LIKE '%AI observability%'</td>\n",
       "      <td>Incorrect</td>\n",
       "      <td>Correct</td>\n",
       "      <td>False</td>\n",
       "      <td>The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about \"AI observability,\" and the SQL query is designed to search for any transcrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What was said about Discord?</td>\n",
       "      <td>SELECT text FROM transcript_segments WHERE text LIKE '%Discord%'</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>True</td>\n",
       "      <td>The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about what was said regarding \"Discord.\" The SQL query is designed to search for an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How many times was 'Discord' mentioned?</td>\n",
       "      <td>SELECT COUNT(*) as count FROM transcript_segments WHERE text LIKE '%Discord%'</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>True</td>\n",
       "      <td>The generated SQL query is semantically correct and appropriate for the user query. The user wants to know how many times the word 'Discord' was mentioned. The SQL query achieves this by counting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>What did they say about reinforcement learning?</td>\n",
       "      <td>SELECT text FROM transcript_segments WHERE text LIKE '%reinforcement learning%'</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Correct</td>\n",
       "      <td>True</td>\n",
       "      <td>The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about what was said regarding \"reinforcement learning.\" The SQL query is designed t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       user_query  \\\n",
       "0                                            Who is Jeff Pidcock?   \n",
       "1           What did Stefan Krawczyk say during his introduction?   \n",
       "2                             List all unique speakers mentioned.   \n",
       "3                         How many words did Hugo speak in total?   \n",
       "4   Find segments mentioning 'evaluation' and provide timestamps.   \n",
       "5                            Which speaker has the most segments?   \n",
       "6         What is the total word count for all segments combined?   \n",
       "7                                          Who mentioned Carvana?   \n",
       "8                       List the builders in residence mentioned.   \n",
       "9                          When did Nathan Danielsen first speak?   \n",
       "10  Summarize the key points about evaluation driven development.   \n",
       "11     Find mentions of 'non-determinism' and provide timestamps.   \n",
       "12                         Did the transcript mention monitoring?   \n",
       "13                                Tell me about AI observability.   \n",
       "14                                   What was said about Discord?   \n",
       "15                        How many times was 'Discord' mentioned?   \n",
       "16                What did they say about reinforcement learning?   \n",
       "\n",
       "                                                                                                                           generated_sql  \\\n",
       "0                                                                     SELECT * FROM transcript_segments WHERE text LIKE '%Jeff Pidcock%'   \n",
       "1                                    SELECT text FROM transcript_segments WHERE speaker='Stefan Krawczyk' AND text LIKE '%introduction%'   \n",
       "2                                                                                       SELECT DISTINCT speaker FROM transcript_segments   \n",
       "3                                                  SELECT SUM(word_count) AS total_words FROM transcript_segments WHERE speaker = 'Hugo'   \n",
       "4                              SELECT start_time_seconds, end_time_seconds, text FROM transcript_segments WHERE text LIKE '%evaluation%'   \n",
       "5      SELECT speaker, COUNT(segment_id) AS segment_count FROM transcript_segments GROUP BY speaker ORDER BY segment_count DESC LIMIT 1;   \n",
       "6                                                                   SELECT SUM(word_count) as total_word_count FROM transcript_segments;   \n",
       "7                                                              SELECT speaker, text FROM transcript_segments WHERE text LIKE '%Carvana%'   \n",
       "8                                                          SELECT text FROM transcript_segments WHERE text LIKE '%builder in residence%'   \n",
       "9   SELECT start_time_seconds, text FROM transcript_segments WHERE speaker = 'Nathan Danielsen' ORDER BY start_time_seconds ASC LIMIT 1;   \n",
       "10                                                   SELECT * FROM transcript_segments WHERE text LIKE '%evaluation driven development%'   \n",
       "11                              SELECT start_time_seconds, end_time_seconds FROM transcript_segments WHERE text LIKE '%non-determinism%'   \n",
       "12                                                                      SELECT * FROM transcript_segments WHERE text LIKE '%monitoring%'   \n",
       "13                                                                SELECT * FROM transcript_segments WHERE text LIKE '%AI observability%'   \n",
       "14                                                                      SELECT text FROM transcript_segments WHERE text LIKE '%Discord%'   \n",
       "15                                                         SELECT COUNT(*) as count FROM transcript_segments WHERE text LIKE '%Discord%'   \n",
       "16                                                       SELECT text FROM transcript_segments WHERE text LIKE '%reinforcement learning%'   \n",
       "\n",
       "   human_sql_label llm_sql_label  match  \\\n",
       "0        Incorrect       Correct  False   \n",
       "1        Incorrect     Incorrect   True   \n",
       "2          Correct       Correct   True   \n",
       "3        Incorrect       Correct  False   \n",
       "4          Correct       Correct   True   \n",
       "5          Correct       Correct   True   \n",
       "6          Correct       Correct   True   \n",
       "7          Correct       Correct   True   \n",
       "8        Incorrect       Correct  False   \n",
       "9          Correct       Correct   True   \n",
       "10         Correct     Incorrect  False   \n",
       "11         Correct       Correct   True   \n",
       "12       Incorrect       Correct  False   \n",
       "13       Incorrect       Correct  False   \n",
       "14         Correct       Correct   True   \n",
       "15         Correct       Correct   True   \n",
       "16         Correct       Correct   True   \n",
       "\n",
       "                                                                                                                                                                                           raw_llm_response  \n",
       "0   The generated SQL is semantically correct and appropriate for the user query. The user is asking for information about \"Jeff Pidcock,\" and the SQL query is designed to search for any transcript se...  \n",
       "1   The Generated SQL is attempting to retrieve segments of text spoken by Stefan Krawczyk that include the word \"introduction.\" However, this approach may not accurately capture the entire introducti...  \n",
       "2   The User Query asks for a list of all unique speakers mentioned. The Generated SQL is:\\n\\n```sql\\nSELECT DISTINCT speaker FROM transcript_segments\\n```\\n\\nThis SQL query is semantically correct an...  \n",
       "3   The User Query asks for the total number of words spoken by Hugo. The Generated SQL aims to calculate this by summing the `word_count` from the `transcript_segments` table where the `speaker` is '...  \n",
       "4   The User Query asks to find segments that mention the word 'evaluation' and provide their timestamps. The Generated SQL correctly addresses this request by selecting the `start_time_seconds`, `end...  \n",
       "5   The User Query asks for the speaker with the most segments. The Generated SQL is designed to achieve this by:\\n\\n1. Selecting the `speaker` and counting the number of `segment_id` for each speaker...  \n",
       "6   The User Query asks for the total word count for all segments combined. The Generated SQL is:\\n\\n```sql\\nSELECT SUM(word_count) as total_word_count FROM transcript_segments;\\n```\\n\\nThis SQL state...  \n",
       "7   The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about who mentioned \"Carvana\" in a transcript. The SQL query is designed to search ...  \n",
       "8   The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for a list of \"builders in residence\" mentioned, and the SQL query is designed to search for any te...  \n",
       "9   The User Query asks for the first time Nathan Danielsen spoke. The Generated SQL is designed to retrieve the start time and text of the first segment where Nathan Danielsen is the speaker. \\n\\n- T...  \n",
       "10  The SQL query is syntactically correct, as it uses a SELECT statement to retrieve all columns from the `transcript_segments` table where the `text` column contains the phrase \"evaluation driven de...  \n",
       "11  The generated SQL is semantically correct and appropriate for the user query. The user is asking to find mentions of 'non-determinism' and provide timestamps. The SQL query is selecting the `start...  \n",
       "12  The User Query asks if the transcript mentioned the word \"monitoring.\" The Generated SQL is designed to search for the presence of the word \"monitoring\" within the text of a table named `transcrip...  \n",
       "13  The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about \"AI observability,\" and the SQL query is designed to search for any transcrip...  \n",
       "14  The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about what was said regarding \"Discord.\" The SQL query is designed to search for an...  \n",
       "15  The generated SQL query is semantically correct and appropriate for the user query. The user wants to know how many times the word 'Discord' was mentioned. The SQL query achieves this by counting ...  \n",
       "16  The Generated SQL is semantically correct and appropriate for the User Query. The user is asking for information about what was said regarding \"reinforcement learning.\" The SQL query is designed t...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agreement between LLM Judge and Human Labels: 64.71% (11/17)\n"
     ]
    }
   ],
   "source": [
    "# --- Compare LLM SQL Correctness Scores to Human Labels ---\n",
    "import pandas as pd # Ensure pandas is imported\n",
    "\n",
    "print(\"Comparing LLM SQL scores to human labels...\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Key for the human label in evaluation_dataset[i].output\n",
    "# Identified from notebook inspection as 'sql_correctness_label'\n",
    "HUMAN_LABEL_KEY = 'sql_correctness_label'\n",
    "# --- End Configuration ---\n",
    "\n",
    "try:\n",
    "    # Check prerequisite DataFrames/Datasets\n",
    "    if 'results_df' not in locals():\n",
    "        raise NameError(\"'results_df' DataFrame not found. Please run the evaluation cell first.\")\n",
    "    if 'evaluation_dataset' not in locals():\n",
    "        raise NameError(\"'evaluation_dataset' not found. Please ensure it is loaded.\")\n",
    "\n",
    "    # 1. Extract Human Labels from the dataset\n",
    "    human_labels = [example.output.get(HUMAN_LABEL_KEY, \"MISSING\") for example in evaluation_dataset]\n",
    "\n",
    "    # Check length consistency\n",
    "    if len(human_labels) != len(results_df):\n",
    "        raise ValueError(f\"Mismatch in lengths: results_df has {len(results_df)} rows, but extracted {len(human_labels)} human labels.\")\n",
    "\n",
    "    # 2. Add Human Labels column to the results DataFrame\n",
    "    results_df['human_sql_label'] = human_labels\n",
    "\n",
    "    # 3. Add LLM Label (string format) & Comparison column\n",
    "    def score_to_label(score):\n",
    "        # Converts 1.0 to \"Correct\", anything else (0.0, errors) to \"Incorrect\"\n",
    "        return \"Correct\" if score == 1.0 else \"Incorrect\"\n",
    "\n",
    "    results_df['llm_sql_label'] = results_df['sql_correctness_score'].apply(score_to_label)\n",
    "\n",
    "    # Compare strings (case-insensitive), handle \"MISSING\" human labels\n",
    "    results_df['match'] = results_df.apply(\n",
    "        lambda row: str(row['llm_sql_label']).lower() == str(row['human_sql_label']).lower()\n",
    "                    if row['human_sql_label'] != \"MISSING\" else None, # Result is None if human label was missing\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # 4. Display Key Comparison Columns\n",
    "    print(\"\\nComparison Results (Human vs. LLM Judge for SQL Correctness):\")\n",
    "    display_cols = ['user_query', 'generated_sql', 'human_sql_label', 'llm_sql_label', 'match', 'raw_llm_response']\n",
    "    # Filter out any columns that might not exist (e.g., if df creation failed partially)\n",
    "    display_cols = [col for col in display_cols if col in results_df.columns]\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    pd.set_option('display.max_colwidth', 200)\n",
    "    display(results_df[display_cols])\n",
    "\n",
    "    # 5. Calculate and Print Match Percentage\n",
    "    if 'match' in results_df.columns:\n",
    "        match_count = results_df['match'].sum() # Counts True values\n",
    "        valid_comparisons = results_df['match'].notna().sum() # Counts non-None values\n",
    "        if valid_comparisons > 0:\n",
    "            match_percentage = (match_count / valid_comparisons) * 100\n",
    "            print(f\"\\nAgreement between LLM Judge and Human Labels: {match_percentage:.2f}% ({match_count}/{valid_comparisons})\")\n",
    "        else:\n",
    "            print(\"\\nCould not calculate agreement percentage (no valid comparisons).\")\n",
    "\n",
    "except (NameError, AttributeError, ValueError, KeyError) as e:\n",
    "    print(f\"\\nError during comparison: {e}\")\n",
    "    print(\"Please ensure 'results_df' exists, 'evaluation_dataset' is loaded correctly,\")\n",
    "    print(f\"and the key '{HUMAN_LABEL_KEY}' is correct for the human labels in evaluation_dataset[i].output.\")\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during comparison: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined: FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE\n",
      "Defined: evaluate_final_answer_quality_direct_api function\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluator 3: Final Answer Quality (Mimicking Successful Pattern) ---\n",
    "import re\n",
    "import logging # Or use print if preferred for errors\n",
    "# Assumes openai_client is initialized globally\n",
    "# Assumes MODEL_TO_USE = \"gpt-4o\" is defined globally\n",
    "\n",
    "# --- Define Prompt Template ---\n",
    "# Make sure this is defined before the function\n",
    "FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE = \"\"\"Evaluate if the Final Answer accurately and completely answers the User Query, based ONLY on the query and answer text. Do not assume external data or SQL.\n",
    "\n",
    "User Query:\n",
    "{user_query}\n",
    "\n",
    "Final Answer:\n",
    "{final_answer}\n",
    "\n",
    "Is the Final Answer good quality (accurate, relevant, complete)?\n",
    "Provide a brief EXPLANATION and finish with LABEL: Good or LABEL: Bad.\n",
    "\"\"\"\n",
    "print(\"Defined: FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE\") # Optional confirmation\n",
    "\n",
    "# --- Evaluator Definition ---\n",
    "def evaluate_final_answer_quality_direct_api(output: dict, expected: dict) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates Final Answer quality via direct OpenAI API call (Mimics successful pattern).\n",
    "    Returns 1.0 for Good, 0.0 otherwise (Bad, Error, Parsing Failure).\n",
    "    \"\"\"\n",
    "    # Rely on global variables: openai_client, FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE, MODEL_TO_USE\n",
    "    score = 0.0 # Default to 0.0 (representing \"Bad\" or error)\n",
    "\n",
    "    # --- 1. Prerequisites & Data Checks ---\n",
    "    if not openai_client:\n",
    "        print(\"Prerequisite Error: OpenAI client not initialized.\") # Changed from logging\n",
    "        return 0.0\n",
    "    if not FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE:\n",
    "        print(\"Prerequisite Error: FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE not defined.\")\n",
    "        return 0.0\n",
    "\n",
    "    user_query = output.get('user_query')\n",
    "    final_answer = output.get('final_answer')\n",
    "\n",
    "    if not user_query or not final_answer:\n",
    "        # print(\"Data Error: Missing 'user_query' or 'final_answer' in output.\") # Optional\n",
    "        return 0.0\n",
    "\n",
    "    # --- 2. Format Prompt ---\n",
    "    try:\n",
    "        prompt = FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE.format(\n",
    "            user_query=user_query,\n",
    "            final_answer=final_answer\n",
    "        )\n",
    "    except KeyError as e:\n",
    "         print(f\"Prompt Formatting Error: Missing key {e}\")\n",
    "         return 0.0\n",
    "\n",
    "    # --- 3. Call API & Parse Result ---\n",
    "    try:\n",
    "        # Assuming call_openai_judge helper is defined and available\n",
    "        raw_output = call_openai_judge(prompt, model=MODEL_TO_USE)\n",
    "        label_match = re.search(r\"LABEL:\\s*(\\w+)\", raw_output, re.IGNORECASE)\n",
    "\n",
    "        # Return 1.0 ONLY if API succeeded AND label is exactly \"Good\"\n",
    "        if label_match and label_match.group(1).strip().capitalize() == \"Good\":\n",
    "            score = 1.0\n",
    "        # else: score remains 0.0\n",
    "\n",
    "    except NameError:\n",
    "        # Explicitly catch if the helper function isn't defined\n",
    "         print(\"Error: call_openai_judge function not found.\")\n",
    "         # score remains 0.0\n",
    "    except Exception as e:\n",
    "        # Covers cases: OpenAI API call failed inside helper, or other unexpected errors\n",
    "        print(f\"API Call/Evaluation Error during Final Answer evaluation: {e}\")\n",
    "        # score remains 0.0\n",
    "\n",
    "    return score\n",
    "\n",
    "print(\"Defined: evaluate_final_answer_quality_direct_api function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Full Evaluation for Final Answer Quality ---\n",
      "Processing 17 examples for Final Answer Quality...\n",
      "\n",
      "Evaluation loop complete. Creating DataFrame...\n",
      "Adding comparison columns...\n",
      "\n",
      "Final Answer Quality Evaluation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>user_query</th>\n",
       "      <th>final_answer</th>\n",
       "      <th>human_answer_label_raw</th>\n",
       "      <th>llm_answer_label</th>\n",
       "      <th>answer_match</th>\n",
       "      <th>raw_llm_response_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Who is Jeff Pidcock?</td>\n",
       "      <td>I cannot answer the question about who Jeff Pidcock is based on the available transcript data.</td>\n",
       "      <td>Fail</td>\n",
       "      <td>Bad</td>\n",
       "      <td>True</td>\n",
       "      <td>The Final Answer accurately states that there is no information available about Jeff Pidcock in the provided data. It is relevant to the User Query as it directly addresses the question. However, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What did Stefan Krawczyk say during his introduction?</td>\n",
       "      <td>I cannot answer the question based on the available transcript data.</td>\n",
       "      <td>Fail</td>\n",
       "      <td>Bad</td>\n",
       "      <td>True</td>\n",
       "      <td>The Final Answer is accurate and relevant because it directly addresses the User Query by stating that the information is not available in the provided transcript data. However, it is not complete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>List all unique speakers mentioned.</td>\n",
       "      <td>The unique speakers mentioned in the transcript are:\\n\\n1. Hugo Bowne-Anderson\\n2. Stefan Krawczyk\\n3. GP\\n4. William Horton\\n5. Nathan Danielsen\\n6. Greg\\n7. Sanket Firodiya\\n8. Philip Kiely\\n9. ...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer appears to be a list of unique speakers mentioned in a transcript, as requested by the User Query. However, without access to the actual transcript or additional context, it is im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>How many words did Hugo speak in total?</td>\n",
       "      <td>I cannot answer the question about how many words Hugo spoke in total based on the available transcript data.</td>\n",
       "      <td>Fail</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer is good quality because it directly addresses the User Query by stating that the information needed to determine how many words Hugo spoke is not available in the transcript data....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Find segments mentioning 'evaluation' and provide timestamps.</td>\n",
       "      <td>Here are the segments that mention 'evaluation' along with their timestamps:\\n\\n1. **Timestamp:** 12:01 - 12:28  \\n   **Text:** \"the foundations of Llm software and evaluation driven development.....</td>\n",
       "      <td>Pass</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer provides a list of segments mentioning 'evaluation' along with their timestamps, which directly addresses the User Query. The answer includes multiple instances where 'evaluation'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Which speaker has the most segments?</td>\n",
       "      <td>The speaker with the most segments is Hugo Bowne-Anderson, with a total of 430 segments.</td>\n",
       "      <td>Pass</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer directly addresses the User Query by identifying the speaker with the most segments and providing the specific number of segments (430) attributed to Hugo Bowne-Anderson. The answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>What is the total word count for all segments combined?</td>\n",
       "      <td>The total word count for all segments combined is 20,212 words.</td>\n",
       "      <td>Pass</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer provides a specific total word count of 20,212 words, which directly addresses the User Query asking for the total word count for all segments combined. The answer is relevant and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Who mentioned Carvana?</td>\n",
       "      <td>Nathan Danielsen mentioned Carvana.</td>\n",
       "      <td>Pass</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer is good quality because it directly and accurately responds to the User Query by specifying who mentioned Carvana. The answer is relevant and complete as it provides the exact inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>List the builders in residence mentioned.</td>\n",
       "      <td>I cannot answer the question based on the available transcript data.</td>\n",
       "      <td>Fail</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer states that it cannot answer the question based on the available transcript data. This implies that the information about the builders in residence is not present in the data the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>When did Nathan Danielsen first speak?</td>\n",
       "      <td>Nathan Danielsen first spoke at 7 minutes and 42.47 seconds into the workshop.</td>\n",
       "      <td>Correct</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer provides a specific time when Nathan Danielsen first spoke, which directly addresses the User Query asking for the timing of his first speech. The answer is relevant and complete ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Summarize the key points about evaluation driven development.</td>\n",
       "      <td>Evaluation driven development focuses on the foundational aspects of software powered by machine learning and large language models (LLMs). It emphasizes a shift from traditional software developm...</td>\n",
       "      <td>Pass</td>\n",
       "      <td>Bad</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer provides a general overview of evaluation driven development, highlighting its focus on software powered by machine learning and large language models, and the importance of itera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Find mentions of 'non-determinism' and provide timestamps.</td>\n",
       "      <td>The mentions of 'non-determinism' occur at the following timestamps:\\n\\n1. From 14:14.54 to 14:37.46\\n2. From 46:20.51 to 46:29.849</td>\n",
       "      <td>Pass</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer provides specific timestamps for mentions of 'non-determinism,' which directly addresses the User Query. The query asked for mentions and their timestamps, and the answer lists tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Did the transcript mention monitoring?</td>\n",
       "      <td>Yes, the transcript mentioned monitoring in several contexts, including discussing its importance in relation to evaluation and business metrics. It also mentioned monitoring questions in a Discor...</td>\n",
       "      <td>Fail</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer is good quality. It directly addresses the User Query by confirming that the transcript mentioned monitoring. It provides additional context by specifying that monitoring was disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Tell me about AI observability.</td>\n",
       "      <td>I cannot answer the question about AI observability based on the available transcript data.</td>\n",
       "      <td>Fail</td>\n",
       "      <td>Bad</td>\n",
       "      <td>True</td>\n",
       "      <td>The Final Answer does not provide any information about AI observability, which is the main focus of the User Query. Instead, it states an inability to answer the question based on the available t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>What was said about Discord?</td>\n",
       "      <td>In the workshop, several points were made regarding Discord:\\n\\n1. Participants expressed excitement about using Discord for community engagement and communication, highlighting the positive inter...</td>\n",
       "      <td>Correcr</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer provides a detailed and comprehensive summary of what was said about Discord during the workshop. It covers various aspects such as community engagement, communication, the facili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>How many times was 'Discord' mentioned?</td>\n",
       "      <td>'Discord' was mentioned 29 times.</td>\n",
       "      <td>Pass</td>\n",
       "      <td>Good</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer directly addresses the User Query by providing a specific number of times 'Discord' was mentioned, which is 29 times. The answer is relevant and complete as it responds precisely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>What did they say about reinforcement learning?</td>\n",
       "      <td>I cannot answer the question based on the available transcript data.</td>\n",
       "      <td>Pass</td>\n",
       "      <td>Bad</td>\n",
       "      <td>False</td>\n",
       "      <td>The Final Answer does not provide any information about what was said regarding reinforcement learning. It simply states an inability to answer based on the available transcript data, which does n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                                     user_query  \\\n",
       "0       0                                           Who is Jeff Pidcock?   \n",
       "1       1          What did Stefan Krawczyk say during his introduction?   \n",
       "2       2                            List all unique speakers mentioned.   \n",
       "3       3                        How many words did Hugo speak in total?   \n",
       "4       4  Find segments mentioning 'evaluation' and provide timestamps.   \n",
       "5       5                           Which speaker has the most segments?   \n",
       "6       6        What is the total word count for all segments combined?   \n",
       "7       7                                         Who mentioned Carvana?   \n",
       "8       8                      List the builders in residence mentioned.   \n",
       "9       9                         When did Nathan Danielsen first speak?   \n",
       "10     10  Summarize the key points about evaluation driven development.   \n",
       "11     11     Find mentions of 'non-determinism' and provide timestamps.   \n",
       "12     12                         Did the transcript mention monitoring?   \n",
       "13     13                                Tell me about AI observability.   \n",
       "14     14                                   What was said about Discord?   \n",
       "15     15                        How many times was 'Discord' mentioned?   \n",
       "16     16                What did they say about reinforcement learning?   \n",
       "\n",
       "                                                                                                                                                                                               final_answer  \\\n",
       "0                                                                                                            I cannot answer the question about who Jeff Pidcock is based on the available transcript data.   \n",
       "1                                                                                                                                      I cannot answer the question based on the available transcript data.   \n",
       "2   The unique speakers mentioned in the transcript are:\\n\\n1. Hugo Bowne-Anderson\\n2. Stefan Krawczyk\\n3. GP\\n4. William Horton\\n5. Nathan Danielsen\\n6. Greg\\n7. Sanket Firodiya\\n8. Philip Kiely\\n9. ...   \n",
       "3                                                                                             I cannot answer the question about how many words Hugo spoke in total based on the available transcript data.   \n",
       "4   Here are the segments that mention 'evaluation' along with their timestamps:\\n\\n1. **Timestamp:** 12:01 - 12:28  \\n   **Text:** \"the foundations of Llm software and evaluation driven development.....   \n",
       "5                                                                                                                  The speaker with the most segments is Hugo Bowne-Anderson, with a total of 430 segments.   \n",
       "6                                                                                                                                           The total word count for all segments combined is 20,212 words.   \n",
       "7                                                                                                                                                                       Nathan Danielsen mentioned Carvana.   \n",
       "8                                                                                                                                      I cannot answer the question based on the available transcript data.   \n",
       "9                                                                                                                            Nathan Danielsen first spoke at 7 minutes and 42.47 seconds into the workshop.   \n",
       "10  Evaluation driven development focuses on the foundational aspects of software powered by machine learning and large language models (LLMs). It emphasizes a shift from traditional software developm...   \n",
       "11                                                                      The mentions of 'non-determinism' occur at the following timestamps:\\n\\n1. From 14:14.54 to 14:37.46\\n2. From 46:20.51 to 46:29.849   \n",
       "12  Yes, the transcript mentioned monitoring in several contexts, including discussing its importance in relation to evaluation and business metrics. It also mentioned monitoring questions in a Discor...   \n",
       "13                                                                                                              I cannot answer the question about AI observability based on the available transcript data.   \n",
       "14  In the workshop, several points were made regarding Discord:\\n\\n1. Participants expressed excitement about using Discord for community engagement and communication, highlighting the positive inter...   \n",
       "15                                                                                                                                                                        'Discord' was mentioned 29 times.   \n",
       "16                                                                                                                                     I cannot answer the question based on the available transcript data.   \n",
       "\n",
       "   human_answer_label_raw llm_answer_label  answer_match  \\\n",
       "0                    Fail              Bad          True   \n",
       "1                    Fail              Bad          True   \n",
       "2                    Pass             Good         False   \n",
       "3                    Fail             Good         False   \n",
       "4                    Pass             Good         False   \n",
       "5                    Pass             Good         False   \n",
       "6                    Pass             Good         False   \n",
       "7                    Pass             Good         False   \n",
       "8                    Fail             Good         False   \n",
       "9                 Correct             Good         False   \n",
       "10                   Pass              Bad         False   \n",
       "11                   Pass             Good         False   \n",
       "12                   Fail             Good         False   \n",
       "13                   Fail              Bad          True   \n",
       "14                Correcr             Good         False   \n",
       "15                   Pass             Good         False   \n",
       "16                   Pass              Bad         False   \n",
       "\n",
       "                                                                                                                                                                                    raw_llm_response_answer  \n",
       "0   The Final Answer accurately states that there is no information available about Jeff Pidcock in the provided data. It is relevant to the User Query as it directly addresses the question. However, ...  \n",
       "1   The Final Answer is accurate and relevant because it directly addresses the User Query by stating that the information is not available in the provided transcript data. However, it is not complete...  \n",
       "2   The Final Answer appears to be a list of unique speakers mentioned in a transcript, as requested by the User Query. However, without access to the actual transcript or additional context, it is im...  \n",
       "3   The Final Answer is good quality because it directly addresses the User Query by stating that the information needed to determine how many words Hugo spoke is not available in the transcript data....  \n",
       "4   The Final Answer provides a list of segments mentioning 'evaluation' along with their timestamps, which directly addresses the User Query. The answer includes multiple instances where 'evaluation'...  \n",
       "5   The Final Answer directly addresses the User Query by identifying the speaker with the most segments and providing the specific number of segments (430) attributed to Hugo Bowne-Anderson. The answ...  \n",
       "6   The Final Answer provides a specific total word count of 20,212 words, which directly addresses the User Query asking for the total word count for all segments combined. The answer is relevant and...  \n",
       "7   The Final Answer is good quality because it directly and accurately responds to the User Query by specifying who mentioned Carvana. The answer is relevant and complete as it provides the exact inf...  \n",
       "8   The Final Answer states that it cannot answer the question based on the available transcript data. This implies that the information about the builders in residence is not present in the data the ...  \n",
       "9   The Final Answer provides a specific time when Nathan Danielsen first spoke, which directly addresses the User Query asking for the timing of his first speech. The answer is relevant and complete ...  \n",
       "10  The Final Answer provides a general overview of evaluation driven development, highlighting its focus on software powered by machine learning and large language models, and the importance of itera...  \n",
       "11  The Final Answer provides specific timestamps for mentions of 'non-determinism,' which directly addresses the User Query. The query asked for mentions and their timestamps, and the answer lists tw...  \n",
       "12  The Final Answer is good quality. It directly addresses the User Query by confirming that the transcript mentioned monitoring. It provides additional context by specifying that monitoring was disc...  \n",
       "13  The Final Answer does not provide any information about AI observability, which is the main focus of the User Query. Instead, it states an inability to answer the question based on the available t...  \n",
       "14  The Final Answer provides a detailed and comprehensive summary of what was said about Discord during the workshop. It covers various aspects such as community engagement, communication, the facili...  \n",
       "15  The Final Answer directly addresses the User Query by providing a specific number of times 'Discord' was mentioned, which is 29 times. The answer is relevant and complete as it responds precisely ...  \n",
       "16  The Final Answer does not provide any information about what was said regarding reinforcement learning. It simply states an inability to answer based on the available transcript data, which does n...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agreement between LLM Judge and Human Labels (Final Answer): 17.65% (3/17)\n",
      "\n",
      "--- End Final Answer Quality Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "# --- Full Evaluation & Comparison for Final Answer Quality ---\n",
    "import pandas as pd\n",
    "import re # Ensure re is imported\n",
    "\n",
    "print(\"\\n--- Running Full Evaluation for Final Answer Quality ---\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# Key for the human label in evaluation_dataset[i].output\n",
    "# Identified from notebook inspection as 'final_answer_quality_label'\n",
    "HUMAN_LABEL_KEY_ANSWER = 'final_answer_quality_label'\n",
    "\n",
    "# Expected Labels from the LLM for this evaluator\n",
    "# Note: Dataset uses \"Fail\" but our prompt asks for \"Bad\". We need to map.\n",
    "HUMAN_LABEL_MAP = {\"Fail\": \"Bad\"} # Map human label \"Fail\" to expected LLM \"Bad\"\n",
    "LLM_POSITIVE_LABEL = \"Good\" # What our prompt asks for as the \"good\" label\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "# --- Prerequisite Check ---\n",
    "eval_passed = True\n",
    "if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "    print(\"Evaluation FAIL: evaluation_dataset not found or empty.\")\n",
    "    eval_passed = False\n",
    "if 'openai_client' not in locals() or not openai_client:\n",
    "    print(\"Evaluation FAIL: openai_client not initialized.\")\n",
    "    eval_passed = False\n",
    "if 'FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE' not in locals() or not FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE:\n",
    "     print(\"Evaluation FAIL: FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE not defined.\")\n",
    "     eval_passed = False\n",
    "if 'evaluate_final_answer_quality_direct_api' not in locals():\n",
    "     print(\"Evaluation FAIL: evaluate_final_answer_quality_direct_api function not defined.\")\n",
    "     eval_passed = False\n",
    "if 'call_openai_judge' not in locals():\n",
    "     print(\"Evaluation FAIL: call_openai_judge function not defined.\")\n",
    "     eval_passed = False\n",
    "# --- End Prerequisite Check ---\n",
    "\n",
    "answer_results_list = [] # Initialize list for results\n",
    "\n",
    "if eval_passed:\n",
    "    print(f\"Processing {len(evaluation_dataset)} examples for Final Answer Quality...\")\n",
    "    # Iterate through ALL examples\n",
    "    for i, test_example in enumerate(evaluation_dataset):\n",
    "        # print(f\"Processing Example {i}...\") # Can uncomment for verbose progress\n",
    "\n",
    "        test_output_data = test_example.input\n",
    "        test_expected_data = test_example.output # needed for function signature\n",
    "\n",
    "        user_query = test_output_data.get('user_query', 'MISSING')\n",
    "        final_answer = test_output_data.get('final_answer', 'MISSING')\n",
    "        human_label_raw = test_example.output.get(HUMAN_LABEL_KEY_ANSWER, \"MISSING_KEY\")\n",
    "\n",
    "        # --- Call LLM judge directly for raw output ---\n",
    "        raw_judge_response_answer = \"Skipped direct call\" # Default\n",
    "        if user_query != 'MISSING' and final_answer != 'MISSING':\n",
    "            try:\n",
    "                test_prompt_answer = FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE.format(\n",
    "                    user_query=user_query,\n",
    "                    final_answer=final_answer\n",
    "                )\n",
    "                raw_judge_response_answer = call_openai_judge(test_prompt_answer, model=MODEL_TO_USE)\n",
    "            except Exception as e:\n",
    "                # print(f\"  Error calling LLM Judge directly on Example {i}: {e}\") # Verbose error\n",
    "                raw_judge_response_answer = f\"Error during direct call: {e}\"\n",
    "        # else:\n",
    "             # print(f\"  Skipping direct LLM call on Example {i} due to missing query/answer.\")\n",
    "\n",
    "\n",
    "        # --- Call the evaluator function ---\n",
    "        test_score_float_answer = 0.0 # Default score\n",
    "        try:\n",
    "            test_score_float_answer = evaluate_final_answer_quality_direct_api(test_output_data, test_expected_data)\n",
    "        except Exception as e:\n",
    "            # print(f\"  Error calling evaluator function on Example {i}: {e}\") # Verbose error\n",
    "            test_score_float_answer = 0.0 # Assign 0.0 on error\n",
    "\n",
    "        # --- Append results to list ---\n",
    "        answer_results_list.append({\n",
    "            \"index\": i,\n",
    "            \"user_query\": user_query,\n",
    "            \"final_answer\": final_answer,\n",
    "            \"human_answer_label_raw\": human_label_raw, # Store the original human label\n",
    "            \"raw_llm_response_answer\": raw_judge_response_answer,\n",
    "            \"final_answer_score\": test_score_float_answer\n",
    "        })\n",
    "\n",
    "    # --- Convert list to DataFrame ---\n",
    "    print(\"\\nEvaluation loop complete. Creating DataFrame...\")\n",
    "    answer_results_df = pd.DataFrame(answer_results_list)\n",
    "\n",
    "    # --- Add Comparison Columns ---\n",
    "    print(\"Adding comparison columns...\")\n",
    "    # Map human label \"Fail\" to \"Bad\" for comparison\n",
    "    answer_results_df['human_answer_label_mapped'] = answer_results_df['human_answer_label_raw'].map(HUMAN_LABEL_MAP).fillna(answer_results_df['human_answer_label_raw'])\n",
    "\n",
    "    # Convert score (1.0/0.0) to LLM label (\"Good\"/\"Bad\")\n",
    "    def answer_score_to_label(score):\n",
    "        return LLM_POSITIVE_LABEL if score == 1.0 else \"Bad\" # Assumes 0.0 means Bad\n",
    "\n",
    "    answer_results_df['llm_answer_label'] = answer_results_df['final_answer_score'].apply(answer_score_to_label)\n",
    "\n",
    "    # Compare mapped human label with LLM label (case-insensitive)\n",
    "    answer_results_df['answer_match'] = answer_results_df.apply(\n",
    "        lambda row: str(row['llm_answer_label']).lower() == str(row['human_answer_label_mapped']).lower()\n",
    "                    if row['human_answer_label_mapped'] not in [\"MISSING\", \"MISSING_KEY\"] else None,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Display DataFrame ---\n",
    "    print(\"\\nFinal Answer Quality Evaluation Results:\")\n",
    "    # Select and reorder columns\n",
    "    display_cols_answer = ['index', 'user_query', 'final_answer', 'human_answer_label_raw', 'llm_answer_label', 'answer_match', 'raw_llm_response_answer']\n",
    "    display_cols_answer = [col for col in display_cols_answer if col in answer_results_df.columns] # Ensure columns exist\n",
    "\n",
    "    pd.set_option('display.max_rows', 100)\n",
    "    pd.set_option('display.max_colwidth', 200)\n",
    "    display(answer_results_df[display_cols_answer])\n",
    "\n",
    "    # --- Calculate Match Percentage ---\n",
    "    if 'answer_match' in answer_results_df.columns:\n",
    "        match_count_answer = answer_results_df['answer_match'].sum() # Counts True values\n",
    "        valid_comparisons_answer = answer_results_df['answer_match'].notna().sum() # Counts non-None values\n",
    "        if valid_comparisons_answer > 0:\n",
    "            match_percentage_answer = (match_count_answer / valid_comparisons_answer) * 100\n",
    "            print(f\"\\nAgreement between LLM Judge and Human Labels (Final Answer): {match_percentage_answer:.2f}% ({match_count_answer}/{valid_comparisons_answer})\")\n",
    "        else:\n",
    "            print(\"\\nCould not calculate agreement percentage (no valid comparisons).\")\n",
    "\n",
    "else:\n",
    "    print(\"--- Final Answer Evaluation Aborted due to failed prerequisites ---\")\n",
    "\n",
    "print(f\"\\n--- End Final Answer Quality Evaluation ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Answer Quality Evaluation & Alignment Challenge\n",
    "\n",
    "We successfully ran the `evaluate_final_answer_quality_direct_api` function across the dataset and compared its judgments to the human labels.\n",
    "\n",
    "**Key Result:**\n",
    "\n",
    "*   **Agreement with Human Labels:** ~18% (3/17)\n",
    "\n",
    "**Analysis:**\n",
    "\n",
    "The LLM judge achieved very low agreement with the human assessment for final answer quality. Examining the results table reveals the likely cause:\n",
    "*   The LLM judge, following our prompt to evaluate *only* based on the query and answer text (ignoring SQL/data correctness), rated almost all answers as \"Good\".\n",
    "*   The human labels (`Pass`/`Fail`/`Correct`), however, likely incorporated factual correctness based on the underlying data, resulting in many \"Fail\" labels.\n",
    "*   This fundamental mismatch in evaluation criteria led to the significant disagreement.\n",
    "\n",
    "**Pedagogical Opportunity:**\n",
    "\n",
    "This outcome serves as an excellent illustration of the challenges in automated evaluation:\n",
    "*   **Technical Success vs. Meaningful Results:** We successfully built and ran the evaluator, but the results lack strong alignment with human judgment in this case.\n",
    "*   **Importance of Criteria & Prompting:** It highlights how critical defining the *right* evaluation criteria and crafting effective prompts is. Simply asking if an answer is \"Good\" based on text alone was insufficient here.\n",
    "*   **Improvement Task:** This presents a clear opportunity for improvement. Students could be tasked with refining the `FINAL_ANSWER_QUALITY_PROMPT_TEMPLATE` to encourage the LLM to consider factual accuracy (perhaps by providing the generated SQL or context), or exploring different evaluation scales beyond simple \"Good/Bad\", aiming to increase alignment with human judgment.\n",
    "\n",
    "This demonstrates that building LLM-as-judge systems requires not just coding but careful consideration of evaluation design and alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Experiment via Phoenix\n",
    "\n",
    "Now that all three evaluator functions (`evaluate_tool_usage_direct_api`, `evaluate_sql_correctness_direct_api`, `evaluate_final_answer_quality_direct_api`) have been defined using the direct OpenAI API call pattern and verified, we will execute the full evaluation harness using `phoenix.experiments.run_experiment`.\n",
    "\n",
    "This function will:\n",
    "1.  Iterate through each example in the `evaluation_dataset`.\n",
    "2.  Run the `dummy_task_function` for each example (simply passing through the pre-computed agent outputs).\n",
    "3.  Call each of our three defined evaluator functions for every example.\n",
    "4.  Log the inputs, outputs, human labels, and the scores from all three evaluators to the Phoenix/Arize platform under a timestamped experiment name.\n",
    "\n",
    "The results, including aggregate scores for each evaluator, will be summarized below the cell upon completion, and the full details can be explored in the linked Phoenix UI. This provides a centralized record of the agent's performance across all evaluation criteria for this version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing to run Phoenix experiment with all OpenAI evaluators ---\n",
      "Created list 'all_final_evaluators' with 3 functions.\n",
      "\n",
      "Running Phoenix experiment...\n",
      "🧪 Experiment started.\n",
      "📺 View dataset experiments: https://app.phoenix.arize.com/datasets/RGF0YXNldDo0/experiments\n",
      "🔗 View this experiment: https://app.phoenix.arize.com/datasets/RGF0YXNldDo0/compare?experimentId=RXhwZXJpbWVudDoxMA==\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c63bcc78cf44af8e0f31c5b2e74477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/17 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task runs completed.\n",
      "🧠 Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aed0f7284514188a8ed386d20adf78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/51 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 View this experiment: https://app.phoenix.arize.com/datasets/RGF0YXNldDo0/compare?experimentId=RXhwZXJpbWVudDoxMA==\n",
      "\n",
      "Experiment Summary (04/28/25 09:41 PM +1000)\n",
      "--------------------------------------------\n",
      "                                  evaluator   n  n_scores  avg_score\n",
      "0  evaluate_final_answer_quality_direct_api  17        17   0.764706\n",
      "1       evaluate_sql_correctness_direct_api  17        17   0.882353\n",
      "2            evaluate_tool_usage_direct_api  17        17   1.000000\n",
      "\n",
      "Tasks Summary (04/28/25 09:39 PM +1000)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0          17      17         0\n",
      "\n",
      "Experiment 'Full_OpenAI_Eval_20250428-213854' run initiated.\n",
      "Check the Phoenix UI for detailed results and scores from all evaluators.\n"
     ]
    }
   ],
   "source": [
    "# --- Run Phoenix Experiment with All Corrected OpenAI Evaluators ---\n",
    "import phoenix as px # Ensure phoenix is imported\n",
    "from phoenix.experiments import run_experiment\n",
    "from datetime import datetime\n",
    "# Assumes 'evaluation_dataset' is loaded\n",
    "# Assumes 'dummy_task_function' is defined (or define it here)\n",
    "# Assumes the three evaluator functions are defined:\n",
    "#   - evaluate_tool_usage_direct_api\n",
    "#   - evaluate_sql_correctness_direct_api\n",
    "#   - evaluate_final_answer_quality_direct_api\n",
    "\n",
    "print(\"\\n--- Preparing to run Phoenix experiment with all OpenAI evaluators ---\")\n",
    "\n",
    "# --- Combine Evaluators ---\n",
    "# Ensure the function names below match exactly how they were defined\n",
    "try:\n",
    "    all_final_evaluators = [\n",
    "        evaluate_tool_usage_direct_api,\n",
    "        evaluate_sql_correctness_direct_api,\n",
    "        evaluate_final_answer_quality_direct_api\n",
    "    ]\n",
    "    print(f\"Created list 'all_final_evaluators' with {len(all_final_evaluators)} functions.\")\n",
    "except NameError as e:\n",
    "    print(f\"Error: One or more evaluator functions not defined: {e}\")\n",
    "    all_final_evaluators = None # Prevent running experiment if list creation failed\n",
    "\n",
    "# --- Define Dummy Task Function (if not already defined) ---\n",
    "# This function simply passes the input data through, as the agent results are pre-computed in the dataset.\n",
    "if 'dummy_task_function' not in locals():\n",
    "    print(\"Defining dummy_task_function...\")\n",
    "    def dummy_task_function(example):\n",
    "        \"\"\"Takes an example and returns its input field.\"\"\"\n",
    "        # Ensure it returns a dictionary suitable for the evaluators\n",
    "        return example.input if hasattr(example, 'input') else {}\n",
    "    print(\"dummy_task_function defined.\")\n",
    "\n",
    "\n",
    "# --- Prerequisite Check for Run ---\n",
    "run_passed = True\n",
    "if 'evaluation_dataset' not in locals() or len(evaluation_dataset) == 0:\n",
    "    print(\"Run FAIL: evaluation_dataset not found or empty.\")\n",
    "    run_passed = False\n",
    "if 'dummy_task_function' not in locals():\n",
    "     print(\"Run FAIL: dummy_task_function not defined.\")\n",
    "     run_passed = False\n",
    "if not all_final_evaluators or len(all_final_evaluators) != 3:\n",
    "     print(\"Run FAIL: Evaluator list 'all_final_evaluators' not ready.\")\n",
    "     run_passed = False\n",
    "# --- End Prerequisite Check ---\n",
    "\n",
    "\n",
    "if run_passed:\n",
    "    print(\"\\nRunning Phoenix experiment...\")\n",
    "    now_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    experiment_name = f\"Full_OpenAI_Eval_{now_str}\"\n",
    "\n",
    "    # Call run_experiment with positional dataset/task_fn and keyword evaluators\n",
    "    try:\n",
    "        experiment_run = run_experiment(\n",
    "            evaluation_dataset,         # 1st Positional: Dataset\n",
    "            dummy_task_function,        # 2nd Positional: Task Function\n",
    "            evaluators=all_final_evaluators, # Keyword: List of evaluator functions\n",
    "            experiment_name=experiment_name,\n",
    "            experiment_description=\"Full evaluation using direct OpenAI calls for ToolUsage, SQLCorrectness, FinalAnswer.\"\n",
    "            # concurrency=... # Optional: Adjust concurrency if needed\n",
    "        )\n",
    "        print(f\"\\nExperiment '{experiment_name}' run initiated.\")\n",
    "        print(\"Check the Phoenix UI for detailed results and scores from all evaluators.\")\n",
    "        # The experiment summary table will print automatically below if successful.\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during run_experiment: {e}\")\n",
    "        print(\"Please check function signatures, dataset structure, and Phoenix connection.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- Experiment Run Aborted due to failed prerequisites ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Running the Agent with a Different Model (`gpt-4o`)\n",
    "\n",
    "Having established a baseline run and evaluation using the **`gpt-4o-mini`** model, our next step is to run an experiment comparing its performance against a different model, **`gpt-4o`**.\n",
    "\n",
    "The process involves:\n",
    "1.  Modifying the `MODEL` constant in `src/agent/agent.py` to `\"gpt-4o\"`.\n",
    "2.  Re-running the agent script (`python src/agent/agent.py`), which executes the predefined test queries using the new model.\n",
    "3.  Crucially, ensuring the OpenTelemetry traces for this experimental run are captured in Arize Phoenix so we can directly compare latency, token usage, and potentially tool calls between the `gpt-4o-mini` (baseline) and `gpt-4o` runs within the Phoenix UI.\n",
    "\n",
    "#### Challenge: Capturing Traces for the Experimental Run (`gpt-4o`)\n",
    "\n",
    "When we attempted to re-run `src/agent/agent.py` (after changing the model to `gpt-4o`), we unexpectedly hit significant issues with the Arize Phoenix tracing configuration. Even though the tracing setup code and the `.env` file *hadn't changed* since the previous successful baseline run (or seemed straightforward based on initial setup), the script failed to connect to the Phoenix cloud endpoint (`https://app.phoenix.arize.com`).\n",
    "\n",
    "**Debugging the Connection Issues:**\n",
    "\n",
    "*   **Defaulting to Localhost:** The primary issue was that the `phoenix.otel.register()` function, despite having environment variables set (either standard `OTEL_...` or `PHOENIX_...`), consistently ignored the cloud endpoint and attempted to connect via **gRPC** to `localhost:4317`, resulting in `StatusCode.UNAVAILABLE` errors.\n",
    "*   **Unreliable Auto-Detection:** Attempts to guide the automatic detection by forcing the protocol (`http/protobuf`) or using specific environment variable names recommended in documentation (`PHOENIX_COLLECTOR_ENDPOINT`, `PHOENIX_CLIENT_HEADERS`) were unsuccessful in making the library use the correct hostname from the environment variables. It kept defaulting to `localhost` (either port 4317 for gRPC or 6006 for HTTP).\n",
    "\n",
    "**Working Solution: Explicit Configuration:**\n",
    "\n",
    "The only reliable way to ensure traces were sent correctly to `https://app.phoenix.arize.com` for this experimental run was to **bypass the automatic environment variable detection entirely within the `phoenix.otel.register()` function.**\n",
    "\n",
    "This required modifying `src/agent/agent.py` for the tracing setup block:\n",
    "1.  Ensure the `.env` file contains the correct header variable as specified in the Phoenix cloud documentation: `PHOENIX_CLIENT_HEADERS=\"api_key=YOUR_KEY_VALUE\"`.\n",
    "2.  Update the Python code to:\n",
    "    *   Explicitly define the full endpoint URL: `endpoint = \"https://app.phoenix.arize.com/v1/traces\"`\n",
    "    *   Read the `PHOENIX_CLIENT_HEADERS` environment variable using `os.getenv()`.\n",
    "    *   Parse the header string into the required dictionary format: `headers_dict = {\"api_key\": \"YOUR_KEY_VALUE\"}`.\n",
    "    *   Pass these directly to the registration function:\n",
    "        ```python\n",
    "        phoenix_tracer_provider = register(\n",
    "            project_name=PROJECT_NAME,\n",
    "            endpoint=endpoint,\n",
    "            headers=headers_dict\n",
    "        )\n",
    "        ```\n",
    "\n",
    "**Outcome:**\n",
    "\n",
    "With this explicit configuration hardcoded in the script, the agent successfully connected to Phoenix, and the traces for our `gpt-4o` experimental run were captured. This allows us to proceed with comparing the two models (`gpt-4o-mini` vs `gpt-4o`) within the Phoenix UI. This troubleshooting detour highlights potential fragility in automatic OTel configuration detection and underscores the utility of explicit configuration when encountering connection problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to load dataset 'Experiment_GPT4o_AllSpans'...\n",
      "Dataset loaded successfully.\n",
      "Number of examples in new dataset 'Experiment_GPT4o_AllSpans': 125\n",
      "\n",
      "--- Inspecting First 3 Examples from 'Experiment_GPT4o_AllSpans' ---\n",
      "\n",
      "--- Example 1 ---\n",
      "\n",
      "Input Data:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"\\n        You are a helpful assistant designed to answer questions about the LearnAIWithAI Workshop 1 transcript. \\n        Use the available tools to query the transcript database when necessary. \\n        The database table is ''transcript_segments'' and contains segments of the transcript.\\n        Base your answers SOLELY on the information retrieved from the database using the tools. \\n        If the information is not found in the database, say that you cannot answer the question based on the available transcript data.\\n        Be concise and directly answer the user's query based on the tool results.\\n        \"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Who is Jeff Pidcock?\"\n",
      "    }\n",
      "  ],\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"query_database\",\n",
      "        \"description\": \"Executes a read-only SQL query against a database containing transcript segments from Workshop 1. Use this to find specific information mentioned in the workshop transcript.\",\n",
      "        \"parameters\": {\n",
      "          \"type\": \"object\",\n",
      "          \"properties\": {\n",
      "            \"sql_query\": {\n",
      "              \"type\": \"string\",\n",
      "              \"description\": \"A valid SQLite SELECT query targeting the 'transcript_segments' table. The table schema is: (segment_id INTEGER PRIMARY KEY, session_name TEXT, start_time_seconds REAL, end_time_seconds REAL, speaker TEXT, text TEXT, word_count INTEGER). Filter based on user request (e.g., time, speaker, keywords in text). Query only the columns you need.\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"sql_query\"\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Output/Label Data:\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"function\": {\n",
      "            \"name\": \"query_database\",\n",
      "            \"arguments\": \"{\\\"sql_query\\\":\\\"SELECT speaker, text FROM transcript_segments WHERE speaker = 'Jeff Pidcock' LIMIT 1;\\\"}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Metadata:\n",
      "{\n",
      "  \"span_kind\": \"LLM\"\n",
      "}\n",
      "\n",
      "--- Example 2 ---\n",
      "\n",
      "Input Data:\n",
      "{}\n",
      "\n",
      "Output/Label Data:\n",
      "{}\n",
      "\n",
      "Metadata:\n",
      "{\n",
      "  \"span_kind\": \"UNKNOWN\"\n",
      "}\n",
      "\n",
      "--- Example 3 ---\n",
      "\n",
      "Input Data:\n",
      "{}\n",
      "\n",
      "Output/Label Data:\n",
      "{}\n",
      "\n",
      "Metadata:\n",
      "{\n",
      "  \"span_kind\": \"UNKNOWN\"\n",
      "}\n",
      "\n",
      "--- Finished Loading and Inspecting GPT-4o Dataset (Experiment_GPT4o_AllSpans) ---\n"
     ]
    }
   ],
   "source": [
    "# Cell to Load GPT-4o Dataset\n",
    "\n",
    "import json # Make sure json is imported\n",
    "\n",
    "# This is the name we decided on for the dataset created in the UI.\n",
    "# Make sure this exactly matches the name in your Phoenix UI.\n",
    "new_dataset_name = \"Experiment_GPT4o_AllSpans\"\n",
    "\n",
    "print(f\"\\nAttempting to load dataset '{new_dataset_name}'...\")\n",
    "\n",
    "# Check if client was initialized successfully in the setup cell\n",
    "if 'px_client' is None or 'px_client' not in locals():\n",
    "    raise NameError(\"Phoenix client 'px_client' was not initialized successfully. Please re-run the modified Setup Cell (#1).\")\n",
    "\n",
    "# Load the specified dataset by its exact name into a NEW variable\n",
    "evaluation_dataset_gpt4o = px_client.get_dataset(name=new_dataset_name)\n",
    "print(\"Dataset loaded successfully.\")\n",
    "\n",
    "# Print number of examples\n",
    "print(f\"Number of examples in new dataset '{new_dataset_name}': {len(evaluation_dataset_gpt4o)}\")\n",
    "\n",
    "# --- Inspect the first few examples ---\n",
    "num_examples_to_show = 3 # Adjust if you want to see more/less\n",
    "print(f\"\\n--- Inspecting First {num_examples_to_show} Examples from '{new_dataset_name}' ---\")\n",
    "\n",
    "if len(evaluation_dataset_gpt4o) > 0:\n",
    "    for i, example in enumerate(evaluation_dataset_gpt4o[:num_examples_to_show]):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        print(\"\\nInput Data:\")\n",
    "        try:\n",
    "            print(json.dumps(example.input, indent=2))\n",
    "        except Exception as e:\n",
    "            print(f\"Could not display input: {e}\")\n",
    "        print(\"\\nOutput/Label Data:\") # Check if labels are present as expected\n",
    "        try:\n",
    "            print(json.dumps(example.output, indent=2))\n",
    "        except Exception as e:\n",
    "            print(f\"Could not display output/labels: {e}\")\n",
    "        print(\"\\nMetadata:\")\n",
    "        try:\n",
    "            print(json.dumps(example.metadata, indent=2))\n",
    "        except Exception as e:\n",
    "            print(f\"Could not display metadata: {e}\")\n",
    "else:\n",
    "    print(f\"Dataset '{new_dataset_name}' appears to be empty.\")\n",
    "\n",
    "print(f\"\\n--- Finished Loading and Inspecting GPT-4o Dataset ({new_dataset_name}) ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing Phoenix Client Explicitly for Cloud ---\n",
      "Found headers: Key='api_key'\n",
      "Attempting px.Client(endpoint='https://app.phoenix.arize.com', headers=...)\n",
      "Phoenix client initialized successfully using explicit arguments.\n",
      "--- Client Initialization Complete ---\n"
     ]
    }
   ],
   "source": [
    "    # Cell 1: Initialize Phoenix Client EXPLICITLY for Cloud\n",
    "\n",
    "    import phoenix as px\n",
    "    import os\n",
    "    import json\n",
    "\n",
    "    print(\"--- Initializing Phoenix Client Explicitly for Cloud ---\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    cloud_api_endpoint = \"https://app.phoenix.arize.com\"\n",
    "    api_headers_str = os.getenv(\"PHOENIX_CLIENT_HEADERS\") # Read from environment\n",
    "\n",
    "    if not api_headers_str:\n",
    "        raise ValueError(\"CRITICAL: PHOENIX_CLIENT_HEADERS environment variable not found.\")\n",
    "\n",
    "    api_headers_dict = {}\n",
    "    try:\n",
    "        key, value = api_headers_str.split('=', 1)\n",
    "        api_headers_dict[key.strip()] = value.strip()\n",
    "        if not api_headers_dict: raise ValueError(\"Parsed is empty.\")\n",
    "        print(f\"Found headers: Key='{list(api_headers_dict.keys())[0]}'\")\n",
    "    except Exception as parse_err:\n",
    "        raise ValueError(f\"Invalid PHOENIX_CLIENT_HEADERS format: '{api_headers_str}'. Expected 'key=value'. Error: {parse_err}\") from parse_err\n",
    "    # --- End Configuration ---\n",
    "\n",
    "    # --- Initialize Client Explicitly ---\n",
    "    try:\n",
    "        print(f\"Attempting px.Client(endpoint='{cloud_api_endpoint}', headers=...)\")\n",
    "        # Use explicit endpoint and headers\n",
    "        px_client = px.Client(endpoint=cloud_api_endpoint, headers=api_headers_dict)\n",
    "        print(\"Phoenix client initialized successfully using explicit arguments.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR initializing Phoenix Client explicitly: {e}\")\n",
    "        px_client = None\n",
    "    # --- End Initialization ---\n",
    "\n",
    "    print(\"--- Client Initialization Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Combined Data for Evaluation\n",
    "\n",
    "We now have two datasets loaded:\n",
    "\n",
    "1.  `evaluation_dataset`: Contains the results from the baseline (`gpt-4o-mini`) run. Crucially, this dataset was potentially created from examples where feedback was provided in the Phoenix UI, meaning it might not contain *all* examples from the original run and its `.output` field contains the ground truth labels and explanations derived from that UI feedback.\n",
    "2.  `evaluation_dataset_gpt4o`: Contains the results from the experimental (`gpt-4o`) run, loaded directly from the trace data. Its `.input` field contains the agent's outputs for this run (e.g., `final_answer`, `generated_sql`), but its `.output` field likely lacks the ground truth labels.\n",
    "\n",
    "Our LLM-as-a-Judge evaluators (`all_custom_evaluators`) need both the agent's output (from the `gpt-4o` run) and the corresponding ground truth labels (from the baseline dataset).\n",
    "\n",
    "Therefore, the next step is to **combine** these two datasets within the notebook. We will iterate through the examples from the `gpt-4o` run (`evaluation_dataset_gpt4o`) and, using the `user_query` as a key, attempt to find the matching example with ground truth labels in the baseline dataset (`evaluation_dataset`).\n",
    "\n",
    "We will create a new list, `combined_eval_data`, containing only the examples where a match was found. Each item in this list will have:\n",
    "*   The `user_query`.\n",
    "*   The outputs generated by the `gpt-4o` model (`gpt4o_output`).\n",
    "*   The ground truth labels and explanations (`ground_truth`) from the baseline dataset.\n",
    "\n",
    "This `combined_eval_data` list will be the input for our evaluation process in the subsequent steps. We will also report how many examples from the `gpt-4o` run could be matched and how many were skipped due to missing labels in the baseline dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing Datasets and Preparing Combined Data ---\n",
      "\n",
      "Extracted 17 labeled examples from baseline dataset ('evaluation_dataset').\n",
      "\n",
      "Extracted 0 examples from experimental dataset ('evaluation_dataset_gpt4o').\n",
      "\n",
      "Combining data...\n",
      "\n",
      "Successfully combined data for 0 examples.\n",
      "\n",
      "No combined data was prepared. Check dataset alignment or content.\n",
      "\n",
      "--- Data Preparation Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell: Compare Datasets and Prepare Combined Data for Evaluation\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"\\n--- Comparing Datasets and Preparing Combined Data ---\")\n",
    "\n",
    "# Ensure both datasets are loaded\n",
    "if 'evaluation_dataset' not in locals() or 'evaluation_dataset_gpt4o' not in locals():\n",
    "    raise NameError(\"One or both datasets ('evaluation_dataset', 'evaluation_dataset_gpt4o') are not loaded.\")\n",
    "\n",
    "# --- Extract Baseline Labels (keyed by user_query) ---\n",
    "baseline_labels = {}\n",
    "for example in evaluation_dataset:\n",
    "    try:\n",
    "        # Assuming 'user_query' is directly in the input field\n",
    "        query = example.input.get(\"user_query\")\n",
    "        if query and example.output: # Check if query exists and there's label data\n",
    "            baseline_labels[query] = example.output\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not process baseline example: {e}\")\n",
    "\n",
    "print(f\"\\nExtracted {len(baseline_labels)} labeled examples from baseline dataset ('evaluation_dataset').\")\n",
    "if len(baseline_labels) < len(evaluation_dataset):\n",
    "     print(f\"  (Note: Some baseline examples might have been skipped if missing 'user_query' or 'output' field).\")\n",
    "\n",
    "\n",
    "# --- Extract Experimental Results (keyed by user_query) ---\n",
    "experimental_results = {}\n",
    "for example in evaluation_dataset_gpt4o:\n",
    "     try:\n",
    "        # Assuming 'user_query' is directly in the input field\n",
    "        query = example.input.get(\"user_query\")\n",
    "        if query:\n",
    "            # Store the whole input dict containing gpt-4o's answers/SQL etc.\n",
    "            experimental_results[query] = example.input\n",
    "     except Exception as e:\n",
    "        print(f\"Warning: Could not process experimental example: {e}\")\n",
    "\n",
    "print(f\"\\nExtracted {len(experimental_results)} examples from experimental dataset ('evaluation_dataset_gpt4o').\")\n",
    "\n",
    "# --- Create Combined Data for Evaluation ---\n",
    "combined_eval_data = []\n",
    "missing_labels_count = 0\n",
    "found_labels_count = 0\n",
    "\n",
    "print(\"\\nCombining data...\")\n",
    "for query, gpt4o_result in experimental_results.items():\n",
    "    if query in baseline_labels:\n",
    "        # Found corresponding labels in the baseline dataset\n",
    "        combined_item = {\n",
    "            \"user_query\": query,\n",
    "            \"gpt4o_output\": gpt4o_result, # Contains final_answer, generated_sql etc. from gpt-4o run\n",
    "            \"ground_truth\": baseline_labels[query] # Contains labels and explanations\n",
    "        }\n",
    "        combined_eval_data.append(combined_item)\n",
    "        found_labels_count += 1\n",
    "    else:\n",
    "        # No labels found for this query in the baseline dataset\n",
    "        # print(f\"  Skipping query (no labels found): {query[:80]}...\") # Uncomment to see skipped queries\n",
    "        missing_labels_count += 1\n",
    "\n",
    "print(f\"\\nSuccessfully combined data for {found_labels_count} examples.\")\n",
    "if missing_labels_count > 0:\n",
    "    print(f\"Skipped {missing_labels_count} examples from the gpt-4o run because corresponding labels were not found in the baseline dataset.\")\n",
    "\n",
    "# --- Inspect the first combined item ---\n",
    "if combined_eval_data:\n",
    "    print(\"\\n--- First Combined Example ---\")\n",
    "    print(json.dumps(combined_eval_data[0], indent=2))\n",
    "else:\n",
    "    print(\"\\nNo combined data was prepared. Check dataset alignment or content.\")\n",
    "\n",
    "print(\"\\n--- Data Preparation Complete ---\")\n",
    "# The variable 'combined_eval_data' now holds the data ready for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inspecting Structure of First Example from 'evaluation_dataset_gpt4o' ---\n",
      "\n",
      "--- first_exp_example.input ---\n",
      "Keys: ['messages', 'tools']\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"system\",\n",
      "      \"content\": \"\\n        You are a helpful assistant designed to answer questions about the LearnAIWithAI Workshop 1 transcript. \\n        Use the available tools to query the transcript database when necessary. \\n        The database table is ''transcript_segments'' and contains segments of the transcript.\\n        Base your answers SOLELY on the information retrieved from the database using the tools. \\n        If the information is not found in the database, say that you cannot answer the question based on the available transcript data.\\n        Be concise and directly answer the user's query based on the tool results.\\n        \"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Who is Jeff Pidcock?\"\n",
      "    }\n",
      "  ],\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"type\": \"function\",\n",
      "      \"function\": {\n",
      "        \"name\": \"query_database\",\n",
      "        \"description\": \"Executes a read-only SQL query against a database containing transcript segments from Workshop 1. Use this to find specific information mentioned in the workshop transcript.\",\n",
      "        \"parameters\": {\n",
      "          \"type\": \"object\",\n",
      "          \"properties\": {\n",
      "            \"sql_query\": {\n",
      "              \"type\": \"string\",\n",
      "              \"description\": \"A valid SQLite SELECT query targeting the 'transcript_segments' table. The table schema is: (segment_id INTEGER PRIMARY KEY, session_name TEXT, start_time_seconds REAL, end_time_seconds REAL, speaker TEXT, text TEXT, word_count INTEGER). Filter based on user request (e.g., time, speaker, keywords in text). Query only the columns you need.\"\n",
      "            }\n",
      "          },\n",
      "          \"required\": [\n",
      "            \"sql_query\"\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "--- first_exp_example.output ---\n",
      "Keys: ['messages']\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"tool_calls\": [\n",
      "        {\n",
      "          \"function\": {\n",
      "            \"name\": \"query_database\",\n",
      "            \"arguments\": \"{\\\"sql_query\\\":\\\"SELECT speaker, text FROM transcript_segments WHERE speaker = 'Jeff Pidcock' LIMIT 1;\\\"}\"\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "--- first_exp_example.metadata ---\n",
      "Keys: ['span_kind']\n",
      "{\n",
      "  \"span_kind\": \"LLM\"\n",
      "}\n",
      "\n",
      "--- Inspection Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell to Inspect Structure of Experimental Dataset Example\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"\\n--- Inspecting Structure of First Example from 'evaluation_dataset_gpt4o' ---\")\n",
    "\n",
    "if 'evaluation_dataset_gpt4o' in locals() and len(evaluation_dataset_gpt4o) > 0:\n",
    "    first_exp_example = evaluation_dataset_gpt4o[0]\n",
    "\n",
    "    print(\"\\n--- first_exp_example.input ---\")\n",
    "    try:\n",
    "        # See what keys are actually inside 'input'\n",
    "        print(f\"Keys: {list(first_exp_example.input.keys())}\")\n",
    "        print(json.dumps(first_exp_example.input, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display input: {e}\")\n",
    "\n",
    "    print(\"\\n--- first_exp_example.output ---\")\n",
    "    try:\n",
    "        # Check if 'output' contains anything useful (might be None or empty)\n",
    "        print(f\"Keys: {list(first_exp_example.output.keys())}\")\n",
    "        print(json.dumps(first_exp_example.output, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display output: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- first_exp_example.metadata ---\")\n",
    "    try:\n",
    "         # Check if metadata holds the query\n",
    "        print(f\"Keys: {list(first_exp_example.metadata.keys())}\")\n",
    "        print(json.dumps(first_exp_example.metadata, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not display metadata: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Variable 'evaluation_dataset_gpt4o' not found or is empty.\")\n",
    "\n",
    "print(\"\\n--- Inspection Complete ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing Datasets and Preparing Combined Data (Corrected Extraction) ---\n",
      "\n",
      "Extracted 17 labeled examples from baseline dataset ('evaluation_dataset').\n",
      "\n",
      "Extracted 19 examples from experimental dataset ('evaluation_dataset_gpt4o').\n",
      "\n",
      "Combining data...\n",
      "\n",
      "Successfully combined data for 17 examples.\n",
      "Skipped 2 examples from the gpt-4o run because corresponding labels were not found in the baseline dataset.\n",
      "\n",
      "--- First Combined Example Structure---\n",
      "Keys: ['user_query', 'gpt4o_output_for_evaluator', 'ground_truth']\n",
      "  user_query: Who is Jeff Pidcock?...\n",
      "  gpt4o_output_for_evaluator keys: ['messages']\n",
      "  ground_truth keys: ['tool_usage_explanation', 'sql_correctness_label', 'tool_usage_correctness_label', 'final_answer_quality_label', 'sql_correctness_explanation', 'final_answer_explanation']\n",
      "\n",
      "--- Data Preparation Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Cell: Compare Datasets and Prepare Combined Data for Evaluation (Corrected Extraction)\n",
    "\n",
    "import json\n",
    "\n",
    "print(\"\\n--- Comparing Datasets and Preparing Combined Data (Corrected Extraction) ---\")\n",
    "\n",
    "# Ensure both datasets are loaded\n",
    "if 'evaluation_dataset' not in locals() or 'evaluation_dataset_gpt4o' not in locals():\n",
    "    raise NameError(\"One or both datasets ('evaluation_dataset', 'evaluation_dataset_gpt4o') are not loaded.\")\n",
    "\n",
    "# Helper function to safely extract user query from messages list\n",
    "def get_user_query(messages_list):\n",
    "    if not isinstance(messages_list, list):\n",
    "        return None\n",
    "    for msg in messages_list:\n",
    "        if isinstance(msg, dict) and msg.get(\"role\") == \"user\" and \"content\" in msg:\n",
    "            return msg[\"content\"]\n",
    "    return None\n",
    "\n",
    "# --- Extract Baseline Labels (keyed by user_query) ---\n",
    "# Assumes baseline dataset structure also has query in input (potentially different structure)\n",
    "# If baseline also failed before, this needs adjustment too. Let's assume it worked.\n",
    "baseline_labels = {}\n",
    "for example in evaluation_dataset:\n",
    "    try:\n",
    "        # --- !! ADJUST THIS IF BASELINE STRUCTURE IS DIFFERENT !! ---\n",
    "        query = example.input.get(\"user_query\") # Assuming baseline WAS flat structure\n",
    "        # If baseline also had nested structure, use:\n",
    "        # query = get_user_query(example.input.get(\"messages\"))\n",
    "        # --- !! ---------------------------------------------- !! ---\n",
    "\n",
    "        if query and example.output: # Check if query exists and there's label data\n",
    "            baseline_labels[query] = example.output\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not process baseline example: {e}\")\n",
    "\n",
    "print(f\"\\nExtracted {len(baseline_labels)} labeled examples from baseline dataset ('evaluation_dataset').\")\n",
    "\n",
    "\n",
    "# --- Extract Experimental Results (keyed by user_query) ---\n",
    "# We now know the structure for the experimental dataset\n",
    "experimental_results = {}\n",
    "for example in evaluation_dataset_gpt4o:\n",
    "     try:\n",
    "        # Extract query using the helper function for the known nested structure\n",
    "        query = get_user_query(example.input.get(\"messages\"))\n",
    "        if query:\n",
    "            # Need to find the agent's actual output (final_answer, generated_sql)\n",
    "            # This likely comes from the processing done in parse_spans.ipynb or similar step\n",
    "            # Let's ASSUME the important outputs were copied into the .input field\n",
    "            # during dataset creation, similar to the baseline structure for simplicity.\n",
    "            # If not, we need to figure out where the final_answer etc. are stored for gpt4o run.\n",
    "            # For now, let's just store the whole input dict.\n",
    "            experimental_results[query] = example.input # Storing the raw input for now\n",
    "     except Exception as e:\n",
    "        print(f\"Warning: Could not process experimental example: {e}\")\n",
    "\n",
    "print(f\"\\nExtracted {len(experimental_results)} examples from experimental dataset ('evaluation_dataset_gpt4o').\")\n",
    "if len(experimental_results) == 0:\n",
    "     print(\"ERROR: Failed to extract any results from the experimental dataset. Check extraction logic.\")\n",
    "\n",
    "\n",
    "# --- Create Combined Data for Evaluation ---\n",
    "combined_eval_data = []\n",
    "missing_labels_count = 0\n",
    "found_labels_count = 0\n",
    "\n",
    "print(\"\\nCombining data...\")\n",
    "if len(experimental_results) > 0:\n",
    "    for query, gpt4o_exp_input in experimental_results.items():\n",
    "        if query in baseline_labels:\n",
    "            # Found corresponding labels in the baseline dataset\n",
    "\n",
    "            # **** IMPORTANT ASSUMPTION ****\n",
    "            # We assume the key agent outputs (final_answer, generated_sql, tool_called)\n",
    "            # are somehow available within gpt4o_exp_input (the .input field from the dataset).\n",
    "            # This might be incorrect if the Experiment dataset only stored raw messages/tools.\n",
    "            # If evaluation fails later, we need to revisit how gpt4o_output is constructed here.\n",
    "            # Let's default to passing the whole dict for now.\n",
    "            gpt4o_output_data_for_eval = gpt4o_exp_input\n",
    "\n",
    "            combined_item = {\n",
    "                \"user_query\": query,\n",
    "                # This is the data the evaluator will receive as the 'output' of the task\n",
    "                \"gpt4o_output_for_evaluator\": gpt4o_output_data_for_eval,\n",
    "                # This is the ground truth data from the baseline dataset\n",
    "                \"ground_truth\": baseline_labels[query]\n",
    "            }\n",
    "            combined_eval_data.append(combined_item)\n",
    "            found_labels_count += 1\n",
    "        else:\n",
    "            # No labels found for this query in the baseline dataset\n",
    "            missing_labels_count += 1\n",
    "\n",
    "    print(f\"\\nSuccessfully combined data for {found_labels_count} examples.\")\n",
    "    if missing_labels_count > 0:\n",
    "        print(f\"Skipped {missing_labels_count} examples from the gpt-4o run because corresponding labels were not found in the baseline dataset.\")\n",
    "    if found_labels_count == 0 and len(experimental_results) > 0:\n",
    "        print(\"WARNING: No examples could be matched between datasets based on user_query. Check for subtle differences in query strings.\")\n",
    "\n",
    "\n",
    "    # --- Inspect the first combined item ---\n",
    "    if combined_eval_data:\n",
    "        print(\"\\n--- First Combined Example Structure---\")\n",
    "        # Print structure, not necessarily full content\n",
    "        first_combined = combined_eval_data[0]\n",
    "        print(f\"Keys: {list(first_combined.keys())}\")\n",
    "        print(f\"  user_query: {first_combined.get('user_query')[:80]}...\")\n",
    "        print(f\"  gpt4o_output_for_evaluator keys: {list(first_combined.get('gpt4o_output_for_evaluator', {}).keys())}\")\n",
    "        print(f\"  ground_truth keys: {list(first_combined.get('ground_truth', {}).keys())}\")\n",
    "        # print(json.dumps(combined_eval_data[0], indent=2)) # Uncomment for full details\n",
    "    else:\n",
    "        print(\"\\nNo combined data was prepared. Check dataset alignment or content.\")\n",
    "\n",
    "else:\n",
    "     print(\"\\nSkipping combination because no experimental results were extracted.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Data Preparation Complete ---\")\n",
    "# The variable 'combined_eval_data' now holds the data ready for evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GPT-4o Evaluation\n",
    "\n",
    "We have successfully loaded the experimental dataset containing the results from the `gpt-4o` agent run into the variable `evaluation_dataset_gpt4o`.\n",
    "\n",
    "We have also confirmed that our custom evaluation functions (`all_custom_evaluators`) operate in a **zero-shot** manner. They evaluate the agent's performance (Tool Usage, SQL Correctness, Final Answer Quality) by analyzing the agent's inputs and outputs (contained within `evaluation_dataset_gpt4o.input`) using an LLM judge (`call_gemini`), without requiring the separate ground truth labels that were added to the baseline dataset via the UI.\n",
    "\n",
    "**Next Step: Run Evaluation**\n",
    "\n",
    "The next step is to execute the evaluation using the `phoenix.experiments.run_experiment` function. We will pass it:\n",
    "*   The `evaluation_dataset_gpt4o` (containing the data from the `gpt-4o` run).\n",
    "*   The `dummy_task_function` (which simply passes the necessary input data through).\n",
    "*   The `all_custom_evaluators` list (containing our zero-shot evaluator functions).\n",
    "\n",
    "This will run the evaluations for the `gpt-4o` model and log the results to a new experiment run in Phoenix, allowing us to compare performance against the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Defining/Re-defining dummy_task_function with data extraction IN THIS CELL ---\n",
      "dummy_task_function defined/re-defined in this cell.\n",
      "\n",
      "Running full experiment for GPT-4o using direct API evaluators...\n",
      "🧪 Experiment started.\n",
      "📺 View dataset experiments: https://app.phoenix.arize.com/datasets/RGF0YXNldDo1/experiments\n",
      "🔗 View this experiment: https://app.phoenix.arize.com/datasets/RGF0YXNldDo1/compare?experimentId=RXhwZXJpbWVudDoxMQ==\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc82bdf108d4109acee5aa8444a3ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/125 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task runs completed.\n",
      "🧠 Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493803e5ed5c4e678fd108aa05508496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/375 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n",
      "WARNING: Skipping evaluation: Missing 'user_query' or 'tool_called' in output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 View this experiment: https://app.phoenix.arize.com/datasets/RGF0YXNldDo1/compare?experimentId=RXhwZXJpbWVudDoxMQ==\n",
      "\n",
      "Experiment Summary (04/28/25 09:48 PM +1000)\n",
      "--------------------------------------------\n",
      "                                  evaluator    n  n_scores  avg_score\n",
      "0  evaluate_final_answer_quality_direct_api  125       125      0.000\n",
      "1       evaluate_sql_correctness_direct_api  125       125      0.112\n",
      "2            evaluate_tool_usage_direct_api  125       125      0.128\n",
      "\n",
      "Tasks Summary (04/28/25 09:43 PM +1000)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0         125     125         0\n",
      "\n",
      "Full Experiment 'GPT4o_Full_OpenAI_Eval_20250428-214319' run initiated.\n",
      "Check Phoenix UI for results associated with experiment name 'GPT4o_Full_OpenAI_Eval_20250428-214319'.\n",
      "\n",
      "--- End GPT-4o Full Experiment ---\n"
     ]
    }
   ],
   "source": [
    "# Cell: Run Experiment on GPT-4o Dataset (with function redefined inside to ensure correct version is used)\n",
    "\n",
    "import json # Ensure json is imported\n",
    "import phoenix as px # Ensure phoenix is imported if needed\n",
    "from phoenix.experiments import run_experiment\n",
    "from datetime import datetime\n",
    "from typing import List, Callable, Dict, Any # Ensure imports\n",
    "from phoenix.experiments.types import Example # Make sure Example is imported\n",
    "\n",
    "# --- Define/Re-define Dummy Task Function HERE to ensure it's used by run_experiment ---\n",
    "print(\"\\n--- Defining/Re-defining dummy_task_function with data extraction IN THIS CELL ---\")\n",
    "\n",
    "# Helper function to safely extract user query from messages list\n",
    "def get_user_query(messages_list):\n",
    "    if not isinstance(messages_list, list): return None\n",
    "    for msg in messages_list:\n",
    "        if isinstance(msg, dict) and msg.get(\"role\") == \"user\" and \"content\" in msg:\n",
    "            return msg[\"content\"]\n",
    "    return None\n",
    "\n",
    "# Helper to find the final assistant answer\n",
    "def get_final_answer(messages_list):\n",
    "     if not isinstance(messages_list, list): return None\n",
    "     for msg in reversed(messages_list): # Look from the end\n",
    "         if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\":\n",
    "             if msg.get(\"content\"): return msg.get(\"content\")\n",
    "     return None\n",
    "\n",
    "# Helper to check if a specific tool was called\n",
    "def check_tool_called(messages_list, tool_name=\"query_database\"):\n",
    "    if not isinstance(messages_list, list): return False\n",
    "    for msg in messages_list:\n",
    "        if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n",
    "             for tool_call in msg.get(\"tool_calls\", []):\n",
    "                  if isinstance(tool_call, dict) and tool_call.get(\"function\", {}).get(\"name\") == tool_name:\n",
    "                       return True\n",
    "    return False\n",
    "\n",
    "# Helper to extract SQL query\n",
    "def get_generated_sql(messages_list):\n",
    "    if not isinstance(messages_list, list): return None\n",
    "    for msg in messages_list:\n",
    "         if isinstance(msg, dict) and msg.get(\"role\") == \"assistant\" and msg.get(\"tool_calls\"):\n",
    "              for tool_call in msg.get(\"tool_calls\", []):\n",
    "                   if isinstance(tool_call, dict) and tool_call.get(\"function\", {}).get(\"name\") == \"query_database\":\n",
    "                        try:\n",
    "                             func_dict = tool_call.get(\"function\", {})\n",
    "                             args_str = func_dict.get(\"arguments\", \"{}\")\n",
    "                             args = json.loads(args_str)\n",
    "                             return args.get(\"sql_query\")\n",
    "                        except json.JSONDecodeError: return None\n",
    "                        except Exception: return None\n",
    "    return None\n",
    "\n",
    "# The actual dummy task function definition\n",
    "def dummy_task_function(example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts relevant fields from the example.input (which contains the raw run data)\n",
    "    and returns a FLAT dictionary matching the structure expected by the evaluators.\n",
    "    \"\"\"\n",
    "    if not hasattr(example, 'input') or not isinstance(example.input, dict): return {}\n",
    "    input_data = example.input\n",
    "    messages = input_data.get(\"messages\", [])\n",
    "    user_query = get_user_query(messages)\n",
    "    final_answer = get_final_answer(messages)\n",
    "    tool_called = check_tool_called(messages)\n",
    "    generated_sql = get_generated_sql(messages)\n",
    "    # Return the flat dictionary that evaluators expect as their 'output' argument\n",
    "    return { \"user_query\": user_query, \"final_answer\": final_answer,\n",
    "             \"tool_called\": tool_called, \"generated_sql\": generated_sql }\n",
    "\n",
    "print(\"dummy_task_function defined/re-defined in this cell.\")\n",
    "# --- End Function Definition ---\n",
    "\n",
    "\n",
    "# --- Now proceed with running the experiment ---\n",
    "# Check if the gpt4o dataset variable is loaded from previous cell\n",
    "if 'evaluation_dataset_gpt4o' in locals() and len(evaluation_dataset_gpt4o) > 0:\n",
    "    missing_deps = []\n",
    "    if 'dummy_task_function' not in locals(): missing_deps.append(\"dummy_task_function\") # Should be found now\n",
    "    # Ensure the evaluator list from earlier setup cells is available\n",
    "    if 'all_final_evaluators' not in locals(): missing_deps.append(\"all_final_evaluators list\")\n",
    "    if 'px_client' not in locals() or px_client is None: missing_deps.append(\"px_client\")\n",
    "\n",
    "    if not missing_deps:\n",
    "        print(\"\\nRunning full experiment for GPT-4o using direct API evaluators...\")\n",
    "        now_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        # Give this experiment run a distinct name\n",
    "        experiment_name_gpt4o = f\"GPT4o_Full_OpenAI_Eval_{now_str}\" # Matched naming convention\n",
    "\n",
    "        # Define type hint for evaluator list (if not defined globally)\n",
    "        EvaluatorList = List[Callable[[Dict[str, Any]], float]]\n",
    "\n",
    "        # Assuming run_experiment works without explicit env vars here\n",
    "        try:\n",
    "            full_experiment_gpt4o = run_experiment(\n",
    "                evaluation_dataset_gpt4o,           # Use the GPT-4o dataset variable\n",
    "                dummy_task_function,                # Uses the function defined above in this cell\n",
    "                evaluators=all_final_evaluators,   # Re-use the same evaluators list\n",
    "                experiment_name=experiment_name_gpt4o, # Use the new, distinct experiment name\n",
    "                experiment_description=\"Eval GPT-4o: Direct API ToolUsage, SQLCorrectness, FinalAnswer.\" # Updated description\n",
    "                # concurrency=5 # Optional: Adjust concurrency if needed\n",
    "            )\n",
    "\n",
    "            print(f\"\\nFull Experiment '{experiment_name_gpt4o}' run initiated.\")\n",
    "            print(f\"Check Phoenix UI for results associated with experiment name '{experiment_name_gpt4o}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Catch potential errors like Connection refused if it reappears\n",
    "            print(f\"\\nERROR during run_experiment for GPT-4o: {e}\")\n",
    "            print(\"If this is a connection error, consider adding os.environ lines before the call.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Skipping GPT-4o experiment - missing dependencies: {', '.join(missing_deps)}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Skipping GPT-4o experiment - 'evaluation_dataset_gpt4o' was not found or is empty. Did you run the cell above to load it?\")\n",
    "\n",
    "print(\"\\n--- End GPT-4o Full Experiment ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
