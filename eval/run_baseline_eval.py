"""
Minimum Viable Evaluation (MVE) - Baseline Report Generator

Purpose:
--------
This script serves as the first step (MVE v1) in our evaluation harness. 
Its primary goal is to establish baseline performance metrics for the RAG 
system based on the initial human feedback collected. It processes the 
`test-set.csv` file containing logged interactions and feedback.

Methodology:
------------
1.  **Input Data:** Reads the `eval/test-set.csv`, which contains user 
    queries, system responses, performance data (latency, tokens), and 
    human feedback (PASS/FAIL ratings, reasons, etc.).
2.  **Filtering:** Focuses ONLY on the subset of interactions that received 
    explicit human feedback ('PASS' or 'FAIL' in `feedback_rating`).
3.  **Metrics Calculated:**
    *   **Quality:** Overall PASS Rate (%).
    *   **Performance:** Average and P95 Latency (seconds).
    *   **Cost/Usage:** Average Total Tokens per query and estimated average 
        cost (USD) per query (based on approximate pricing).
4.  **Output:** Prints a formatted report summarizing these baseline metrics.

Evaluation Philosophy & Limitations (MVE Context):
-------------------------------------------------
*   **Goal:** The ultimate aim of evaluation is to understand if changes to the 
    system (e.g., switching models, changing prompts, improving retrieval) 
    lead to better performance across quality, latency, and cost dimensions. 
    This script provides the essential "before" picture (the baseline).
*   **Focus on End-to-End Quality:** This initial MVE focuses on the human-perceived 
    quality (PASS/FAIL) of the final answer. 
*   **RAG Metrics Deferred:** We are *explicitly not* evaluating the retrieval 
    component's performance (e.g., context precision/recall, faithfulness) 
    in this MVE. This is a deliberate choice to keep the initial harness simple. 
    These can be added in future evaluation iterations.
*   **No Automated Judging Yet:** This script *only* analyzes existing human 
    feedback. It does *not* generate new responses or use automated methods 
    like LLM-as-Judge to evaluate them.

Next Steps / Future Evaluation Harness:
--------------------------------------
The next iteration of the evaluation harness would likely involve:
1.  Taking a test set of questions (likely from `test-set.csv`).
2.  Running these questions through a *modified* version of the RAG system 
    to generate *new* responses.
3.  Evaluating these new responses using automated methods, primarily:
    *   **LLM-as-Judge:** Using a powerful LLM to assign PASS/FAIL ratings 
      based on predefined criteria (potentially using few-shot examples).
    *   **(Optional) Deterministic Checks:** For specific questions, adding 
      checks like regex or string matching to verify key facts or keywords 
      (e.g., checking for specific instructor names).
4.  Calculating the same metrics (PASS Rate, Latency, Cost) for the *new* system.
5.  Comparing the new metrics against the baseline generated by *this* script.
"""

import pandas as pd
import numpy as np
import os

# --- Configuration ---
TEST_SET_PATH = os.path.join(os.path.dirname(__file__), "test-set.csv")
BASELINE_VERSION_NAME = "Human Feedback v1" # Identifier for this baseline
PROMPT_OVERHEAD_TOKENS = 100 # Assumed tokens for system prompt, question text etc.

# --- Pricing (Approximate - Update as needed!) ---
# Prices per 1K tokens in USD
PRICING = {
    "gpt-3.5-turbo-16k": { # Assuming same as gpt-3.5-turbo-0125
        "input": 0.0005, 
        "output": 0.0015
    },
    "gpt-4-turbo": {
        "input": 0.01,
        "output": 0.03
    },
    # Add other models used in your logs if necessary
    "DEFAULT_COMPLETION": { # Fallback if model not listed
        "input": 0.001, 
        "output": 0.002
    }
}
EMBEDDING_PRICING = {
    "text-embedding-3-small": 0.00002,
    "text-embedding-ada-002": 0.0001,
    # Add other embedding models if necessary
    "DEFAULT_EMBEDDING": 0.0001 # Fallback
}

# --- Helper Functions ---
def calculate_pass_rate(df):
    """Calculates the PASS rate from the feedback_rating column."""
    # Filter for rows with valid feedback
    rated_df = df[df['feedback_rating'].isin(['PASS', 'FAIL'])].copy()
    
    if rated_df.empty:
        return 0.0, 0 # Return 0 if no rated feedback
    
    pass_count = (rated_df['feedback_rating'] == 'PASS').sum()
    total_rated = len(rated_df)
    pass_rate = (pass_count / total_rated) * 100 if total_rated > 0 else 0
    return pass_rate, total_rated

def calculate_stats(series):
    """Calculates average, median, and P95 for a numeric series."""
    # Ensure data is numeric, coercing errors to NaN
    numeric_series = pd.to_numeric(series, errors='coerce').dropna()
    
    if numeric_series.empty:
        return np.nan, np.nan, np.nan
        
    avg = numeric_series.mean()
    median = numeric_series.median()
    p95 = numeric_series.quantile(0.95)
    return avg, median, p95

def calculate_estimated_cost(row):
    """Estimates the cost for a single interaction row."""
    try:
        model = row.get('model', "DEFAULT_COMPLETION")
        completion_pricing = PRICING.get(model, PRICING["DEFAULT_COMPLETION"])
        
        # Use text-embedding-3-small pricing if embedding tokens exist, otherwise use default
        # TODO: Ideally, check an 'embedding_model' column if available
        embedding_model_price = EMBEDDING_PRICING.get("text-embedding-3-small", EMBEDDING_PRICING["DEFAULT_EMBEDDING"])

        # --- Token Counts (handle potential missing values) ---
        context_tokens = pd.to_numeric(row.get('context_tokens', 0), errors='coerce')
        completion_tokens = pd.to_numeric(row.get('completion_tokens', 0), errors='coerce')
        embedding_tokens = pd.to_numeric(row.get('embedding_tokens', 0), errors='coerce')
        
        # Replace NaN with 0 after coercion attempt
        context_tokens = 0 if pd.isna(context_tokens) else context_tokens
        completion_tokens = 0 if pd.isna(completion_tokens) else completion_tokens
        embedding_tokens = 0 if pd.isna(embedding_tokens) else embedding_tokens
        
        # Estimate input tokens for completion model
        input_tokens_est = context_tokens + PROMPT_OVERHEAD_TOKENS
        
        # --- Cost Calculation ---
        embedding_cost = (embedding_tokens / 1000) * embedding_model_price
        completion_cost = (input_tokens_est / 1000) * completion_pricing["input"] + \
                          (completion_tokens / 1000) * completion_pricing["output"]
                          
        total_cost = embedding_cost + completion_cost
        return total_cost
        
    except Exception as e:
        # print(f"Warning: Could not calculate cost for row {row.get('id', '')}: {e}") # Optional debug
        return np.nan # Return NaN if calculation fails

# --- Main Execution ---
def main():
    print(f"--- Generating Baseline Evaluation Report ({BASELINE_VERSION_NAME}) ---")
    
    # Load the test set
    try:
        df = pd.read_csv(TEST_SET_PATH)
        print(f"Loaded {len(df)} records from {TEST_SET_PATH}")
    except FileNotFoundError:
        print(f"Error: Test set file not found at {TEST_SET_PATH}")
        return
    except Exception as e:
        print(f"Error loading CSV: {e}")
        return

    # --- Calculate Quality Metrics ---
    print("\nCalculating quality metrics based on human feedback...")
    pass_rate, num_rated = calculate_pass_rate(df)
    print(f"Found {num_rated} interactions with PASS/FAIL feedback.")

    # --- Calculate Performance/Cost Metrics (on rated subset) ---
    print("Calculating performance/cost metrics for rated interactions...")
    rated_df = df[df['feedback_rating'].isin(['PASS', 'FAIL'])].copy()
    
    avg_latency, median_latency, p95_latency = calculate_stats(rated_df['latency'])
    avg_tokens, median_tokens, p95_tokens = calculate_stats(rated_df['total_tokens'])
    
    # Calculate estimated cost for each rated interaction
    rated_df['estimated_cost_usd'] = rated_df.apply(calculate_estimated_cost, axis=1)
    avg_cost, median_cost, p95_cost = calculate_stats(rated_df['estimated_cost_usd'])
    
    # --- Print Report ---
    print("\n--- Baseline Evaluation Report --- ")
    print(f"Baseline Version: {BASELINE_VERSION_NAME}")
    total_records = len(df)
    print(f"Based on {num_rated} human-rated interactions (out of {total_records} total records) from {os.path.basename(TEST_SET_PATH)}")
    
    print("\n--- Quality --- ")
    if num_rated > 0:
        print(f"PASS Rate:        {pass_rate:.1f}%" )
    else:
        print("PASS Rate:        N/A (No feedback found)")
        
    print("\n--- Latency (seconds) --- ")
    if not np.isnan(avg_latency):
        print(f"Average:          {avg_latency:.2f}s")
        # print(f"Median:           {median_latency:.2f}s") # Optional: uncomment to show median
        print(f"P95:              {p95_latency:.2f}s")
    else:
        print("Latency Stats:    N/A (No valid latency data for rated interactions)")

    print("\n--- Tokens (total per query) --- ")
    if not np.isnan(avg_tokens):
        print(f"Average:          {avg_tokens:.0f}")
        # print(f"Median:           {median_tokens:.0f}") # Optional: uncomment to show median
        # print(f"P95:              {p95_tokens:.0f}") # Optional: uncomment to show P95 tokens
    else:
        print("Token Stats:      N/A (No valid token data for rated interactions)")

    print("\n--- Estimated Cost (USD per query) --- ")
    if not np.isnan(avg_cost):
        print(f"Average:          ${avg_cost:.6f}") 
        # print(f"Median:           ${median_cost:.6f}") # Optional
        # print(f"P95:              ${p95_cost:.6f}")    # Optional
        print("(Note: Cost is estimated based on token counts, assumed overhead, and approx. pricing.)")
    else:
        print("Cost Stats:       N/A (No valid cost data for rated interactions)")

    print("\n------------------------------------")

if __name__ == "__main__":
    main() 